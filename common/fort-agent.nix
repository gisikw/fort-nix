# Fort Agent Module
#
# Defines fort.needs and fort.capabilities options for the unified control plane.
# See docs/control-plane-design.md for architecture details.
#
# fort.needs.<type>.<name>: Declares what a host needs from capability providers
# fort.capabilities.<name>: Declares what capabilities a host exposes
#
{ rootManifest, cluster, ... }:
{
  config,
  lib,
  pkgs,
  ...
}:
let
  domain = rootManifest.fortConfig.settings.domain;
  hostName = config.networking.hostName;

  # Read all host manifests for RBAC derivation
  hostFiles = builtins.readDir cluster.hostsDir;
  allHostManifests = builtins.mapAttrs
    (name: _: import (cluster.hostsDir + "/" + name + "/manifest.nix"))
    hostFiles;

  # Build hosts.json with peer public keys from cluster topology
  # For each host, get its device UUID and then the device's SSH public key
  getHostPubkey = hostName':
    let
      hostConfig = allHostManifests.${hostName'};
      deviceUuid = hostConfig.device;
      deviceManifestPath = cluster.devicesDir + "/${deviceUuid}/manifest.nix";
      deviceConfig = import deviceManifestPath;
    in {
      name = hostName';
      pubkey = deviceConfig.pubkey;
    };

  # Build hosts.json structure: { "hostname": { "pubkey": "ssh-ed25519 ..." }, ... }
  # Includes both hosts (from device manifests) and principals with agentKeys
  hostEntries = map (h:
    let info = getHostPubkey h;
    in { name = info.name; value = { pubkey = info.pubkey; }; }
  ) (builtins.attrNames allHostManifests);

  # Extract principals with agentKey for agent authentication
  principals = rootManifest.fortConfig.settings.principals or {};
  principalEntries = lib.mapAttrsToList (name: cfg:
    { inherit name; value = { pubkey = cfg.agentKey; }; }
  ) (lib.filterAttrs (name: cfg: cfg ? agentKey) principals);

  hostsJson = builtins.listToAttrs (hostEntries ++ principalEntries);

  fcgiSocket = "/run/fort-agent/fcgi.sock";

  # Mandatory capability handlers (always present on all hosts)
  mandatoryHandlers = {
    # Return host status (generated by host-status aspect timer)
    status = pkgs.writeShellScript "handler-status" ''
      if [ -f /var/lib/fort/status/status.json ]; then
        ${pkgs.coreutils}/bin/cat /var/lib/fort/status/status.json
      else
        # Fallback if status.json doesn't exist yet
        uptime_secs=$(${pkgs.coreutils}/bin/cut -d' ' -f1 /proc/uptime | ${pkgs.coreutils}/bin/cut -d. -f1)
        ${pkgs.jq}/bin/jq -n \
          --arg hostname "${hostName}" \
          --argjson uptime "$uptime_secs" \
          --arg generated "$(${pkgs.coreutils}/bin/date -Iseconds)" \
          '{hostname: $hostname, status: "unknown", uptime_seconds: $uptime, generated_at: $generated}'
      fi
    '';

    # Return host manifest (apps, aspects, roles, exposedServices)
    manifest = pkgs.writeShellScript "handler-manifest" ''
      ${pkgs.coreutils}/bin/cat /var/lib/fort/host-manifest.json
    '';

    # Return holdings (handles this host is actively using)
    holdings = pkgs.writeShellScript "handler-holdings" ''
      if [ -f /var/lib/fort/holdings.json ]; then
        ${pkgs.coreutils}/bin/cat /var/lib/fort/holdings.json
      else
        echo '{"handles":[]}'
      fi
    '';
  };

  # Mandatory capabilities config (no GC needed for these)
  mandatoryCapabilities = {
    status = { needsGC = false; ttl = 0; };
    manifest = { needsGC = false; ttl = 0; };
    holdings = { needsGC = false; ttl = 0; };
  };

  # All capabilities = mandatory + user-defined
  allCapabilities = mandatoryCapabilities // lib.mapAttrs (name: cfg: {
    needsGC = cfg.needsGC;
    ttl = cfg.ttl;
  }) config.fort.capabilities;

  # All hosts AND principals with agentKeys allowed to call mandatory endpoints
  principalNames = builtins.attrNames (lib.filterAttrs (name: cfg: cfg ? agentKey) principals);
  allHosts = builtins.attrNames allHostManifests ++ principalNames;

  # Derive RBAC from cluster topology
  # For each capability this host exposes, determine which hosts can call it
  deriveRbac = capabilities:
    lib.mapAttrs (capName: capCfg:
      # For now, allow all cluster hosts and principals to call any capability
      # A more sophisticated version could use capCfg.satisfies to find
      # hosts that declare matching fort.needs.<satisfies>.* entries
      allHosts
    ) capabilities;

  # Need option type
  needOptions = {
    providers = lib.mkOption {
      type = lib.types.listOf lib.types.str;
      description = "Hostnames of capability providers to request from";
      example = [ "drhorrible" ];
    };

    request = lib.mkOption {
      type = lib.types.attrsOf lib.types.anything;
      default = { };
      description = "Request payload passed to the capability handler";
      example = { service = "outline"; };
    };

    store = lib.mkOption {
      type = lib.types.nullOr lib.types.str;
      default = null;
      description = "Path to store the response (null = don't store)";
      example = "/var/lib/fort/oidc/outline";
    };

    restart = lib.mkOption {
      type = lib.types.listOf lib.types.str;
      default = [ ];
      description = "Systemd services to restart after successful fulfillment";
      example = [ "outline.service" ];
    };
  };

  # Capability option type
  capabilityOptions = {
    handler = lib.mkOption {
      type = lib.types.path;
      description = "Path to handler script";
    };

    needsGC = lib.mkOption {
      type = lib.types.bool;
      default = false;
      description = "Whether this capability needs garbage collection (adds handle wrapper)";
    };

    ttl = lib.mkOption {
      type = lib.types.int;
      default = 86400;
      description = "Time-to-live in seconds for GC-able responses (only used if needsGC = true)";
      example = 3600;
    };

    satisfies = lib.mkOption {
      type = lib.types.nullOr lib.types.str;
      default = null;
      description = ''
        The need type this capability satisfies. Used for documentation and
        potentially for RBAC derivation (finding hosts that declare matching needs).
        If null, defaults to the capability name itself.

        Example: capability "oidc-register" might set satisfies = "oidc" to match
        fort.needs.oidc.* declarations.
      '';
      example = "oidc";
    };

    description = lib.mkOption {
      type = lib.types.str;
      default = "";
      description = "Human-readable description of the capability";
    };
  };

  # Generate needs.json from all fort.needs declarations
  #
  # Structure: fort.needs.<capability>.<name> = { providers, request, store, restart }
  # The first key IS the capability name - no magic transformation.
  # Example: fort.needs.ssl-cert.wildcard calls the "ssl-cert" capability
  needsJson = let
    flattenNeeds = needs:
      lib.concatLists (lib.mapAttrsToList (capability:
        lib.mapAttrsToList (name: cfg: {
          id = "${capability}-${name}";
          inherit capability;
          inherit (cfg) providers request restart;
          store = cfg.store;
        })
      ) needs);
  in builtins.toJSON (flattenNeeds config.fort.needs);

  # RBAC for mandatory endpoints (all hosts can call)
  mandatoryRbac = lib.mapAttrs (name: _: allHosts) mandatoryCapabilities;

  # Generate rbac.json from capabilities and topology (includes mandatory)
  rbacJson = builtins.toJSON (mandatoryRbac // deriveRbac config.fort.capabilities);

  # Generate capabilities.json with needsGC and ttl settings (includes mandatory)
  capabilitiesJson = builtins.toJSON allCapabilities;

  # Import the agent wrapper
  fortAgentWrapper = import ../pkgs/fort-agent-wrapper { inherit pkgs; };

  # Check if we have any needs or capabilities defined
  hasNeeds = config.fort.needs != { };
  hasCapabilities = config.fort.capabilities != { };

in
{
  options.fort = {
    needs = lib.mkOption {
      type = lib.types.attrsOf (lib.types.attrsOf (lib.types.submodule { options = needOptions; }));
      default = { };
      description = ''
        Declares what this host needs from capability providers.
        Structure: fort.needs.<capability>.<id> = { providers, request, store, restart }

        The first key is the capability to call. The second key is an arbitrary
        identifier for disambiguation - use "default" for singletons, or a
        descriptive id when you have multiple needs of the same capability.
        All actual parameters go in the request field.

        Examples:
          # Singleton - just use "default"
          fort.needs.ssl-cert.default = {
            providers = [ "drhorrible" ];
            store = "/var/lib/fort/ssl";
            restart = [ "nginx.service" ];
          };

          # Multiple of same capability - use descriptive ids
          fort.needs.oidc-register.outline = {
            providers = [ "drhorrible" ];
            request = { service = "outline"; };
            store = "/var/lib/fort/oidc/outline";
            restart = [ "outline.service" ];
          };
      '';
      example = {
        ssl-cert.default = {
          providers = [ "drhorrible" ];
          request = { };
          store = "/var/lib/fort/ssl";
          restart = [ "nginx.service" ];
        };
      };
    };

    capabilities = lib.mkOption {
      type = lib.types.attrsOf (lib.types.submodule { options = capabilityOptions; });
      default = { };
      description = ''
        Declares what capabilities this host exposes via the agent API.
        RBAC rules are derived automatically from cluster topology.

        Examples:
          fort.capabilities.ssl-cert = {
            handler = ./handlers/ssl-cert;
            description = "Return cluster SSL certificates";
          };

          fort.capabilities.oidc-register = {
            handler = ./handlers/oidc-register;
            needsGC = true;  # Creates GC-able state
            description = "Register OIDC client in pocket-id";
          };
      '';
      example = {
        ssl-cert = {
          handler = ./handlers/ssl-cert;
          description = "Return cluster SSL certificates";
        };
      };
    };
  };

  config = lib.mkMerge [
    # Core agent infrastructure - always present on all hosts
    {
      # Generate hosts.json with peer public keys and install mandatory handlers
      system.activationScripts.fortAgentHosts = {
        deps = [ ];
        text = ''
          install -d -m0755 /etc/fort-agent
          install -d -m0755 /etc/fort-agent/handlers
          install -Dm0644 ${pkgs.writeText "hosts.json" (builtins.toJSON hostsJson)} /etc/fort-agent/hosts.json

          # Install mandatory handlers
          install -Dm0755 ${mandatoryHandlers.status} /etc/fort-agent/handlers/status
          install -Dm0755 ${mandatoryHandlers.manifest} /etc/fort-agent/handlers/manifest
          install -Dm0755 ${mandatoryHandlers.holdings} /etc/fort-agent/handlers/holdings

          # Install RBAC and capabilities config (includes mandatory endpoints)
          install -Dm0644 ${pkgs.writeText "rbac.json" rbacJson} /etc/fort-agent/rbac.json
          install -Dm0644 ${pkgs.writeText "capabilities.json" capabilitiesJson} /etc/fort-agent/capabilities.json
        '';
      };

      # Runtime directory for socket and persistent handles storage
      systemd.tmpfiles.rules = [
        "d /run/fort-agent 0755 root root -"
        "d /var/lib/fort-agent 0700 root root -"
        "d /var/lib/fort-agent/handles 0700 root root -"
      ];

      # Socket activation for the FastCGI wrapper
      systemd.sockets.fort-agent = {
        description = "Fort Agent FastCGI Socket";
        wantedBy = [ "sockets.target" ];
        listenStreams = [ fcgiSocket ];
        socketConfig = {
          SocketMode = "0660";
          SocketUser = "root";
          SocketGroup = "nginx";
        };
      };

      # The actual service (activated by socket)
      systemd.services.fort-agent = {
        description = "Fort Agent FastCGI Wrapper";
        requires = [ "fort-agent.socket" ];
        after = [ "fort-agent.socket" ];

        serviceConfig = {
          Type = "simple";
          ExecStart = "${fortAgentWrapper}/bin/fort-agent-wrapper";
          StandardInput = "socket";
          StandardOutput = "socket";
        };
      };

      # Add /agent/* location to the host's nginx vhost
      # Extends the existing virtualHost from host-status aspect
      services.nginx.virtualHosts."${hostName}.fort.${domain}" = {
        locations."/agent/" = {
          extraConfig = ''
            # VPN-only access (cluster-internal)
            if ($is_vpn = 0) {
              return 444;
            }

            # FastCGI to the agent wrapper
            fastcgi_pass unix:${fcgiSocket};
            include ${pkgs.nginx}/conf/fastcgi_params;
            fastcgi_param SCRIPT_NAME $uri;
            fastcgi_param REQUEST_METHOD $request_method;
            fastcgi_param CONTENT_TYPE $content_type;
            fastcgi_param CONTENT_LENGTH $content_length;
            fastcgi_param QUERY_STRING $query_string;

            # Pass auth headers for signature verification
            fastcgi_param HTTP_X_FORT_ORIGIN $http_x_fort_origin;
            fastcgi_param HTTP_X_FORT_TIMESTAMP $http_x_fort_timestamp;
            fastcgi_param HTTP_X_FORT_SIGNATURE $http_x_fort_signature;
          '';
        };
      };
    }

    # Generate needs.json if any needs are declared
    (lib.mkIf hasNeeds {
      system.activationScripts.fortNeedsJson = {
        deps = [ "fortHostManifest" ];
        text = ''
          install -Dm0644 ${pkgs.writeText "needs.json" needsJson} /var/lib/fort/needs.json
        '';
      };
    })

    # Install user-defined capability handlers (if any)
    (lib.mkIf hasCapabilities {
      system.activationScripts.fortAgentUserHandlers = {
        deps = [ "fortAgentHosts" ];
        text = ''
          # Install user-defined handler scripts
          ${lib.concatStringsSep "\n" (lib.mapAttrsToList (name: cfg: ''
            install -Dm0755 ${cfg.handler} /etc/fort-agent/handlers/${name}
          '') config.fort.capabilities)}
        '';
      };
    })
  ];
}
