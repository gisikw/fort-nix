{"id":"fort-040","title":"oauth2-proxy groups claim not working with Pocket ID","description":"oauth2-proxy configured with --scope=groups and --oidc-groups-claim=groups but still returns 403 for users in the allowed group. Pocket ID supports groups scope/claim per OIDC discovery. Needs investigation.","status":"closed","priority":4,"issue_type":"bug","created_at":"2025-12-21T23:56:02.129313-06:00","updated_at":"2026-01-15T04:08:08.790990108Z","closed_at":"2026-01-15T04:08:08.790990108Z","close_reason":"Closed"}
{"id":"fort-0aa","title":"Investigate and fix drhorrible failed unit","description":"drhorrible showing 1 failed unit as of 2026-01-01.\n\nDiscovered while debugging headscale outage. Need to identify which unit and fix.\n\n**Update**: Cannot identify failing unit remotely - fort-agent status capability only returns count, not unit names. See fort-ydn for capability enhancement.\n\nTo investigate, need to SSH to drhorrible and run:\n```\nsystemctl --failed\n```","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-01T17:51:38.338732673Z","updated_at":"2026-01-01T17:55:18.572759013Z","closed_at":"2026-01-01T17:55:18.572759013Z","close_reason":"Transient - no failed units found when investigated"}
{"id":"fort-0by","title":"Bump Termix to 1.10.0","description":"Upgrade the Termix derivation to the latest release (1.10.0) before adding OIDC integration. This ensures we're building on a current base.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-02T15:25:55.400561185Z","updated_at":"2026-01-02T15:33:52.853358938Z","closed_at":"2026-01-02T15:33:52.853358938Z","close_reason":"Closed"}
{"id":"fort-0f8","title":"Add claude-code-ui to q host manifest","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-21T21:14:10.125918-06:00","updated_at":"2025-12-22T01:01:28.046688-06:00","closed_at":"2025-12-22T01:01:28.046688-06:00","close_reason":"Claude Code UI deployed and working on q host","dependencies":[{"issue_id":"fort-0f8","depends_on_id":"fort-zk9","type":"blocks","created_at":"2025-12-21T21:14:15.403159-06:00","created_by":"daemon"},{"issue_id":"fort-0f8","depends_on_id":"fort-4sb","type":"blocks","created_at":"2025-12-21T21:14:15.599336-06:00","created_by":"daemon"},{"issue_id":"fort-0f8","depends_on_id":"fort-ee2","type":"blocks","created_at":"2025-12-21T21:15:54.37237-06:00","created_by":"daemon"}]}
{"id":"fort-0ij","title":"Refactor forge config to support multiple repos","description":"Parent: fort-q3t\nDepends: fort-d9u (need token ready before testing)\n\n## Schema change\nIn `clusters/bedlam/manifest.nix`, change from:\n```nix\nforge = {\n  org = \"infra\";\n  repo = \"fort-nix\";\n  mirrors = { github = {...}; };\n};\n```\n\nTo:\n```nix\nforge = {\n  org = \"infra\";\n  repos = {\n    \"fort-nix\" = {\n      mirrors = { github = { remote = \"github.com/gisikw/fort-nix\"; tokenFile = ./github-mirror-token.age; }; };\n    };\n  };\n};\n```\n\n## Bootstrap refactor\nIn `apps/forgejo/default.nix`:\n1. Update `forgeConfig` references to use new schema\n2. Change `FORGEJO_REPO` scalar to iterate over `forgeConfig.repos`\n3. Per-repo: create repo, configure mirrors\n4. Keep org creation as-is (still single org)\n5. Keep token generation as-is (not per-repo)\n\n## Key changes in bootstrap script\n- `for repo in $(echo \"$REPOS\" | jq -r 'keys[]'); do`\n- Each repo gets its own mirror config\n- Existing fort-nix behavior preserved\n\n## Testing\n- `nix flake check` passes\n- Deploy to drhorrible\n- Verify fort-nix mirror still works (push a commit, check GitHub)\n\n## Acceptance\n- [ ] Schema updated\n- [ ] Bootstrap iterates over repos\n- [ ] fort-nix mirror still works after deploy","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-12T06:01:50.208968671Z","updated_at":"2026-01-12T06:13:03.800356103Z","closed_at":"2026-01-12T06:13:03.800356103Z","close_reason":"Closed"}
{"id":"fort-0pb","title":"Create skill for SSO integration guidance","description":"## Context\nCLAUDE.md documents the SSO modes table and the OIDC credential delivery contract, but implementing other modes (headers, basicauth, gatekeeper) still requires archaeology through examples.\n\nRather than bloating always-loaded context with howtos for every mode, create a skill that provides mode-specific implementation guidance.\n\n## Proposed skill: `sso-guide`\n\nProgressive disclosure skill that loads when configuring authentication for services.\n\n### Content to extract from AGENTS.md\n- \"SSO Modes\" section (lines 66-75)\n- \"OIDC Credential Delivery\" section (lines 76-110)\n\n### Skill structure\n```\n.claude/skills/sso-guide/\n├── SKILL.md              # Mode selection overview, when to use each\n├── oidc.md               # Native OIDC pattern + pocket-id provisioning flow\n├── headers.md            # oauth2-proxy X-Auth-* pattern\n├── basicauth.md          # Basic auth translation pattern\n├── gatekeeper.md         # Login wall pattern\n└── troubleshooting.md    # Callback URLs, token scopes, groups claim\n```\n\n### Definition of done\n- [ ] Skill created in .claude/skills/sso-guide/\n- [ ] AGENTS.md SSO section reduced to mode table only (no detailed howtos)\n- [ ] Tested: skill loads when discussing SSO configuration\n\n## Labels\ndx sso","status":"closed","priority":3,"issue_type":"feature","created_at":"2025-12-28T02:23:10.244451095Z","updated_at":"2025-12-31T04:44:30.701340425Z","closed_at":"2025-12-31T04:44:30.701340425Z","close_reason":"Closed","labels":["dx","sso"]}
{"id":"fort-0rj","title":"Support group-based OIDC client restrictions in pocket-id","description":"## Context\nPocket-id supports restricting OIDC client access to specific LDAP groups. This is cleaner than app-level enforcement (e.g., Forgejo's `--required-claim-*`) because:\n\n- Centralized policy: \"who can access what\" lives in one place\n- Apps stay simple - just \"use OIDC\", no group logic\n- Consistent enforcement across all services\n\n## Implementation\nExtend `service-registry` to configure group restrictions when creating OIDC clients in pocket-id.\n\nCould leverage the existing `sso.groups` option in `fortCluster.exposedServices`:\n```nix\nsso = {\n  mode = \"oidc\";\n  groups = [ \"admin\" ];  # Currently only used for oauth2-proxy\n};\n```\n\nThe registry.rb `create_pocketid_client` function would need to pass allowed groups in the API request.\n\n## Notes\n- Need to verify pocket-id API supports this (UI shows the option)\n- Once implemented, can remove app-level group checks (e.g., Forgejo's `--required-claim-*`)\n- Defense-in-depth: could keep app-level as fallback initially","status":"closed","priority":3,"issue_type":"feature","created_at":"2025-12-28T03:27:02.216830613Z","updated_at":"2026-01-15T14:26:14.010774972Z","closed_at":"2026-01-15T14:26:14.010774972Z","close_reason":"Closed","labels":["pocket-id","sso"],"dependencies":[{"issue_id":"fort-0rj","depends_on_id":"fort-040","type":"blocks","created_at":"2026-01-13T16:16:18.067918496Z","created_by":"daemon"}]}
{"id":"fort-0xm","title":"CI/CD pipeline for deploys","description":"Set up CI/CD to handle deploys. Addresses previous pain points: deploy failure log visibility, sharing output between user and agent, auto-rollback debugging. Depends on self-hosted git.","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-27T19:43:36.680223096Z","updated_at":"2025-12-30T04:36:50.899464279Z","closed_at":"2025-12-30T04:36:50.899464279Z","close_reason":"Closed","dependencies":[{"issue_id":"fort-0xm","depends_on_id":"fort-okx","type":"blocks","created_at":"2025-12-27T19:43:44.274304103Z","created_by":"daemon"}]}
{"id":"fort-12e","title":"Termix loses state on reboot (q)","description":"After q crashed and rebooted, termix lost its state.\n\n## Investigation needed\n\n1. Check where termix stores its state\n2. Verify if it's using `/var/lib/termix` or similar (which would be persisted)\n3. If it's using a non-persisted location (e.g., `/tmp`, home dir on tmpfs), fix it\n\n## Context\n\nq uses impermanence (tmpfs root with `/persist/system` for state). Services need to store persistent data in `/var/lib` or explicitly configure persistence.\n\n## Fix options\n\n- Configure termix to use `/var/lib/termix` for state\n- Or add its state directory to impermanence persistence rules","status":"closed","priority":2,"issue_type":"bug","created_at":"2025-12-29T23:25:55.532266246Z","updated_at":"2025-12-30T05:40:15.049807754Z","closed_at":"2025-12-30T05:40:15.049807754Z","close_reason":"Closed"}
{"id":"fort-19c","title":"Add blog content to fort-nix monorepo","description":"Move blog content into fort-nix repository. Full monorepo mode.\n\nRationale: If we can have in this repo:\n- Home Assistant automations\n- Runtime orchestration  \n- OIDC reconciliation\n- Infrastructure as code\n\n...then blog posts can be declarative too.\n\nStructure TBD, probably:\n```\ncontent/\n  blog/\n    posts/\n      2024-01-01-my-post.md\n    pages/\n      about.md\n```\n\nPairs with Hugo app (fort-xxx) for build/serve.","status":"closed","priority":2,"issue_type":"feature","created_at":"2026-01-04T06:19:23.120183721Z","updated_at":"2026-01-06T05:44:54.721409048Z","closed_at":"2026-01-06T05:44:54.721409048Z","close_reason":"Closed"}
{"id":"fort-1ei","title":"Home Assistant: house armed/disarmed UX improvements","description":"The current house armed/disarmed flow is meh. Improvements to consider:\n\n- Skip 'alert cleared' notification to the person who dismissed it\n- Better feedback when arming/disarming\n- Clearer status visibility\n\nNeeds UX review to identify specific pain points.","status":"in_progress","priority":2,"issue_type":"feature","created_at":"2026-01-14T16:32:49.98440554Z","updated_at":"2026-01-16T04:48:38.065152346Z"}
{"id":"fort-1hd","title":"Expand exposedServices schema for external domains","description":"Current exposedServices only supports subdomains of the cluster domain. Add support for arbitrary external domains (e.g., catdevurandom.com) with proper ACME integration. This enables hosting external-domain services on non-beacon hosts.","status":"open","priority":3,"issue_type":"feature","created_at":"2026-01-04T19:55:57.891381133Z","updated_at":"2026-01-04T19:55:57.891381133Z"}
{"id":"fort-1mb","title":"Patch Termix to keep admin password login enabled","description":"When bootstrap disables password login, it locks out the admin user we created, preventing future OIDC reconfiguration. Patch Termix to either: (1) keep password login enabled for the admin user only, or (2) find another way to preserve admin access for OIDC config changes. This caused a lockout when pocket-id credentials rotated.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-08T05:43:43.31980399Z","updated_at":"2026-01-13T16:31:00.146998518Z","closed_at":"2026-01-13T16:31:00.146998518Z","close_reason":"no-longer-needed"}
{"id":"fort-1nj","title":"Add monitoring for headscale/mesh health","description":"Proactive alerting when tailnet is degraded or headscale is unhealthy, rather than discovering issues when MagicDNS stops resolving.\n\nContext: During the 2026-01-01 outage, the only signal was DNS resolution failures noticed manually.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-01T17:51:24.035103761Z","updated_at":"2026-01-14T16:39:10.428384593Z","closed_at":"2026-01-14T16:39:10.428384593Z","close_reason":"Covered by existing Gatus host monitoring - tailnet issues surface as host unreachability"}
{"id":"fort-1rf","title":"Add radicale htpasswd secret","description":"Create agenix secret for Radicale htpasswd file:\n\n1. Generate a secure password for the CalDAV user\n2. Create htpasswd file content (e.g., `htpasswd -nb kevin \u003cpassword\u003e`)\n3. Encrypt and add to `secrets.nix`\n4. Reference in radicale app module via `age.secrets`\n\nThis password will be used by:\n- External calendar clients (Apple Calendar, Thunderbird) for CalDAV sync\n- vdirsyncer for server-side calendar sync\n\nParent epic: fort-zye","status":"closed","priority":3,"issue_type":"task","created_at":"2026-01-15T14:53:27.983237259Z","updated_at":"2026-01-15T15:14:13.362666138Z","closed_at":"2026-01-15T15:14:13.362666138Z","close_reason":"Closed","dependencies":[{"issue_id":"fort-1rf","depends_on_id":"fort-zye","type":"parent-child","created_at":"2026-01-15T14:55:27.857733949Z","created_by":"daemon"}]}
{"id":"fort-1w6","title":"Configure gh CLI for Forgejo","description":"The gh CLI can apparently be configured to work with Forgejo instances.\n\nThis would enable:\n- `gh run list` to check CI status\n- `gh pr` commands\n- Other gh conveniences from dev-sandbox\n\nInvestigate how to configure this and add it to dev-sandbox aspect if viable.","status":"closed","priority":3,"issue_type":"feature","created_at":"2025-12-29T23:32:45.355476089Z","updated_at":"2025-12-30T07:10:32.407495174Z","closed_at":"2025-12-30T07:10:32.407495174Z","close_reason":"gh CLI doesn't support non-GitHub forges. The alternative (tea CLI) also lacks workflow run monitoring - its 'actions' command only manages secrets/variables. No clean CLI solution exists for the original use case."}
{"id":"fort-23j","title":"Fix Termix mobile theme/font support","description":"## Background\nfort-twh added Monokai Pro Spectrum theme and ProggyClean Nerd Font to Termix via container startup patching. Desktop works perfectly, mobile does not.\n\n## What's Working (Desktop)\n- Theme dropdown shows \"Monokai Pro Spectrum\"\n- Font dropdown shows \"ProggyClean Nerd Font\" (first in list)\n- Colors apply correctly (directories are orange #fd9353)\n\n## What's Broken (Mobile)\n- Theme selected on desktop doesn't apply colors on mobile\n- Directories show default blue (#729fcf) instead of orange (#fd9353)\n- CSS shows: `.xterm-fg-12 { color: #729fcf; }` instead of `#fd9353`\n\n## Current Patching (apps/termix/default.nix)\nContainer startup script patches /app/html/assets/index-ByvOA9sR.js:\n1. Adds ProggyClean to TERMINAL_FONTS array (before Caskaydia)\n2. Adds @font-face CSS for ProggyClean\n3. Adds monokaiProSpectrum to TERMINAL_THEMES object (after dracula)\n\nUses global regex to catch all instances - but only finds 1 dracula instance.\n\n## Investigation Findings\n- Only 1 JS chunk contains theme data (index-ByvOA9sR.js)\n- Source code shows mobile Terminal.tsx has HARDCODED theme colors (just bg/fg)\n- Mobile imports TERMINAL_THEMES but never uses it for lookup\n- Desktop uses `TERMINAL_THEMES[config.theme]?.colors` - mobile doesn't\n- #729fcf is \"Termix Dark\" default brightBlue - confirms fallback\n\n## Contradiction\nUser reports pre-existing themes (Dracula, Solarized) DO work on mobile with full ANSI colors. Source code suggests they shouldn't. Possibilities:\n1. Docker image differs from GitHub source\n2. There's a code path not found in source analysis\n3. User observation needs verification\n\n## Next Steps\n1. Have user verify: does Dracula on mobile show purple (#bd93f9) or default blue (#729fcf)?\n2. If mobile truly is hardcoded, consider patching hardcoded color values directly\n3. If there's a hidden code path, find and patch it\n\n## Related\n- fort-1mb: Admin password lockout issue (separate problem discovered during this work)","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-08T12:42:54.163979662Z","updated_at":"2026-01-08T22:10:10.693254253Z","closed_at":"2026-01-08T22:10:10.693254253Z","close_reason":"Closed"}
{"id":"fort-244","title":"Create pkgs/claude-code-ui derivation","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-21T21:14:09.480545-06:00","updated_at":"2025-12-21T21:23:44.616418-06:00","closed_at":"2025-12-21T21:23:44.616418-06:00","close_reason":"Superseded by fort-xzk and fort-69s for proper npm derivations"}
{"id":"fort-2i8","title":"Improve deploy-rs failure debugging","description":"When deploy-rs fails, currently requires SSH + journalctl to diagnose. Options: capture activation logs, surface systemd failures in deploy output, or create a 'just diagnose \u003chost\u003e' helper that pulls recent failures.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-21T11:41:09.489375-06:00","updated_at":"2025-12-27T19:43:36.211456156Z","closed_at":"2025-12-27T19:43:36.211456156Z","close_reason":"Superseded by CI/CD approach"}
{"id":"fort-2nc","title":"Configure vdirsyncer for Radicale sync","description":"Update vdirsyncer configuration to include the Radicale calendar:\n\n1. Add Radicale as a CalDAV storage backend in vdirsyncer config\n2. Configure bidirectional sync (Radicale ↔ local)\n3. Integrate with existing Google Calendar sync for merged khal view\n4. Test creating events in Radicale shows up in khal\n5. Test creating events via khal syncs to Radicale\n\nThe goal is a merged calendar view in khal:\n- Work calendar (Google, read-only except for timeblocking)\n- Personal calendar (Radicale, full read/write)\n\nDepends on: fort-815 (deployment)\n\nParent epic: fort-zye","status":"closed","priority":3,"issue_type":"task","created_at":"2026-01-15T14:53:43.95140246Z","updated_at":"2026-01-15T15:44:25.292000347Z","closed_at":"2026-01-15T15:44:25.292000347Z","close_reason":"Closed","dependencies":[{"issue_id":"fort-2nc","depends_on_id":"fort-815","type":"blocks","created_at":"2026-01-15T14:53:49.351406843Z","created_by":"daemon"},{"issue_id":"fort-2nc","depends_on_id":"fort-zye","type":"parent-child","created_at":"2026-01-15T14:55:27.960803556Z","created_by":"daemon"}]}
{"id":"fort-2xw","title":"Deprovision claude-code-ui from q host","description":"Remove claude-code-ui app module from q host manifest, delete the derivations and app module code. CC-UI approach abandoned in favor of direct ratched dev environment.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-27T19:31:29.319511184Z","updated_at":"2025-12-28T06:59:02.666281142Z","closed_at":"2025-12-28T06:59:02.666281142Z","close_reason":"Closed"}
{"id":"fort-2zd","title":"Decompose common/fort.nix into modular concerns","description":"common/fort.nix is currently handling multiple concerns:\n- Service exposure (fort.cluster.services)\n- nginx virtual host generation\n- oauth2-proxy setup\n- SSL certificate handling\n\nNow that common/fort/ exists (with control-plane.nix), consider decomposing fort.nix into:\n- common/fort/services.nix - service exposure\n- common/fort/nginx.nix - virtual host generation\n- common/fort/auth.nix - oauth2-proxy/SSO\n\ncommon/fort.nix would become a simple aggregator that imports from common/fort/*.\n\nLow priority - current structure works, this is about maintainability.","status":"open","priority":4,"issue_type":"task","created_at":"2026-01-10T16:49:01.266812569Z","updated_at":"2026-01-10T16:49:01.266812569Z"}
{"id":"fort-2zl","title":"Make attic cache inclusion resilient to network failure","description":"When cache.gisi.network is unreachable, builds should proceed without the cache rather than blocking.\n\nOptions:\n- Use nix's fallback-if-missing behavior\n- Check connectivity before including\n- Set appropriate timeouts\n\nContext: During 2026-01-01 outage, builds were blocked because the attic cache was unreachable over the down tailnet.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-01T17:51:24.193238664Z","updated_at":"2026-01-14T15:27:35.262788931Z","closed_at":"2026-01-14T15:27:35.262788931Z","close_reason":"Closed"}
{"id":"fort-3sy","title":"Add nginx-\u003eheadscale systemd dependency","description":"At boot, nginx starts before headscale is ready to accept connections, causing ~12 seconds of 502/400 errors.\n\nFix: Add After=headscale.service to nginx on the beacon host.\n\nContext: During 2026-01-01 debugging, saw connection refused errors in nginx logs from boot time.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-01T17:51:38.693485611Z","updated_at":"2026-01-09T14:30:22.561367839Z","closed_at":"2026-01-09T14:30:22.561367839Z","close_reason":"Closed"}
{"id":"fort-3xl","title":"Implement Gatus monitoring on beacon with polling discovery","description":"Implement the polling-based monitoring solution from docs/beacon-monitoring-exploration.md:\n\n1. Add health option to fort.cluster.services schema\n2. Extend status.json to include services array\n3. Create apps/gatus/default.nix with:\n   - Poll script (5min timer, fetches status.json from tailnet peers)\n   - GC script (activation, removes orphan hosts using Nix list)\n   - Gatus service config\n4. Add gatus to beacon role\n5. VPN-only visibility for Phase 1\n\nRelated tickets this helps address:\n- fort-1nj: Headscale health monitoring\n- fort-576: Failed systemd alerting","status":"closed","priority":2,"issue_type":"feature","created_at":"2026-01-13T20:47:47.292081821Z","updated_at":"2026-01-13T22:54:25.901382769Z","closed_at":"2026-01-13T22:54:25.901382769Z","close_reason":"Closed"}
{"id":"fort-48d","title":"fort-agent should restart when capabilities change","description":"The fort-agent wrapper reads handler config at startup. When capabilities change via deploy, the agent doesn't pick them up until manually restarted.\n\nShould add a PathChanged or similar trigger on /etc/fort-agent/handlers/ or /etc/fort-agent/capabilities.json to auto-restart.\n\nRelated: fort-5bj (agent needs restart after deploy)","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-01-06T04:15:19.172643643Z","updated_at":"2026-01-06T05:54:29.120547047Z","closed_at":"2026-01-06T05:54:29.120547047Z","close_reason":"Closed"}
{"id":"fort-49n","title":"Wire more services behind Pocket ID/LDAP auth","description":"Extend SSO coverage. Candidates: SillyTavern, qbittorrent (likely feasible). Jellyfin, Home Assistant (harder - native auth integration). Current SSO modes: none, oidc, headers, basicauth, gatekeeper. Need to verify each service's auth capabilities.","status":"open","priority":2,"issue_type":"feature","created_at":"2025-12-21T11:41:08.545668-06:00","updated_at":"2025-12-21T11:41:08.545668-06:00","dependencies":[{"issue_id":"fort-49n","depends_on_id":"fort-4fm","type":"blocks","created_at":"2025-12-21T11:41:23.088026-06:00","created_by":"daemon"},{"issue_id":"fort-49n","depends_on_id":"fort-0rj","type":"blocks","created_at":"2026-01-13T16:16:18.010870096Z","created_by":"daemon"}]}
{"id":"fort-4fm","title":"Custom derivation for Pocket ID","description":"Pocket ID on forge (drhorrible) currently requires nixpkgs unstable. Create custom derivation following Zot pattern to pin specific version and allow stable channel on forge host.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-21T11:41:08.237954-06:00","updated_at":"2025-12-21T14:10:09.907207-06:00","closed_at":"2025-12-21T14:10:09.907207-06:00","close_reason":"Closed","dependencies":[{"issue_id":"fort-4fm","depends_on_id":"fort-ax1","type":"blocks","created_at":"2025-12-21T11:41:22.888708-06:00","created_by":"daemon"}]}
{"id":"fort-4ge","title":"Module refactor: express inter-service dependencies","description":"Currently host manifests must 'just know' service dependencies. Refactor to load unparameterized module definitions into a dict first, allowing modules to express dependencies on other services (e.g., zigbee2mqtt depends on mosquitto). Manifests then select from available modules.","status":"open","priority":3,"issue_type":"feature","created_at":"2025-12-21T11:41:08.858223-06:00","updated_at":"2025-12-21T11:41:08.858223-06:00"}
{"id":"fort-4ih","title":"Fix claude-code derivation - binary is raw bun, not bundled app","description":"The binary downloaded from Anthropic's GCS bucket is just raw bun runtime, not Claude Code bundled into bun. Need to investigate proper installation - may need to run 'claude install' post-download or use npm package instead.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-12-22T00:39:34.734941-06:00","updated_at":"2025-12-27T11:43:16.392596-06:00","closed_at":"2025-12-27T11:43:16.392596-06:00","close_reason":"Fixed with npm/buildNpmPackage approach, version 2.0.76"}
{"id":"fort-4jd","title":"Skill: debug-deploy troubleshooting","description":"## Context\nDeployment debugging requires constructing SSH commands, knowing common failure modes, understanding deploy-rs rollback behavior. AGENTS.md has the basics but could be more comprehensive.\n\n## Proposed skill: `debug-deploy`\n\nProgressive disclosure skill for deployment troubleshooting.\n\n### Content to extract from AGENTS.md\n- \"Debugging Deployment Failures\" section (lines 393-408)\n- SSH command templates\n- Common issues list\n\n### Skill structure\n```\n.claude/skills/debug-deploy/\n├── SKILL.md              # Systematic debugging flow\n├── ssh-commands.md       # Command templates with cluster manifest lookups\n└── common-issues.md      # Expanded troubleshooting guide\n```\n\n### Definition of done\n- [ ] Skill created in .claude/skills/debug-deploy/\n- [ ] AGENTS.md debugging section reduced to brief overview\n- [ ] Tested: skill loads when deployment fails or debugging discussed\n\n## Labels\ndx ops","status":"open","priority":3,"issue_type":"feature","created_at":"2025-12-31T04:20:42.451120713Z","updated_at":"2025-12-31T04:20:42.451120713Z"}
{"id":"fort-4sb","title":"Add Anthropic API key secret","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-21T21:14:09.910321-06:00","updated_at":"2025-12-21T22:15:27.382419-06:00","closed_at":"2025-12-21T22:15:27.382419-06:00","close_reason":"Not needed - using OAuth login instead of API key"}
{"id":"fort-549","title":"Derive VPN CIDR from mesh config instead of hardcoding","description":"The nginx geo block in common/fort.nix hardcodes 100.64.0.0/10 (Tailscale's CGNAT range). This should be derived from mesh/tailscale config to avoid magic numbers that happen to align with our tailnet setup.","status":"closed","priority":3,"issue_type":"feature","created_at":"2025-12-30T07:38:10.952090605Z","updated_at":"2026-01-13T16:37:21.964396971Z","closed_at":"2026-01-13T16:37:21.964396971Z","close_reason":"Closed"}
{"id":"fort-576","title":"Add alerting for failed systemd services across cluster","description":"Proactive alerting when any host has failed systemd units, rather than discovering them while debugging other issues.\n\nCould integrate with fort-agent status endpoint which already reports failed_units count.","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-01T17:51:38.500340467Z","updated_at":"2026-01-01T17:51:38.500340467Z","dependencies":[{"issue_id":"fort-576","depends_on_id":"fort-uqd","type":"blocks","created_at":"2026-01-14T17:15:08.010405025Z","created_by":"daemon"}]}
{"id":"fort-59e","title":"Formalize incident response and blameless post-mortem process","description":"Create a structured process for handling incidents, possibly as a skill or documented workflow.\n\nShould include:\n- Incident declaration and triage\n- Debugging workflow (what to check first, how to escalate)\n- Blameless post-mortem template\n- After-action ticketing (like we did for the 2026-01-01 headscale outage)\n- Timeline reconstruction\n\nCould be a skill that guides through the process, or a documented runbook in AGENTS.md.\n\nContext: The 2026-01-01 headscale outage was handled ad-hoc but produced good outcomes (root cause found, 7 follow-up tickets). Formalizing this would make future incidents smoother.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-01T17:57:26.673327052Z","updated_at":"2026-01-13T16:31:00.253139424Z","closed_at":"2026-01-13T16:31:00.253139424Z","close_reason":"out-of-scope"}
{"id":"fort-5bj","title":"fort-agent needs restart after deploy to pick up new config","description":"The fort-agent service loads config files (rbac.json, capabilities.json, hosts.json) once at startup. After a deploy that updates these files, the service continues using the old config until manually restarted.\n\n**Observed**: After deploying new capabilities (journal, restart), they returned 404 until the service was restarted.\n\n**Fix options**:\n1. Add `restartTriggers` to the systemd service to restart on config changes\n2. Use `ExecReload` and `ReloadPropagatedFrom` for graceful reload\n3. Have the Go wrapper watch for config file changes\n\nOption 1 is simplest for a socket-activated service.","status":"closed","priority":3,"issue_type":"bug","created_at":"2025-12-31T21:28:41.378134064Z","updated_at":"2026-01-06T05:55:49.691973025Z","closed_at":"2026-01-06T05:55:49.691973025Z","close_reason":"Closed"}
{"id":"fort-5f9","title":"Optimize just test for faster dev loops","description":"Currently `just test` validates all hosts/devices which takes a while. Since we have CI/CD, consider a lighter validation for dev loops - maybe just targeting known impacted hosts based on changed files, or having a `just test-host \u003cname\u003e` variant.","status":"closed","priority":4,"issue_type":"task","created_at":"2025-12-31T21:52:58.213121699Z","updated_at":"2026-01-13T16:34:03.00522383Z","closed_at":"2026-01-13T16:34:03.00522383Z","close_reason":"Closed"}
{"id":"fort-5g7","title":"Improve AGENTS.md with project context","description":"Current AGENTS.md is minimal (just bd reference). Add: architecture overview, key patterns (fortCluster.exposedServices, SSO modes, derivation pattern), common tasks, deployment workflow, debugging tips.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-21T11:41:09.171375-06:00","updated_at":"2025-12-21T12:10:34.249686-06:00","closed_at":"2025-12-21T12:10:34.249686-06:00","close_reason":"Closed"}
{"id":"fort-5j8","title":"Skill: add-secret workflow","description":"## Context\nAdding secrets involves: creating .age file, updating secrets.nix with recipient keys, understanding principal→recipient mapping. Currently scattered knowledge.\n\n## Proposed skill: `add-secret`\n\nProgressive disclosure skill for agenix secret management.\n\n### Content to extract from AGENTS.md\n- \"Secrets\" section (lines 326-342)\n- Dev sandbox decryption testing\n\n### Skill structure\n```\n.claude/skills/add-secret/\n├── SKILL.md              # Workflow: create .age, update secrets.nix\n└── recipients.md         # Principal roles → recipient selection\n```\n\n### Definition of done\n- [ ] Skill created in .claude/skills/add-secret/\n- [ ] AGENTS.md secrets section reduced to brief pointer\n- [ ] Tested: skill loads when adding encrypted secrets\n\n## Labels\ndx","status":"open","priority":3,"issue_type":"feature","created_at":"2025-12-31T04:20:42.29311195Z","updated_at":"2025-12-31T04:20:42.29311195Z"}
{"id":"fort-5n3","title":"Self-service tailscale registration behind OIDC","description":"Create a self-registration endpoint protected by Pocket ID OIDC that:\n1. Generates a pre-auth key\n2. Returns it to the authenticated user\n3. Auto-approves the resulting node\n\nThis removes the two-step SSH-dependent registration process when a client forgets its config.\n\nContext: During 2026-01-01 outage, macbook tailscale client forgot headscale config entirely, requiring manual re-auth and SSH to approve the key.","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-01T17:51:24.364678997Z","updated_at":"2026-01-01T17:51:24.364678997Z"}
{"id":"fort-5of","title":"Move HA device helpers to apps/homeassistant/","description":"The device helper functions in devices.nix (mkAqaraContactSensor, mkHueLight, mkSenckitAlarm, etc.) aren't cluster-specific - they define entity naming patterns for device types.\n\nThese should live in apps/homeassistant/ as shared helpers, while the actual device instantiations (mkSenckitAlarm \"bedroom_4__alarm\") remain in the host-specific devices.nix.","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-02T06:33:43.682962595Z","updated_at":"2026-01-02T06:33:43.682962595Z"}
{"id":"fort-67y","title":"Add cluster-level flakes as intermediary between root and host flakes","description":"## Context\n\nHost flakes currently `follows` the root flake for inputs. We want cluster-specific inputs (like home-manager configs) without polluting the root flake.\n\n## Proposed Structure\n\n```\nroot flake (core inputs: nixpkgs, agenix, deploy-rs, etc.)\n    ↑ follows\ncluster flake (cluster-specific inputs like home-kevin, cluster config)\n    ↑ follows\nhost flake (nixosConfiguration, inherits from cluster)\n```\n\n## Changes\n\n1. Add `clusters/bedlam/flake.nix`:\n   - `follows` root flake for core inputs\n   - Adds cluster-specific inputs (e.g., `home-kevin.url = \"github:gisikw/home-config\"`)\n   - Exports those inputs for hosts to consume\n\n2. Update host flakes to `follows` cluster flake instead of root directly\n\n3. Update `just deploy` / `just test` if needed to evaluate from the right entry point\n\n## Motivation\n\n- Cluster-level inputs stay in cluster config (consistent with domain, principals being cluster-level)\n- Enables home-manager configs to be specified per-cluster without hardcoding in root\n- Cleaner separation: root = shared infra, cluster = \"my stuff\"","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-31T04:56:47.845146453Z","updated_at":"2025-12-31T07:18:05.253440738Z","closed_at":"2025-12-31T07:18:05.253440738Z","close_reason":"Closed"}
{"id":"fort-69s","title":"Create pkgs/claude-code-ui derivation for @siteboon/claude-code-ui","description":"Replace runtime npm install with proper Nix derivation using buildNpmPackage or similar","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-21T21:23:27.494844-06:00","updated_at":"2025-12-21T22:17:42.510421-06:00","closed_at":"2025-12-21T22:17:42.510421-06:00","close_reason":"Derivation written and evaluates cleanly - runtime verification deferred to deploy"}
{"id":"fort-6g9","title":"Agent debug capabilities","description":"Capabilities to enable agent-driven debug loops without SSH access:\n\n- deploy: trigger comin for on-demand deploys to forge/beacon\n- journal: fetch journalctl output for a unit\n- restart: restart a systemd unit\n\nThis enables a tight debug loop:\n1. Push fix\n2. fort-agent-call \u003chost\u003e deploy\n3. fort-agent-call \u003chost\u003e journal '{\"unit\": \"...\", \"lines\": 50}'\n4. Iterate\n\nRemoves human-in-loop for forge/beacon deploys while keeping them off automatic GitOps.","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-31T20:23:25.759203034Z","updated_at":"2025-12-31T21:15:55.721555364Z","closed_at":"2025-12-31T21:15:55.721555364Z","close_reason":"Closed"}
{"id":"fort-6g9.1","title":"deploy capability (trigger comin)","description":"Handler that triggers comin for on-demand deploy.\n\nRequest: {} (no params needed)\nResponse: { status: \"triggered\" } or error\n\nImplementation: systemctl start comin (or comin fetch \u0026\u0026 comin apply)\n\nShould be available on all hosts but particularly useful for forge/beacon\nwhich stay off automatic GitOps.\n\nRBAC: Only dev-sandbox principal should be able to call this.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-31T20:23:43.709735031Z","updated_at":"2025-12-31T21:11:55.551580922Z","closed_at":"2025-12-31T21:11:55.551580922Z","close_reason":"Closed","dependencies":[{"issue_id":"fort-6g9.1","depends_on_id":"fort-6g9","type":"parent-child","created_at":"2025-12-31T20:23:43.719886214Z","created_by":"daemon"}]}
{"id":"fort-6g9.2","title":"journal capability","description":"Handler that returns journalctl output for a unit.\n\nRequest: { unit: \"fort-agent\", lines: 50, since: \"5 min ago\" }\nResponse: { lines: [...] } or { output: \"...\" }\n\nImplementation: journalctl -u \u003cunit\u003e -n \u003clines\u003e --since \u003csince\u003e --no-pager\n\nRBAC: Only dev-sandbox principal should be able to call this.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-31T20:23:53.630208397Z","updated_at":"2025-12-31T21:15:47.610458881Z","closed_at":"2025-12-31T21:15:47.610458881Z","close_reason":"Closed","dependencies":[{"issue_id":"fort-6g9.2","depends_on_id":"fort-6g9","type":"parent-child","created_at":"2025-12-31T20:23:53.640100165Z","created_by":"daemon"}]}
{"id":"fort-6g9.3","title":"restart capability","description":"Handler that restarts a systemd unit.\n\nRequest: { unit: \"fort-agent\" }\nResponse: { status: \"restarted\" } or error\n\nImplementation: systemctl restart \u003cunit\u003e\n\nShould validate unit name against allowlist to prevent arbitrary service restarts.\n\nRBAC: Only dev-sandbox principal should be able to call this.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-31T20:23:53.695205024Z","updated_at":"2025-12-31T21:15:47.659102642Z","closed_at":"2025-12-31T21:15:47.659102642Z","close_reason":"Closed","dependencies":[{"issue_id":"fort-6g9.3","depends_on_id":"fort-6g9","type":"parent-child","created_at":"2025-12-31T20:23:53.696103738Z","created_by":"daemon"}]}
{"id":"fort-6yr","title":"Skill: add-aspect workflow","description":"## Context\nAspects are reusable cross-cutting concerns (mesh, observable, egress-vpn, etc.). Adding one involves understanding parameterized vs simple aspects, the aspects/ directory structure, and how they integrate with host manifests.\n\n## Proposed skill: `add-aspect`\n\nProgressive disclosure skill for creating new aspects.\n\n### Content to extract from AGENTS.md\n- \"Parameterized Aspects\" section (lines 202-214)\n- Aspect directory structure from codebase navigation\n\n### Skill structure\n```\n.claude/skills/add-aspect/\n├── SKILL.md              # When to use aspects vs apps, structure overview\n├── template.nix          # Base aspect template\n└── examples.md           # Links to mesh, egress-vpn, zigbee2mqtt\n```\n\n### Definition of done\n- [ ] Skill created in .claude/skills/add-aspect/\n- [ ] AGENTS.md parameterized aspects section reduced to brief pointer\n- [ ] Tested: skill loads when creating new aspects\n\n## Labels\ndx","status":"open","priority":3,"issue_type":"feature","created_at":"2025-12-31T04:25:11.81763913Z","updated_at":"2025-12-31T04:25:11.81763913Z","dependencies":[{"issue_id":"fort-6yr","depends_on_id":"fort-4ge","type":"blocks","created_at":"2026-01-13T16:30:59.760720218Z","created_by":"daemon"}]}
{"id":"fort-74l","title":"Gatus: Support health checks for auth-protected services","description":"Currently sillytavern (basicAuth) and mcp (API auth) show 401 in Gatus. Options:\n- Add Gatus basic-auth support to health config schema\n- Use internal endpoints that bypass auth\n- TCP port checks instead of HTTP","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-14T02:15:03.827326591Z","updated_at":"2026-01-14T05:34:57.083807825Z","closed_at":"2026-01-14T05:34:57.083807825Z","close_reason":"Closed"}
{"id":"fort-7nv","title":"Triggerable gitops deploys for forge/beacon","description":"Currently forge (drhorrible) and beacon (raishan) require manual deploys via `just deploy`, which is getting onerous. These hosts are excluded from auto-deploy because we don't want them deploying willy-nilly on every push.\n\nConsider options for triggerable gitops-based deploys:\n- Manual trigger in CI (workflow_dispatch or similar)\n- Approval-gated deploys (require human click before deploy runs)\n- Separate deploy branch that only updates on explicit action\n- Webhook-triggered deploys from a trusted source\n- Combination: auto-deploy on release, but forge/beacon require approval step\n\nConstraints:\n- Must not auto-deploy on every push to main/release\n- Should still go through CI validation\n- Ideally integrates with existing comin/gitops infrastructure\n- Need to maintain ability to quickly deploy in emergencies","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-31T18:58:56.836896359Z","updated_at":"2026-01-09T14:45:40.169181911Z","closed_at":"2026-01-09T14:45:40.169181911Z","close_reason":"Closed"}
{"id":"fort-815","title":"Deploy radicale on ratched","description":"Add radicale to ratched's manifest:\n\n1. Add `\"radicale\"` to apps array in `clusters/bedlam/hosts/ratched/manifest.nix`\n2. Deploy and verify service starts\n3. Test web interface is accessible at `calendar.\u003cdomain\u003e`\n4. Verify CalDAV endpoint responds to requests\n\nDepends on: fort-nyj (app module), fort-1rf (htpasswd secret)\n\nParent epic: fort-zye","status":"closed","priority":3,"issue_type":"task","created_at":"2026-01-15T14:53:33.683884446Z","updated_at":"2026-01-15T15:18:50.55993501Z","closed_at":"2026-01-15T15:18:50.55993501Z","close_reason":"Closed","dependencies":[{"issue_id":"fort-815","depends_on_id":"fort-nyj","type":"blocks","created_at":"2026-01-15T14:53:49.254161802Z","created_by":"daemon"},{"issue_id":"fort-815","depends_on_id":"fort-1rf","type":"blocks","created_at":"2026-01-15T14:53:49.302210936Z","created_by":"daemon"},{"issue_id":"fort-815","depends_on_id":"fort-zye","type":"parent-child","created_at":"2026-01-15T14:55:27.912126371Z","created_by":"daemon"}]}
{"id":"fort-89e","title":"Unified runtime control plane","description":"Replace the current fragmented credential delivery and service discovery mechanisms with a unified agent-based control plane.\n\n## Design Document\n\nSee `docs/control-plane-design.md` for full technical design.\n\n## Architecture Summary\n\n**Agent:** Generic capability-exposure mechanism. Every host runs one. CGI handlers behind nginx at `/agent/*`. Auth via SSH signatures, RBAC from cluster topology. All POST. Handle/TTL in response headers for GC.\n\n**Fulfillment:** How hosts resolve their needs at activation. `fort-fulfill.service` reads `needs.json`, calls providers, stores results. Best-effort (doesn't block deploy). Timer retries failures.\n\n**Holdings Protocol:** Distributed GC. Providers return handles, callers advertise them via `/agent/holdings`. Positive absence triggers cleanup.\n\n**Two providers:**\n- **Forge** (drhorrible): Identity \u0026 secrets - OIDC registration, SSL certs, git tokens\n- **Beacon** (raishan): Network edge - public proxy config\n\n## Nix Abstractions\n\n- `fort.needs.*` - Apps declare what they need, system generates needs.json\n- `fort.capabilities.*` - Providers declare what they expose, system generates handlers + RBAC\n- `needsGC = true` - Auto-wires handle headers and GC timer\n\n## Key Decisions\n\n- Hosts pull from providers (not push-based)\n- Deploy resilience: fulfillment is best-effort, never blocks deploy\n- CGI-style handlers (bash scripts, upgradeable to Go/Rust per-endpoint)\n- Holdings protocol for distributed GC (two-generals safe)\n\n## Related Tickets\n\n- fort-c33: Consolidate fortCluster options under fort.cluster (absorb into this epic)\n- fort-0rj: Group-based OIDC client restrictions (adjacent, not blocking)\n- fort-bkv: Typo bug in outline tmpfiles (quick fix)\n\n## Migration Path\n\n1. Add agent to all hosts (mandatory endpoints: status, manifest, holdings, release)\n2. Add forge capabilities (oidc-register, ssl-cert, git-token)\n3. Add beacon capabilities (proxy-configure)\n4. Add fort-fulfill.service\n5. Run in parallel with existing SSH-based mechanisms\n6. Validate all coordination patterns\n7. Remove SSH-based delivery (service-registry, acme-sync, token-sync)\n8. Enable GC sweeps\n\n## Audit Notes (from codebase review)\n\n- Only outline uses sso.mode=\"oidc\" directly (forgejo has custom setup)\n- 17 apps use custom subdomains (goes in request field)\n- Path consolidation needed: various /var/lib/fort-* paths → /var/lib/fort/\n- Current beacon is passive (receives nginx config via SCP) - needs real provider implementation","status":"closed","priority":3,"issue_type":"epic","created_at":"2025-12-28T05:10:28.14077079Z","updated_at":"2026-01-08T04:08:00.241167217Z","closed_at":"2026-01-08T04:08:00.241167217Z","close_reason":"Superseded by fort-c8y"}
{"id":"fort-89e.1","title":"Agent Nix module skeleton","description":"Create aspects/fort-agent/ module structure:\n- Generate /etc/fort-agent/ directory (handlers/, rbac.json, hosts.json with peer public keys)\n- Nginx location for /agent/* → FastCGI socket\n- Systemd socket activation for wrapper\n- Wire host SSH keys from cluster topology into hosts.json\n\nThis is the foundation - no actual wrapper implementation yet, just scaffolding.","acceptance_criteria":"- /etc/fort-agent/ structure exists on hosts with fort-agent enabled\n- nginx routes /agent/* to FastCGI socket\n- hosts.json contains peer public keys from cluster topology","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-30T22:00:42.251940795Z","updated_at":"2025-12-30T22:20:18.08353589Z","closed_at":"2025-12-30T22:20:18.08353589Z","close_reason":"Closed","dependencies":[{"issue_id":"fort-89e.1","depends_on_id":"fort-89e","type":"parent-child","created_at":"2025-12-30T22:00:42.25300164Z","created_by":"daemon"}]}
{"id":"fort-89e.10","title":"Migrate dev-sandbox git access to control plane","description":"Switch git token distribution from timer-push to agent-pull:\n\n1. Add fort.needs.git-token to dev-sandbox aspect\n2. Store token at /var/lib/fort-git/forge-token (existing location)\n3. Verify git operations work\n4. Remove forgejo-deploy-token-sync timer from forgejo app\n\nShould be straightforward - same pattern as cert migration.","acceptance_criteria":"- dev-sandbox hosts receive git tokens via control plane\n- Git clone/push works with new token delivery\n- forgejo-deploy-token-sync removed","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-30T22:03:38.868657192Z","updated_at":"2026-01-06T05:41:48.788814773Z","closed_at":"2026-01-06T05:41:48.788814773Z","close_reason":"Closed","dependencies":[{"issue_id":"fort-89e.10","depends_on_id":"fort-89e","type":"parent-child","created_at":"2025-12-30T22:03:38.879028619Z","created_by":"daemon"},{"issue_id":"fort-89e.10","depends_on_id":"fort-89e.9","type":"blocks","created_at":"2025-12-30T22:07:01.056221502Z","created_by":"daemon"},{"issue_id":"fort-89e.10","depends_on_id":"fort-89e.7","type":"blocks","created_at":"2025-12-30T22:07:01.106484983Z","created_by":"daemon"}]}
{"id":"fort-89e.11","title":"attic-token capability","description":"Handler on forge that creates/returns Attic cache tokens:\n\nRequest: { host: \"ursula\" }\nResponse: { token: \"...\" }\nHandle: yes (for GC)\n\nUses attic CLI to create token with appropriate permissions.\nReplaces current token distribution in attic bootstrap.","acceptance_criteria":"- Handler creates attic token via CLI\n- Returns handle for GC tracking\n- Token has correct cache permissions","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-30T22:03:54.859465037Z","updated_at":"2026-01-08T04:07:50.582218169Z","closed_at":"2026-01-08T04:07:50.582218169Z","close_reason":"Superseded by fort-c8y","dependencies":[{"issue_id":"fort-89e.11","depends_on_id":"fort-89e","type":"parent-child","created_at":"2025-12-30T22:03:54.868383319Z","created_by":"daemon"},{"issue_id":"fort-89e.11","depends_on_id":"fort-89e.4","type":"blocks","created_at":"2025-12-30T22:07:01.158562815Z","created_by":"daemon"},{"issue_id":"fort-89e.11","depends_on_id":"fort-89e.5","type":"blocks","created_at":"2025-12-30T22:07:01.209230946Z","created_by":"daemon"}]}
{"id":"fort-89e.12","title":"Migrate attic token distribution to control plane","description":"Switch attic token distribution to agent-pull:\n\n1. Add fort.needs.attic-token to relevant hosts/aspects\n2. Store token at existing location\n3. Verify nix builds can push to cache\n4. Remove old distribution mechanism from attic app\n\nReview apps/attic/ to identify current distribution pattern.","acceptance_criteria":"- Hosts receive attic tokens via control plane\n- Nix builds successfully push to cache\n- Old distribution mechanism removed","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-30T22:04:21.39373523Z","updated_at":"2026-01-08T04:07:50.583453367Z","closed_at":"2026-01-08T04:07:50.583453367Z","close_reason":"Superseded by fort-c8y","dependencies":[{"issue_id":"fort-89e.12","depends_on_id":"fort-89e","type":"parent-child","created_at":"2025-12-30T22:04:21.403778529Z","created_by":"daemon"},{"issue_id":"fort-89e.12","depends_on_id":"fort-89e.11","type":"blocks","created_at":"2025-12-30T22:07:01.255367354Z","created_by":"daemon"},{"issue_id":"fort-89e.12","depends_on_id":"fort-89e.7","type":"blocks","created_at":"2025-12-30T22:07:01.30586868Z","created_by":"daemon"}]}
{"id":"fort-89e.13","title":"oidc-register capability","description":"Handler on forge that registers OIDC clients in pocket-id:\n\nRequest: { service: \"outline\", fqdn: \"outline.example.com\" }\nResponse: { client_id: \"...\", client_secret: \"...\" }\nHandle: yes (client ID for GC)\n\nUses pocket-id API via service key.\nReplaces create_pocketid_client logic in service-registry.\n\nRBAC: all hosts (simplest) or hosts with SSO-enabled services.","acceptance_criteria":"- Handler creates OIDC client via pocket-id API\n- Returns client_id and client_secret\n- Returns handle for GC tracking","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-30T22:04:44.908884915Z","updated_at":"2026-01-08T04:07:50.58472719Z","closed_at":"2026-01-08T04:07:50.58472719Z","close_reason":"Superseded by fort-c8y","dependencies":[{"issue_id":"fort-89e.13","depends_on_id":"fort-89e","type":"parent-child","created_at":"2025-12-30T22:04:44.918284702Z","created_by":"daemon"},{"issue_id":"fort-89e.13","depends_on_id":"fort-89e.4","type":"blocks","created_at":"2025-12-30T22:07:01.484507416Z","created_by":"daemon"},{"issue_id":"fort-89e.13","depends_on_id":"fort-89e.5","type":"blocks","created_at":"2025-12-30T22:07:01.534872836Z","created_by":"daemon"}]}
{"id":"fort-89e.14","title":"Migrate outline to control plane OIDC","description":"First OIDC consumer migration:\n\n1. Add fort.needs.oidc.outline to outline app\n2. Store at /var/lib/fort/oidc/outline/{client-id,client-secret}\n3. Update outline config to read from new location (or symlink)\n4. Verify outline SSO login works\n\nThis validates the OIDC flow end-to-end before migrating other services.","acceptance_criteria":"- Outline receives OIDC creds via control plane\n- SSO login works\n- No manual credential setup required","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-30T22:04:48.686666459Z","updated_at":"2026-01-08T04:07:50.58605011Z","closed_at":"2026-01-08T04:07:50.58605011Z","close_reason":"Superseded by fort-c8y","dependencies":[{"issue_id":"fort-89e.14","depends_on_id":"fort-89e","type":"parent-child","created_at":"2025-12-30T22:04:48.69625279Z","created_by":"daemon"},{"issue_id":"fort-89e.14","depends_on_id":"fort-89e.13","type":"blocks","created_at":"2025-12-30T22:07:01.586365075Z","created_by":"daemon"},{"issue_id":"fort-89e.14","depends_on_id":"fort-89e.7","type":"blocks","created_at":"2025-12-30T22:07:01.638478587Z","created_by":"daemon"}]}
{"id":"fort-89e.15","title":"Holdings management and GC foundation","description":"Infrastructure for distributed GC:\n\n1. /var/lib/fort/holdings.json structure: { handles: [\"sha256:...\", ...] }\n2. Helper scripts: fort-holdings-add, fort-holdings-remove\n3. Wire holdings endpoint to return this file\n4. Release endpoint (self-release mode):\n   - Remove handles from holdings.json\n   - Notify relevant providers\n\nProvider-side GC (checking holdings, sweeping orphans) is separate ticket.","acceptance_criteria":"- holdings.json maintained correctly\n- Holdings endpoint returns current handles\n- Release endpoint removes handles and notifies providers","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-30T22:04:52.341823463Z","updated_at":"2026-01-08T04:07:50.587292963Z","closed_at":"2026-01-08T04:07:50.587292963Z","close_reason":"Superseded by fort-c8y","dependencies":[{"issue_id":"fort-89e.15","depends_on_id":"fort-89e","type":"parent-child","created_at":"2025-12-30T22:04:52.351364146Z","created_by":"daemon"},{"issue_id":"fort-89e.15","depends_on_id":"fort-89e.4","type":"blocks","created_at":"2025-12-30T22:07:01.690716942Z","created_by":"daemon"}]}
{"id":"fort-89e.16","title":"proxy-configure capability","description":"Handler on beacon that configures nginx reverse proxy:\n\nRequest: { service: \"outline\", fqdn: \"outline.example.com\", upstream: \"10.0.0.5:4654\" }\nResponse: { configured: true }\nNo handle - beacon maintains its own nginx state.\n\nGenerates server block, writes to /var/lib/fort/nginx/services/\u003cservice\u003e.conf, reloads nginx.\nReplaces public_vhosts logic in service-registry.\n\n2xx = 'I have configured the proxy for this service'","acceptance_criteria":"- Handler generates valid nginx config\n- nginx reloads successfully\n- Service is accessible through beacon","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-30T22:04:56.988134132Z","updated_at":"2026-01-08T04:07:50.588479155Z","closed_at":"2026-01-08T04:07:50.588479155Z","close_reason":"Superseded by fort-c8y","dependencies":[{"issue_id":"fort-89e.16","depends_on_id":"fort-89e","type":"parent-child","created_at":"2025-12-30T22:04:57.005270277Z","created_by":"daemon"},{"issue_id":"fort-89e.16","depends_on_id":"fort-89e.4","type":"blocks","created_at":"2025-12-30T22:07:01.74446846Z","created_by":"daemon"},{"issue_id":"fort-89e.16","depends_on_id":"fort-89e.5","type":"blocks","created_at":"2025-12-30T22:07:01.794416255Z","created_by":"daemon"}]}
{"id":"fort-89e.17","title":"Wire exposedServices.visibility=public to proxy needs","description":"Auto-generate fort.needs.proxy from exposedServices:\n\nWhen a service declares visibility = 'public':\n- Generate fort.needs.proxy.\u003cservice\u003e automatically\n- Provider: beacon host\n- Request: { service, fqdn, upstream }\n\nThis replaces the service-registry scan that finds public services and pushes nginx config.\n\nImplementation in common/fort.nix or related module.","acceptance_criteria":"- Public services automatically get proxy needs generated\n- No manual fort.needs.proxy declaration required\n- Proxy configuration happens via fulfillment","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-30T22:05:29.325064741Z","updated_at":"2026-01-08T04:07:50.589668824Z","closed_at":"2026-01-08T04:07:50.589668824Z","close_reason":"Superseded by fort-c8y","dependencies":[{"issue_id":"fort-89e.17","depends_on_id":"fort-89e","type":"parent-child","created_at":"2025-12-30T22:05:29.334141293Z","created_by":"daemon"},{"issue_id":"fort-89e.17","depends_on_id":"fort-89e.16","type":"blocks","created_at":"2025-12-30T22:07:01.844268093Z","created_by":"daemon"},{"issue_id":"fort-89e.17","depends_on_id":"fort-89e.7","type":"blocks","created_at":"2025-12-30T22:07:01.891696903Z","created_by":"daemon"}]}
{"id":"fort-89e.18","title":"DNS update capabilities","description":"Handlers for DNS record management:\n\n1. headscale-dns (beacon): Update /var/lib/headscale/extra-records.json\n   - Request: { records: [{name, type, value}, ...] }\n   - Manages MagicDNS entries for VPN-internal resolution\n\n2. coredns-update (forge): Update /var/lib/coredns/custom.conf  \n   - Request: { records: [{ip, fqdn}, ...] }\n   - Manages LAN DNS entries\n\nThese replace the DNS update logic in service-registry.\nMay need to rethink: currently service-registry computes all records centrally.\nWith pull model, each host would declare its own records? Or forge aggregates?","design":"Open question: per-host DNS declarations vs centralized computation. \nCurrent model scans all hosts and computes full record set.\nPull model options:\nA) Each host declares its DNS records as needs (inverted - host pushes to DNS provider)\nB) DNS providers poll hosts for their desired records\nC) Hybrid - fulfillment on DNS hosts pulls manifest from each host\n\nLeaning toward (A) with 'dns-register' capability - host says 'please add this A record'.","acceptance_criteria":"- DNS records updated via control plane\n- Both headscale and coredns records managed\n- Old service-registry DNS logic removable","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-30T22:06:04.752994195Z","updated_at":"2026-01-08T04:07:50.590861939Z","closed_at":"2026-01-08T04:07:50.590861939Z","close_reason":"Superseded by fort-c8y","dependencies":[{"issue_id":"fort-89e.18","depends_on_id":"fort-89e","type":"parent-child","created_at":"2025-12-30T22:06:04.762973011Z","created_by":"daemon"},{"issue_id":"fort-89e.18","depends_on_id":"fort-89e.4","type":"blocks","created_at":"2025-12-30T22:07:02.080757126Z","created_by":"daemon"},{"issue_id":"fort-89e.18","depends_on_id":"fort-89e.5","type":"blocks","created_at":"2025-12-30T22:07:02.132473693Z","created_by":"daemon"}]}
{"id":"fort-89e.19","title":"Remove service-registry aspect","description":"After all functionality migrated to control plane:\n\n1. Verify all consumers working via new system:\n   - OIDC registration ✓\n   - Proxy configuration ✓\n   - DNS updates ✓\n2. Remove aspects/service-registry/ entirely\n3. Remove registry.rb and related systemd units\n4. Clean up any references in host manifests\n\nThis is the 'flip the switch' moment for the control plane.","acceptance_criteria":"- service-registry aspect deleted\n- No regressions in OIDC, proxy, or DNS functionality\n- All coordination happens via agent calls","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-30T22:06:08.611599466Z","updated_at":"2026-01-08T04:07:50.59201709Z","closed_at":"2026-01-08T04:07:50.59201709Z","close_reason":"Superseded by fort-c8y","dependencies":[{"issue_id":"fort-89e.19","depends_on_id":"fort-89e","type":"parent-child","created_at":"2025-12-30T22:06:08.62185879Z","created_by":"daemon"},{"issue_id":"fort-89e.19","depends_on_id":"fort-89e.8","type":"blocks","created_at":"2025-12-30T22:07:02.181886646Z","created_by":"daemon"},{"issue_id":"fort-89e.19","depends_on_id":"fort-89e.14","type":"blocks","created_at":"2025-12-30T22:07:02.229839078Z","created_by":"daemon"},{"issue_id":"fort-89e.19","depends_on_id":"fort-89e.17","type":"blocks","created_at":"2025-12-30T22:07:02.280299263Z","created_by":"daemon"},{"issue_id":"fort-89e.19","depends_on_id":"fort-89e.18","type":"blocks","created_at":"2025-12-30T22:07:02.331257531Z","created_by":"daemon"}]}
{"id":"fort-89e.2","title":"Go FastCGI wrapper","description":"Implement pkgs/fort-agent-wrapper/main.go (~300 lines):\n- Parse X-Fort-Origin, X-Fort-Timestamp, X-Fort-Signature headers\n- Verify SSH signature against /etc/fort-agent/hosts.json\n- Timestamp validation (reject if \u003e5min drift)\n- Check RBAC against /etc/fort-agent/rbac.json\n- Exec handler script from /etc/fort-agent/handlers/\n- Capture stdout as response body\n- Pass through X-Fort-Handle / X-Fort-TTL headers from handler\n\nSignature format: sign(method + path + timestamp + sha256(body)) using ssh-keygen -Y sign format.","acceptance_criteria":"- Wrapper authenticates requests using SSH signatures\n- Invalid signatures return 401\n- RBAC violations return 403\n- Valid requests dispatch to handler and return response","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-30T22:00:46.237689762Z","updated_at":"2025-12-31T03:29:38.094158939Z","closed_at":"2025-12-31T03:29:38.094158939Z","close_reason":"Closed","dependencies":[{"issue_id":"fort-89e.2","depends_on_id":"fort-89e","type":"parent-child","created_at":"2025-12-30T22:00:46.246967447Z","created_by":"daemon"},{"issue_id":"fort-89e.2","depends_on_id":"fort-89e.1","type":"blocks","created_at":"2025-12-30T22:07:00.189152949Z","created_by":"daemon"}]}
{"id":"fort-89e.20","title":"Enable GC sweeps","description":"Provider-side garbage collection:\n\n1. Add GC timer to forge (and beacon if needed)\n2. For each capability with needsGC:\n   - Get list of issued handles from provider state\n   - For each handle, check holder's /agent/holdings\n   - If 200 + handle absent: mark eligible (after grace period)\n   - If 200 + handle present: still in use\n   - If error: assume still in use (two-generals safe)\n3. Sweep eligible handles:\n   - OIDC: delete pocket-id client\n   - Git tokens: revoke Forgejo token\n   - Attic tokens: revoke attic token\n\nGrace period: configurable, default 1 hour?","acceptance_criteria":"- GC timer runs on providers\n- Orphan credentials identified correctly\n- Credentials revoked after grace period\n- No false positives (never revoke in-use credentials)","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-30T22:06:15.47832576Z","updated_at":"2026-01-08T04:07:50.593157252Z","closed_at":"2026-01-08T04:07:50.593157252Z","close_reason":"Superseded by fort-c8y","dependencies":[{"issue_id":"fort-89e.20","depends_on_id":"fort-89e","type":"parent-child","created_at":"2025-12-30T22:06:15.48732162Z","created_by":"daemon"},{"issue_id":"fort-89e.20","depends_on_id":"fort-89e.15","type":"blocks","created_at":"2025-12-30T22:07:02.383836902Z","created_by":"daemon"},{"issue_id":"fort-89e.20","depends_on_id":"fort-89e.19","type":"blocks","created_at":"2025-12-30T22:07:02.437329275Z","created_by":"daemon"}]}
{"id":"fort-89e.21","title":"Large file transfer protocol","description":"Design and implement secure large file transfer over control plane.\n\nPrimary use case: Media ingestion - 'please put this 100GB file on the NAS'\n\nThe agent protocol is request/response JSON, not suitable for large transfers.\nNeed a separate protocol built on top of the agent:\n\nOptions to explore:\n1. Agent returns a ticket (nonce + port + one-time TLS cert), separate listener accepts upload\n2. Agent returns a signed URL for direct upload to destination\n3. Agent coordinates rsync/rclone with pre-shared credentials\n\nConsiderations:\n- Must be encrypted in transit\n- Must handle connection failures gracefully\n- Must not exhaust file descriptors with unclaimed transfers\n- Timeout/cleanup for abandoned transfers\n\nThis is a design + implementation ticket - flesh out the approach before building.","design":"Deferred until core control plane is working.\nStart with design doc exploring the options above.\nMay want to prototype with a simple use case before full implementation.","acceptance_criteria":"- Design document with chosen approach\n- Working implementation for file upload to NAS\n- Handles failures gracefully (timeouts, cleanup)\n- No resource leaks","status":"closed","priority":3,"issue_type":"task","created_at":"2025-12-30T22:06:45.735240703Z","updated_at":"2026-01-08T04:07:50.594447818Z","closed_at":"2026-01-08T04:07:50.594447818Z","close_reason":"Superseded by fort-c8y","dependencies":[{"issue_id":"fort-89e.21","depends_on_id":"fort-89e","type":"parent-child","created_at":"2025-12-30T22:06:45.744503456Z","created_by":"daemon"},{"issue_id":"fort-89e.21","depends_on_id":"fort-89e.19","type":"blocks","created_at":"2025-12-30T22:07:02.48988502Z","created_by":"daemon"}]}
{"id":"fort-89e.22","title":"fort-agent-call: Remove hardcoded values, improve handle output","description":"Issues from code review:\n\n1. **Hardcoded domain**: Line 25 defaults to gisi.network - should either:\n   - Inject at derivation time from cluster settings (preferred)\n   - Read from /var/lib/fort/cluster.json at runtime\n   - Error if FORT_DOMAIN not set (no silent defaults)\n\n2. **Hardcoded example in usage**: 'drhorrible' shouldn't be in the script\n\n3. **stderr for handle/TTL is awkward**: Outputting FORT_HANDLE=... to stderr requires callers to do capture gymnastics. Consider:\n   - JSON envelope on stdout: { \"body\": ..., \"handle\": ..., \"ttl\": ... }\n   - Separate file output (--handle-file flag)\n   - Environment file output that can be sourced\n\nDerivation-time injection is cleanest - the script becomes cluster-specific at build time.","acceptance_criteria":"- No hardcoded domain or hostname examples\n- Handle/TTL available without stderr parsing\n- Works correctly with fulfillment service","status":"closed","priority":2,"issue_type":"bug","created_at":"2025-12-31T02:30:27.203801144Z","updated_at":"2025-12-31T02:53:23.224961867Z","closed_at":"2025-12-31T02:53:23.224961867Z","close_reason":"Closed","dependencies":[{"issue_id":"fort-89e.22","depends_on_id":"fort-89e","type":"parent-child","created_at":"2025-12-31T02:30:27.205166856Z","created_by":"daemon"}]}
{"id":"fort-89e.23","title":"fort-agent.nix: Remove hardcoded capability-to-need mapping","description":"Code review issue: capabilityToNeedType mapping on lines 49-55 hardcodes the relationship between capabilities and need types. This is backwards - specifics shouldn't be baked into the abstraction.\n\nCurrent (bad):\n```nix\ncapabilityToNeedType = {\n  \"oidc-register\" = \"oidc\";\n  \"ssl-cert\" = \"ssl\";\n  ...\n};\n```\n\nFix: Capabilities should declare what they satisfy:\n```nix\nfort.capabilities.ssl-cert = {\n  handler = ./handlers/ssl-cert;\n  satisfies = \"ssl\";  # Links to fort.needs.ssl.*\n  needsGC = false;\n};\n```\n\nThis keeps the abstraction clean - new capabilities don't require editing a central lookup table.\n\nAlso affects needs.json generation (line 127) which assumes capability = \"${needType}-register\" - wrong for ssl-cert, etc.","acceptance_criteria":"- No hardcoded capability/need mappings\n- Capabilities declare their own 'satisfies' type\n- needs.json correctly references actual capability names","status":"closed","priority":2,"issue_type":"bug","created_at":"2025-12-31T02:30:50.382546326Z","updated_at":"2025-12-31T02:53:23.280775797Z","closed_at":"2025-12-31T02:53:23.280775797Z","close_reason":"Closed","dependencies":[{"issue_id":"fort-89e.23","depends_on_id":"fort-89e","type":"parent-child","created_at":"2025-12-31T02:30:50.392900433Z","created_by":"daemon"}]}
{"id":"fort-89e.24","title":"Dev-sandbox agent identity for direct agent calls","description":"Enable dev-sandbox users to make fort-agent-call directly (not via sudo).\n\nCurrent state:\n- fort-agent-call signs with /etc/ssh/ssh_host_ed25519_key (root-only)\n- dev user cannot access this key\n- Agent calls from dev-sandbox require root\n\nProposed solution:\n1. Generate a dedicated agent key for dev-sandbox at /var/lib/fort/dev-sandbox/agent-key\n2. Make it readable by the dev user (or dev group)\n3. Extend hosts.json generation to include this key under identity \"dev-sandbox\" or similar\n4. Set FORT_SSH_KEY and FORT_ORIGIN env vars in dev-sandbox shell profile\n\nThis introduces non-host principals to the agent RBAC system. The dev-sandbox would authenticate as its own identity, not as the host.\n\nConsiderations:\n- RBAC implications: should dev-sandbox have same access as the host, or restricted?\n- Key generation: at host activation time, or as part of dev-sandbox aspect?\n- Identity naming: \"dev-sandbox\", \"ratched-dev\", or principal-based?","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-31T04:22:41.789548078Z","updated_at":"2025-12-31T05:38:17.420386812Z","closed_at":"2025-12-31T05:38:17.420386812Z","close_reason":"Closed","dependencies":[{"issue_id":"fort-89e.24","depends_on_id":"fort-89e","type":"parent-child","created_at":"2025-12-31T04:22:41.791197708Z","created_by":"daemon"}]}
{"id":"fort-89e.25","title":"Proxy vhost handles for GC-only lifecycle","description":"Public proxy vhosts need handle-based lifecycle management even though the requester has no credential to act on.\n\n## Context\n\nWhen a host declares `fort.cluster.services` with `visibility = \"public\"`, it needs proxy config from the beacon. The beacon creates an nginx vhost, but unlike OIDC credentials or SSL certs, there's nothing the requester *does* with this - they just need the vhost to exist.\n\nHowever, the handle mechanism is still essential for GC:\n- Host requests proxy config → beacon creates vhost, returns handle\n- Host stores handle in holdings\n- Host is decommissioned (or service removed) → handle no longer advertised\n- Beacon's GC sweep detects missing handle → cleans up orphaned vhost\n\n## Design Questions\n\n1. Should proxy-configure return a handle even when there's no credential payload?\n2. How does fort-fulfill store a \"GC-only\" handle that has no transform/destination?\n3. Should this be a distinct need type (e.g., `fort.needs.proxy` vs `fort.needs.oidc`)?\n\n## Relationship to fort-89e.16/17\n\nThis is orthogonal to implementing the proxy-configure capability itself - it's about ensuring the handle lifecycle is wired correctly for the GC-only case.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-07T02:08:34.957567488Z","updated_at":"2026-01-08T04:07:50.595723716Z","closed_at":"2026-01-08T04:07:50.595723716Z","close_reason":"Superseded by fort-c8y","dependencies":[{"issue_id":"fort-89e.25","depends_on_id":"fort-89e","type":"parent-child","created_at":"2026-01-07T02:08:34.958723859Z","created_by":"dev"}]}
{"id":"fort-89e.3","title":"fort-agent-call client","description":"Bash script in pkgs/fort-agent-call/:\n- Sign request body with host's SSH key (ssh-keygen -Y sign)\n- Build canonical string: METHOD\\nPATH\\nTIMESTAMP\\nSHA256(body)\n- POST via curl with X-Fort-Origin, X-Fort-Timestamp, X-Fort-Signature headers\n- Parse X-Fort-Handle and X-Fort-TTL from response headers\n- Expose handle via FORT_HANDLE env var or stdout marker\n- Exit codes: 0=success, 1=http error, 2=auth error\n\nUsage: fort-agent-call \u003chost\u003e \u003ccapability\u003e [request-json]","acceptance_criteria":"- Can successfully call agent endpoints on other hosts\n- Properly signs requests\n- Extracts handle from response headers\n- Clear error codes for different failure modes","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-30T22:00:55.859422495Z","updated_at":"2025-12-30T22:17:25.627065661Z","closed_at":"2025-12-30T22:17:25.627065661Z","close_reason":"Closed","dependencies":[{"issue_id":"fort-89e.3","depends_on_id":"fort-89e","type":"parent-child","created_at":"2025-12-30T22:00:55.868811639Z","created_by":"daemon"}]}
{"id":"fort-89e.4","title":"Mandatory agent endpoints","description":"Implement mandatory endpoints as bash handlers:\n- status: return {version, uptime, hostname}\n- manifest: return contents of /var/lib/fort/host-manifest.json\n- holdings: return contents of /var/lib/fort/holdings.json\n\nThese have no RBAC (any cluster host can call). Deploy agent to forge initially for testing.\n\nNote: release endpoint deferred to GC phase.","acceptance_criteria":"- All three endpoints return valid JSON\n- Endpoints work without RBAC restrictions\n- Agent deployed and functional on forge","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-30T22:01:29.957039978Z","updated_at":"2025-12-31T04:21:58.557642595Z","closed_at":"2025-12-31T04:21:58.557642595Z","close_reason":"Closed","dependencies":[{"issue_id":"fort-89e.4","depends_on_id":"fort-89e","type":"parent-child","created_at":"2025-12-30T22:01:29.966238419Z","created_by":"daemon"},{"issue_id":"fort-89e.4","depends_on_id":"fort-89e.1","type":"blocks","created_at":"2025-12-30T22:07:00.24405932Z","created_by":"daemon"},{"issue_id":"fort-89e.4","depends_on_id":"fort-89e.2","type":"blocks","created_at":"2025-12-30T22:07:00.292343108Z","created_by":"daemon"},{"issue_id":"fort-89e.4","depends_on_id":"fort-89e.3","type":"blocks","created_at":"2025-12-30T22:07:00.344998156Z","created_by":"daemon"}]}
{"id":"fort-89e.5","title":"fort.needs / fort.capabilities option types","description":"Nix module options for declaring needs and capabilities:\n\nfort.needs.\u003ctype\u003e.\u003cname\u003e:\n- providers: list of hostnames\n- request: attrset passed to capability\n- store: path to store response (null = don't store)\n- restart: list of services to restart after fulfillment\n\nfort.capabilities.\u003cname\u003e:\n- handler: path to handler script\n- needsGC: bool (adds handle wrapper, GC timer)\n- description: human-readable\n\nGenerate:\n- /var/lib/fort/needs.json from all fort.needs declarations\n- /etc/fort-agent/rbac.json from capabilities + topology\n- Handler wrappers in /etc/fort-agent/handlers/\n\nMinimal implementation for slice 1 - can expand later.","acceptance_criteria":"- Options defined and documented\n- needs.json generated correctly\n- rbac.json generated from topology\n- Handlers installed in correct location","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-30T22:02:05.246179199Z","updated_at":"2025-12-30T22:23:24.084251354Z","closed_at":"2025-12-30T22:23:24.084251354Z","close_reason":"Closed","dependencies":[{"issue_id":"fort-89e.5","depends_on_id":"fort-89e","type":"parent-child","created_at":"2025-12-30T22:02:05.256271649Z","created_by":"daemon"}]}
{"id":"fort-89e.6","title":"ssl-cert capability","description":"Handler on forge that returns SSL cert files:\n\nRequest: { domain: \"example.com\" } (or could be implicit from cluster)\nResponse: { cert: \"base64...\", key: \"base64...\", chain: \"base64...\" }\n\nReads from /var/lib/acme/\u003cdomain\u003e/ (existing ACME cert location).\nNo handle needed - certs are idempotent, no GC required.\n\nDeclare via fort.capabilities.ssl-cert in certificate-broker aspect.","acceptance_criteria":"- Handler returns valid cert data as JSON\n- Works with existing ACME-managed certs\n- Callable by any cluster host","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-30T22:02:34.465157032Z","updated_at":"2025-12-31T19:35:30.199349571Z","closed_at":"2025-12-31T19:35:30.199349571Z","close_reason":"Closed","dependencies":[{"issue_id":"fort-89e.6","depends_on_id":"fort-89e","type":"parent-child","created_at":"2025-12-30T22:02:34.474996557Z","created_by":"daemon"},{"issue_id":"fort-89e.6","depends_on_id":"fort-89e.4","type":"blocks","created_at":"2025-12-30T22:07:00.535476723Z","created_by":"daemon"},{"issue_id":"fort-89e.6","depends_on_id":"fort-89e.5","type":"blocks","created_at":"2025-12-30T22:07:00.583681707Z","created_by":"daemon"}]}
{"id":"fort-89e.7","title":"fort-fulfill.service","description":"Systemd oneshot that runs on activation:\n\n1. Read /var/lib/fort/needs.json\n2. For each need where store path doesn't exist (or no handle file):\n   - Call provider via fort-agent-call\n   - On 2xx: store response at declared path, restart declared services\n   - On non-2xx: log warning, continue to next need\n3. Exit 0 even if some needs failed (best-effort)\n\nAdd fort-fulfill-retry.timer:\n- Runs every 5 minutes\n- Re-attempts any needs that haven't succeeded\n- Stops retrying once all needs fulfilled\n\nKey principle: fulfillment never blocks deploy.","acceptance_criteria":"- Service runs on activation\n- Successfully fulfilled needs have response stored\n- Failed needs logged but don't block\n- Retry timer re-attempts failures\n- Dependent services restarted after fulfillment","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-30T22:03:02.10662668Z","updated_at":"2026-01-06T03:45:59.902707176Z","closed_at":"2026-01-06T03:45:59.902707176Z","close_reason":"Closed","dependencies":[{"issue_id":"fort-89e.7","depends_on_id":"fort-89e","type":"parent-child","created_at":"2025-12-30T22:03:02.115401513Z","created_by":"daemon"},{"issue_id":"fort-89e.7","depends_on_id":"fort-89e.3","type":"blocks","created_at":"2025-12-30T22:07:00.634767132Z","created_by":"daemon"},{"issue_id":"fort-89e.7","depends_on_id":"fort-89e.5","type":"blocks","created_at":"2025-12-30T22:07:00.681840919Z","created_by":"daemon"}]}
{"id":"fort-89e.8","title":"Migrate cert distribution to control plane","description":"Switch SSL cert distribution from rsync-push to agent-pull:\n\n1. Add fort.needs.ssl.wildcard to one test host (not forge/beacon)\n2. Verify cert arrives via fulfillment and nginx reloads\n3. Expand to all hosts\n4. Remove acme-sync timer from certificate-broker\n\nStorage: /var/lib/fort/ssl/\u003cdomain\u003e/{fullchain.pem,key.pem,chain.pem}\n\nThis is the first end-to-end validation of the control plane.","acceptance_criteria":"- All hosts receive certs via control plane\n- nginx reloads after cert delivery\n- acme-sync timer removed\n- No regressions in SSL functionality","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-30T22:03:06.345789733Z","updated_at":"2026-01-08T04:07:50.579937966Z","closed_at":"2026-01-08T04:07:50.579937966Z","close_reason":"Superseded by fort-c8y","dependencies":[{"issue_id":"fort-89e.8","depends_on_id":"fort-89e","type":"parent-child","created_at":"2025-12-30T22:03:06.355981588Z","created_by":"daemon"},{"issue_id":"fort-89e.8","depends_on_id":"fort-89e.6","type":"blocks","created_at":"2025-12-30T22:07:00.727131406Z","created_by":"daemon"},{"issue_id":"fort-89e.8","depends_on_id":"fort-89e.7","type":"blocks","created_at":"2025-12-30T22:07:00.780444478Z","created_by":"daemon"}]}
{"id":"fort-89e.9","title":"git-token capability","description":"Handler on forge that creates/returns Forgejo deploy tokens:\n\nRequest: { host: \"ursula\", access: \"rw\" | \"ro\" }\nResponse: { token: \"...\" }\nHandle: yes (for GC when host removed)\n\nUses Forgejo API via forge-admin service account.\nReplaces forgejo-deploy-token-sync timer logic.\n\nRBAC: hosts with dev-sandbox aspect get rw, others get ro.","acceptance_criteria":"- Handler creates deploy token via Forgejo API\n- Returns handle for GC tracking\n- Token has correct access level based on request","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-30T22:03:29.654479207Z","updated_at":"2026-01-06T04:32:37.581314202Z","closed_at":"2026-01-06T04:32:37.581314202Z","close_reason":"Closed","dependencies":[{"issue_id":"fort-89e.9","depends_on_id":"fort-89e","type":"parent-child","created_at":"2025-12-30T22:03:29.664203608Z","created_by":"daemon"},{"issue_id":"fort-89e.9","depends_on_id":"fort-89e.4","type":"blocks","created_at":"2025-12-30T22:07:00.949748126Z","created_by":"daemon"},{"issue_id":"fort-89e.9","depends_on_id":"fort-89e.5","type":"blocks","created_at":"2025-12-30T22:07:01.005000632Z","created_by":"daemon"}]}
{"id":"fort-8gl","title":"Improve deploy log visibility for agent collaboration","description":"Current friction: when agent runs deploy, token bloat + user can't review logs easily. When user runs deploy, agent can't see output. Explore options: CI/CD infra, tee to /tmp with standard filtering, or other patterns.","status":"closed","priority":3,"issue_type":"task","created_at":"2025-12-21T14:22:00.169664-06:00","updated_at":"2025-12-27T19:43:36.363584454Z","closed_at":"2025-12-27T19:43:36.363584454Z","close_reason":"Superseded by CI/CD approach"}
{"id":"fort-8i4","title":"Investigate Outline data loss on q","description":"## Problem\n\nUser's Outline data appears to be missing. Initial investigation found:\n- `/var/lib/outline/` has older data\n- `/var/lib/postgresql/` is quite recent (suggests possible reset)\n- Persistence config in `common/host.nix` looks correct (`/var/lib` → `/persist/system/var/lib`)\n\n## Possible causes to investigate\n\n1. **PostgreSQL data reset** - Version upgrade, schema migration, or other reset\n2. **OIDC identity mismatch** - Recent OIDC changes may have caused Outline to treat user as a new account (different team/workspace)\n3. **Persistence gap** - Brief period where `/var/lib` wasn't properly bound to persistent storage\n4. **Host reprovisioning** - If `q` was reinstalled at some point\n\n## Investigation steps\n\n1. Check postgres data timestamps and contents\n2. Review Outline logs for auth/user creation events\n3. Check if there are multiple teams/workspaces in the database\n4. Review git history for OIDC-related changes around the time data was last seen\n5. Verify impermanence bind mounts are working correctly\n\n## Context\n\n- Host: q (beelink profile, impermanent=true)\n- Outline added in commit 1908fb5 (Nov 2025)\n- Recent OIDC work on pocket-id may be relevant","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-03T15:33:11.702049834Z","updated_at":"2026-01-09T14:45:34.889765969Z","closed_at":"2026-01-09T14:45:34.889765969Z","close_reason":"Closed"}
{"id":"fort-8vf","title":"Add Go toolchain to dev-sandbox","description":"Add Go and Rust toolchains to the dev-sandbox environment: go, rustc, cargo, rustfmt, clippy","status":"closed","priority":3,"issue_type":"task","created_at":"2026-01-15T03:02:15.636301306Z","updated_at":"2026-01-15T04:14:00.851242181Z","closed_at":"2026-01-15T04:14:00.851242181Z","close_reason":"Closed"}
{"id":"fort-99q","title":"Add 'just discover' task to trigger service registry","description":"Currently requires SSH to drhorrible to manually kick off service discovery. Add a just task for convenience.","status":"closed","priority":3,"issue_type":"task","created_at":"2025-12-21T22:38:48.191652-06:00","updated_at":"2025-12-30T04:41:24.1308803Z","closed_at":"2025-12-30T04:41:24.1308803Z","close_reason":"Closed"}
{"id":"fort-9gl","title":"Control plane: async callbacks and TTL rotation","description":"Complete the async capability flow with proper callbacks and TTL-based credential rotation.\n\n## Context\nThe v2 control plane (fort-c8y) implemented the core schema and handler invocation. However, async capabilities currently work via an accidental sync fallback - the 202 response includes the payload, and consumers use that directly.\n\n## Goal\nImplement true fire-and-forget async flow:\n1. Provider invokes handler, stores response\n2. Provider fires callback to consumer with credentials\n3. Consumer receives credentials via callback (not sync response)\n4. Provider rotates credentials proactively based on TTL\n\n## Tickets (in dependency order)\n- fort-rfv: Implement sendCallback\n- fort-vqo: Validate e2e callback flow\n- fort-rh6: Remove sync response fallback\n- fort-le9: TTL-based handler invocation for rotation\n- fort-vqv: git-token revocation and rotation","status":"closed","priority":2,"issue_type":"epic","created_at":"2026-01-14T17:19:30.145719596Z","updated_at":"2026-01-15T02:27:03.934989646Z","closed_at":"2026-01-15T02:27:03.934989646Z","close_reason":"Closed"}
{"id":"fort-9jg","title":"OIDC groups not auto-syncing to pocket-id on some hosts","description":"## Context\n\nAfter fort-0rj implementation, sso.groups should automatically sync to pocket-id when services deploy. However, q's services (outline, silverbullet, termix) haven't had their groups restrictions applied in pocket-id despite having groups = [\"admin\"] in their service declarations.\n\n## Observed Behavior\n\n- ratched's services (flatnotes, vdirsyncer-auth) got groups synced after force-nag\n- q's services still show no group restrictions in pocket-id UI\n- The hash-based change detection should have triggered re-requests\n\n## Test Case\n\n**Do not force-nag q's services** - leave them as-is to investigate:\n1. Why didn't the consumer detect the request change?\n2. Is the hash being stored correctly?\n3. Is there a timing issue with the aggregate handler?\n\n## Investigation Steps\n\n- Check q's fulfillment-state.json for request_hash values\n- Check if q's fort-consumer ran after the groups change deployed\n- Compare q's needs.json with ratched's to see if groups are present\n\n## Related\n\n- fort-0rj: Original groups implementation\n- fort-ajd: Handler testing strategy (may surface same root cause)","status":"open","priority":2,"issue_type":"bug","created_at":"2026-01-15T14:31:29.144876452Z","updated_at":"2026-01-15T14:31:29.144876452Z","labels":["control-plane","pocket-id"]}
{"id":"fort-9k0","title":"Investigate nix develop-based runner environment","description":"Explore using `nix develop` to provide runner dependencies from repo flake.\n\n## Problem\nCurrently the runner PATH is hardcoded in forge config. Adding/updating dependencies requires:\n1. Modifying forgejo app module\n2. Deploying to forge\n3. Restarting runner\n\nThis couples runner infrastructure to repo-specific needs (e.g., testing package updates).\n\n## Desired State\nWorkflows could declare their own dependencies via the repo devShell:\n\n```yaml\n- name: Run tests\n  run: nix develop -c ./run-tests.sh\n```\n\nOr the runner could automatically wrap commands in `nix develop`.\n\n## Investigation Areas\n- Can forgejo-runner be configured to wrap commands in `nix develop`?\n- Should we create a custom action that enters the devShell?\n- Performance implications of nix develop per-step vs per-job\n- Caching devShell dependencies\n\n## References\n- Current hardcoded PATH: apps/forgejo/default.nix runner config\n- Forgejo runner host execution model\n\n## Notes\nLow priority - current setup works, this is about flexibility for future needs.","status":"open","priority":3,"issue_type":"task","created_at":"2025-12-28T06:48:03.971366754Z","updated_at":"2025-12-28T06:48:03.971366754Z","labels":["runner investigation backlog"]}
{"id":"fort-9s5","title":"Set up ratched as dev sandbox host","description":"Configure ratched as the primary remote dev sandbox. Include: dev user with SSH access, claude-code, nix-direnv for per-project devshells, universal dev tools (git, ripgrep, fd, jq, tmux, neovim), and persistent /home.","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-23T22:57:16.2191-06:00","updated_at":"2025-12-27T19:28:38.348752495Z","closed_at":"2025-12-27T19:28:38.348752495Z","close_reason":"Ratched dev environment is live and operational","dependencies":[{"issue_id":"fort-9s5","depends_on_id":"fort-4ih","type":"blocks","created_at":"2025-12-23T22:57:24.577545-06:00","created_by":"daemon"}]}
{"id":"fort-9w1","title":"Auto tmux attach in dev sandbox zshrc","description":"Add tmux auto-attach to zshrc for ratched dev sandbox. Must handle absent tmux sessions gracefully (don't fail if no session exists, create one or skip).","status":"closed","priority":3,"issue_type":"task","created_at":"2025-12-27T19:43:36.870634395Z","updated_at":"2025-12-27T19:57:42.5555827Z","closed_at":"2025-12-27T19:57:42.5555827Z","close_reason":"Closed"}
{"id":"fort-a68","title":"Add just task to trigger service-registry remotely","description":"## Problem\nCurrently requires manual SSH to drhorrible to run:\n```bash\nsystemctl start fort-service-registry.service\nsystemctl status fort-service-registry.service\n```\n\n## Solution\nAdd a Just task that:\n1. Infers the forge host from the cluster (whoever has the `forge` role)\n2. SSHs and triggers the service\n3. Waits and shows status output\n\nSomething like:\n```\njust sync-services\n```\n\n## Notes\n- Should use the SSH key from cluster manifest\n- Don't hardcode drhorrible - derive from role assignment","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-28T02:29:14.029129834Z","updated_at":"2025-12-28T07:04:09.673230201Z","closed_at":"2025-12-28T07:04:09.673230201Z","close_reason":"Closed","labels":["dx"]}
{"id":"fort-a7j","title":"Work calendar sync for Exocortex","description":"**Desired state:** Claude (Exo) can view and write to Kevin's Google Workspace calendar via CLI.\n\n**Stack:** vdirsyncer (bidirectional sync) + khal (CLI interface)\n\n**Requirements:**\n- Bidirectional sync with Google Calendar (read AND write)\n- khal available on ratched (dev-sandbox) for CLI access\n- Systemd timer for periodic sync (start with 15 min interval)\n- Exo should be able to check sync freshness (file mtimes or similar)\n\n**Use cases:**\n- 'You have a 1:1 in 30 minutes'\n- 'Your afternoon looks clear for deep work'\n- Timeblocking: Exo creates calendar events for focus time\n- Awareness of surprise meetings from Product\n\n**Context:** This is part of Exocortex calendar integration (exocortex-5o6). Radicale for a separate self-hosted calendar is a follow-on epic.","status":"closed","priority":2,"issue_type":"epic","created_at":"2026-01-09T15:01:50.778594299Z","updated_at":"2026-01-10T05:03:02.016738577Z","closed_at":"2026-01-10T05:03:02.016738577Z","close_reason":"Closed"}
{"id":"fort-a7j.1","title":"Break down work calendar sync epic into tickets","description":"**Instructions for local agent:**\n\n1. Read the parent epic (fort-a7j) for desired state\n2. Explore the fort-nix codebase to understand:\n   - How services are currently deployed\n   - Where vdirsyncer/khal would fit in the NixOS module structure\n   - What secrets/credentials management looks like\n3. Clarify any ambiguities with Kevin (OAuth setup, which host, etc.)\n4. Create tickets under this epic for the actual implementation work\n\nThis ticket was created by Exo (exocortex) as a cross-repo handoff. The goal is to test product-manager-style delegation where requirements come from one context and implementation planning happens in another.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-09T15:02:20.33770369Z","updated_at":"2026-01-10T03:37:14.562008613Z","closed_at":"2026-01-10T03:37:14.562008613Z","close_reason":"Closed","dependencies":[{"issue_id":"fort-a7j.1","depends_on_id":"fort-a7j","type":"parent-child","created_at":"2026-01-09T15:02:20.338812142Z","created_by":"dev"}]}
{"id":"fort-a7j.2","title":"Create Google Cloud OAuth credentials for calendar sync","description":"**Type:** Manual prerequisite (documentation/instructions, not code)\n\n**Steps:**\n1. Go to [Google Cloud Console](https://console.cloud.google.com/)\n2. Create a new project (or use existing)\n3. Enable the CalDAV API (not the Calendar API - different thing)\n4. Create OAuth 2.0 Client ID credentials:\n   - Application type: Web application\n   - Authorized redirect URI: `https://vdirsyncer-auth.\u003cdomain\u003e/callback`\n5. Save client_id and client_secret\n\n**Output:**\n- `client_id` and `client_secret` to be encrypted as agenix secret\n- Document the project/credential names somewhere for future reference\n\n**Notes:**\n- This is a one-time manual step that Kevin needs to do\n- The redirect URI will point to the auth helper service (next ticket)","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-10T03:31:28.300811357Z","updated_at":"2026-01-10T03:53:54.791685745Z","closed_at":"2026-01-10T03:53:54.791685745Z","close_reason":"Closed","dependencies":[{"issue_id":"fort-a7j.2","depends_on_id":"fort-a7j","type":"parent-child","created_at":"2026-01-10T03:31:28.302819395Z","created_by":"dev"}]}
{"id":"fort-a7j.3","title":"Add vdirsyncer OAuth token service","description":"**Goal:** Expose vdirsyncer's built-in OAuth handler as a gatekeeper-protected public service.\n\n**Implementation:**\n1. Create `apps/vdirsyncer-auth/default.nix`\n2. Systemd service that runs vdirsyncer in OAuth mode (or a thin wrapper that triggers `vdirsyncer discover`)\n3. Declare in `fort.cluster.services`:\n   ```nix\n   fort.cluster.services = [{\n     name = \"vdirsyncer-auth\";\n     port = 8088;  # vdirsyncer's default OAuth callback port\n     visibility = \"public\";  # Accessible from work laptop\n     sso = {\n       mode = \"gatekeeper\";  # Login required, no identity passed\n       groups = [ \"admins\" ];  # Restrict to Kevin\n     };\n   }];\n   ```\n4. Store OAuth token in `/var/lib/vdirsyncer/token` (readable by vdirsyncer-sync timer)\n\n**Secrets needed:**\n- `vdirsyncer-oauth-client.age`: Contains `client_id` and `client_secret` from Google Cloud\n\n**Notes:**\n- Service can remain running for token refresh, or be disabled after initial auth\n- The port 8088 is hardcoded in vdirsyncer's google.py - need to verify this works with nginx proxy\n- May need to patch redirect URI handling if vdirsyncer expects localhost\n\n**Depends on:** fort-a7j.2 (OAuth credentials created)","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-10T03:33:51.83922333Z","updated_at":"2026-01-10T04:05:44.244589415Z","closed_at":"2026-01-10T04:05:44.244589415Z","close_reason":"Closed","dependencies":[{"issue_id":"fort-a7j.3","depends_on_id":"fort-a7j","type":"parent-child","created_at":"2026-01-10T03:33:51.840403018Z","created_by":"dev"},{"issue_id":"fort-a7j.3","depends_on_id":"fort-a7j.2","type":"blocks","created_at":"2026-01-10T03:36:28.343333552Z","created_by":"dev"}]}
{"id":"fort-a7j.4","title":"Add vdirsyncer + khal to dev-sandbox","description":"**Goal:** Make vdirsyncer and khal available in the dev user's PATH on ratched.\n\n**Implementation:**\n1. Add to `aspects/dev-sandbox/default.nix` devTools list:\n   ```nix\n   devTools = with pkgs; [\n     # ... existing tools ...\n     vdirsyncer  # Calendar sync daemon\n     khal        # CLI calendar interface\n   ];\n   ```\n\n2. Create vdirsyncer config directory:\n   ```nix\n   systemd.tmpfiles.rules = [\n     \"d /home/dev/.config/vdirsyncer 0700 dev users -\"\n     \"d /home/dev/.local/share/vdirsyncer 0700 dev users -\"  # Local calendar storage\n   ];\n   ```\n\n3. Template vdirsyncer config that points to:\n   - Token file from OAuth service: `/var/lib/vdirsyncer/token`\n   - Local storage: `/home/dev/.local/share/vdirsyncer/`\n   - Multiple calendars (primary + team calendars)\n\n**khal config:**\n- Point to the local vdirsyncer storage directory\n- Configure default calendar for new events\n\n**Testing:**\n```bash\nssh dev@ratched\nwhich vdirsyncer khal\nkhal list  # Should show synced events\nkhal new \"Test event\" tomorrow 10:00-11:00\n```\n\n**Depends on:** fort-a7j.3 (OAuth service for token acquisition)","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-10T03:34:09.602737443Z","updated_at":"2026-01-10T04:42:00.000129717Z","closed_at":"2026-01-10T04:42:00.000129717Z","close_reason":"Closed","dependencies":[{"issue_id":"fort-a7j.4","depends_on_id":"fort-a7j","type":"parent-child","created_at":"2026-01-10T03:34:09.603911419Z","created_by":"dev"},{"issue_id":"fort-a7j.4","depends_on_id":"fort-a7j.3","type":"blocks","created_at":"2026-01-10T03:36:33.528639602Z","created_by":"dev"}]}
{"id":"fort-a7j.5","title":"Add vdirsyncer-sync systemd timer","description":"**Goal:** Periodic bidirectional sync between Google Calendar and local storage.\n\n**Implementation:**\nAdd to dev-sandbox aspect (or as a separate vdirsyncer aspect):\n\n```nix\nsystemd.timers.\"vdirsyncer-sync\" = {\n  wantedBy = [ \"timers.target\" ];\n  timerConfig = {\n    OnBootSec = \"2m\";           # First sync 2min after boot\n    OnUnitActiveSec = \"15m\";    # Then every 15 minutes per epic spec\n  };\n};\n\nsystemd.services.\"vdirsyncer-sync\" = {\n  description = \"Sync calendars with Google\";\n  path = with pkgs; [ vdirsyncer ];\n  serviceConfig = {\n    Type = \"oneshot\";\n    User = \"dev\";\n    Group = \"users\";\n    ExecStart = \"${pkgs.vdirsyncer}/bin/vdirsyncer sync\";\n    StandardOutput = \"journal\";\n    StandardError = \"journal\";\n  };\n  environment = {\n    HOME = \"/home/dev\";\n  };\n};\n```\n\n**Monitoring:**\n```bash\nsystemctl list-timers vdirsyncer-sync\njournalctl -u vdirsyncer-sync -n 20\n```\n\n**Depends on:** fort-a7j.4 (vdirsyncer + khal installed and configured)","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-10T03:34:24.888605414Z","updated_at":"2026-01-10T05:02:35.39174647Z","closed_at":"2026-01-10T05:02:35.39174647Z","close_reason":"Closed","dependencies":[{"issue_id":"fort-a7j.5","depends_on_id":"fort-a7j","type":"parent-child","created_at":"2026-01-10T03:34:24.889816934Z","created_by":"dev"},{"issue_id":"fort-a7j.5","depends_on_id":"fort-a7j.4","type":"blocks","created_at":"2026-01-10T03:36:38.709375421Z","created_by":"dev"}]}
{"id":"fort-a7j.6","title":"Add sync freshness indicator for Exo","description":"**Goal:** Give Exo a way to check if calendar data is fresh before making decisions.\n\n**Options (pick one during implementation):**\n\n1. **File mtime approach** (simplest):\n   - Check mtime of `/home/dev/.local/share/vdirsyncer/.sync_complete`\n   - Touch this file at end of successful sync\n   - Exo runs: `stat -c %Y ~/.local/share/vdirsyncer/.sync_complete` and compares to now\n\n2. **Systemd status approach**:\n   - `systemctl show vdirsyncer-sync --property=ActiveExitTimestamp`\n   - Parse the timestamp, compare to now\n   - Also check `ExecMainStatus=0` for success\n\n3. **Wrapper script**:\n   ```bash\n   # /home/dev/.local/bin/calendar-sync-status\n   last_sync=$(stat -c %Y ~/.local/share/vdirsyncer/.sync_complete 2\u003e/dev/null || echo 0)\n   now=$(date +%s)\n   age=$((now - last_sync))\n   if [ $age -lt 1800 ]; then  # 30 min threshold\n     echo \"fresh ($age seconds ago)\"\n   else\n     echo \"stale ($age seconds ago)\"\n   fi\n   ```\n\n**Integration with khal:**\n- Exo can then confidently use `khal list today tomorrow` knowing data is fresh\n- Or trigger manual sync: `vdirsyncer sync` if stale\n\n**Depends on:** fort-a7j.5 (sync timer running)","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-10T03:36:05.987763807Z","updated_at":"2026-01-10T05:02:40.567902234Z","closed_at":"2026-01-10T05:02:40.567902234Z","close_reason":"Closed","dependencies":[{"issue_id":"fort-a7j.6","depends_on_id":"fort-a7j","type":"parent-child","created_at":"2026-01-10T03:36:05.989044302Z","created_by":"dev"},{"issue_id":"fort-a7j.6","depends_on_id":"fort-a7j.5","type":"blocks","created_at":"2026-01-10T03:36:43.890416307Z","created_by":"dev"}]}
{"id":"fort-a8o","title":"Review/improve secrets management","description":"Current setup uses agenix with device key rotation. Areas to evaluate: secret rotation workflow, onboarding new secrets, whether KEYED_FOR_DEVICES pattern is optimal, documentation of secrets workflow.","status":"closed","priority":3,"issue_type":"task","created_at":"2025-12-21T11:41:09.822948-06:00","updated_at":"2025-12-30T21:14:14.39105666Z","closed_at":"2025-12-30T21:14:14.39105666Z","close_reason":"Decided against Vault - agenix for static secrets, runtime fulfillment for dynamic credentials. No need for additional secrets management complexity."}
{"id":"fort-ajd","title":"Design handler testing and validation strategy","description":"## Context\n\nDuring fort-0rj implementation, we encountered corrupted handler responses (null client_ids) that got cached and propagated. The root cause was unclear - possibly API errors during testing that weren't surfaced.\n\n## Problem\n\nHandlers currently have no:\n- Input validation (malformed requests silently fail or produce garbage)\n- Output validation (handlers can return malformed JSON that breaks consumers)\n- Schema alignment verification between handler output and consumer callback expectations\n\n## Proposed Direction\n\n1. **Input validation**: Handlers should validate required fields exist and have expected types\n2. **Output validation**: Handler output should be validated before caching/dispatching\n3. **Schema contracts**: Define shared schemas that both handlers and consumers reference\n4. **Testing harness**: Ability to run handlers with mock inputs and verify outputs\n\n## Open Questions\n\n- Do we want a lightweight JSON schema approach, or actual types (e.g., via a typed language)?\n- Should validation live in the handler scripts, or in fort-provider wrapper?\n- How do we test aggregate handlers that depend on external APIs (pocket-id, etc.)?\n\n## Acceptance Criteria\n\n- Strategy document with chosen approach\n- Broken down into implementation tickets","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-15T14:25:56.3525465Z","updated_at":"2026-01-15T14:25:56.3525465Z","labels":["control-plane","testing"]}
{"id":"fort-ax1","title":"Establish custom derivations pattern","description":"Define a consistent pattern/location for custom derivations. Currently Zot lives inline in apps/zot/default.nix. Options: dedicated pkgs/ or overlays/ directory, or keep inline. Should inform Termix and Pocket ID work.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-21T11:41:07.637411-06:00","updated_at":"2025-12-21T12:46:12.65483-06:00","closed_at":"2025-12-21T12:46:12.65483-06:00","close_reason":"Closed"}
{"id":"fort-beu","title":"Set up home-config repo in dev-sandbox and Forgejo","description":"Mirror the home-config (home-manager) repo into Forgejo and make it accessible from dev-sandbox, similar to fort-nix.\n\n- Import repo into Forgejo under infra org\n- Set up GitHub mirror (if desired)\n- Clone into dev-sandbox workspace\n- Ensure git credentials work for push\n\nThis makes it easier to iterate on home-manager config alongside infrastructure changes.","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-01T19:51:45.940347811Z","updated_at":"2026-01-01T19:51:45.940347811Z"}
{"id":"fort-bkv","title":"Typo in outline tmpfiles: fort-authy instead of fort-auth","description":"In apps/outline/default.nix, the tmpfiles rule has a typo:\n\n```nix\nsystemd.tmpfiles.rules = [\n  \"d /var/lib/fort-authy/outline 0700 outline outline -\"  # Should be fort-auth\n];\n```\n\nThis might be causing credential directory issues. Found during control plane design audit.","status":"closed","priority":3,"issue_type":"bug","created_at":"2025-12-30T21:14:14.218020738Z","updated_at":"2026-01-09T12:50:23.187539886Z","closed_at":"2026-01-09T12:50:23.187539886Z","close_reason":"Closed"}
{"id":"fort-btm","title":"Auto-populate Termix with configured hosts","description":"Automatically add cluster hosts to Termix so they appear in the SSH connection list without manual configuration.\n\nLikely approach:\n- Query cluster manifest for hosts\n- Write directly to Termix SQLite database (or use its config format)\n- Run on activation or via timer\n\nShould be fun database hackery.","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-01T19:59:38.010668187Z","updated_at":"2026-01-01T19:59:38.010668187Z"}
{"id":"fort-bxs","title":"Handle-based credential renewal mechanism","description":"Design pattern for provider-initiated credential renewal:\n\n1. Provider generates credential with handle\n2. Client stores handle in holdings\n3. Provider detects renewal needed (e.g., ACME cert refreshed)\n4. Provider marks handles as stale/invalidated\n5. Client's fulfill-retry timer detects stale handle, re-fetches\n6. Same transform logic applies new credentials\n\nThis enables SSL cert distribution via control plane without a dedicated push mechanism. The provider just invalidates handles, and clients naturally re-fetch.\n\nRelated: fort-89e.8 (SSL cert migration), fort-89e.15 (GC foundation)","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-06T04:23:44.540426368Z","updated_at":"2026-01-14T23:29:50.97363382Z","closed_at":"2026-01-14T23:29:50.97363382Z","close_reason":"Superseded by v2 TTL/GC mechanism"}
{"id":"fort-c33","title":"Consolidate fortCluster options under fort.cluster","description":"Rename fortCluster.* options to fort.cluster.* (e.g., fort.cluster.exposedServices). Reduces magic globals and provides a single namespace for fort-specific NixOS options.","status":"closed","priority":3,"issue_type":"task","created_at":"2025-12-21T12:18:03.246583-06:00","updated_at":"2026-01-06T15:08:59.207618548Z","closed_at":"2026-01-06T15:08:59.207618548Z","close_reason":"Closed"}
{"id":"fort-c8y","title":"Runtime control plane v2","description":"Revised control plane design based on learnings from fort-89e implementation.\n\n## Authoritative Design\n\n**`docs/control-plane-interfaces.md`** is the authoritative specification.\n\nKey design decisions:\n- Fire-and-forget communication with nag-based reliability\n- Handlers receive all active requests, return all responses (no incremental mode)\n- `mode = \"rpc\"` for synchronous request-response (journal, restart, status)\n- `cacheResponse = true` for credentials that shouldn't churn (PATs)\n- `triggers = { initialize, systemd }` for boot and event-driven reconciliation\n- GC sweep doubles as periodic reconciliation\n- Provider state: `{capability → {origin:need → {request, response?, updated_at}}}`\n\n## Prior Art\n\n- `docs/control-plane-design.md` - original architecture doc (some concepts superseded)\n- `fort-89e` - original epic with partial implementation\n\n## Current State\n\nRPC-style capabilities (`journal`, `restart`, `status`, `deploy`) are already working. The async/orchestrated flow for credential distribution is not yet implemented.\n\n## Next Steps\n\nSee child tickets for audit and implementation planning work.","status":"closed","priority":2,"issue_type":"epic","created_at":"2026-01-07T06:33:16.233561494Z","updated_at":"2026-01-14T23:29:55.56935026Z","closed_at":"2026-01-14T23:29:55.56935026Z","close_reason":"v2 control plane complete - all children closed"}
{"id":"fort-c8y.1","title":"Audit existing implementation and create fresh implementation plan","description":"Review the current codebase and prior design work, then produce a fresh implementation plan.\n\n## Inputs\n\n**Prior art (for context, not authority):**\n- `docs/control-plane-design.md` - original architecture concepts\n- `fort-89e` - original epic and its 24 child tickets\n\n**Authoritative specification:**\n- `docs/control-plane-interfaces.md` - the interfaces doc we iterated on\n\n**Existing implementation:**\n- RPC-style capabilities already working: `journal`, `restart`, `status`, `deploy`, `manifest`, `holdings`\n- `fort-agent-call` client script\n- Agent nginx/CGI infrastructure\n- Whatever else is in the codebase\n\n## Deliverable\n\nA new implementation plan at `docs/control-plane-implementation.md` that:\n1. Inventories what's already built and working\n2. Identifies gaps between current state and the interfaces spec\n3. Proposes implementation order with dependencies\n4. Calls out any spec ambiguities discovered during audit\n\nThis plan will inform a fresh ticket breakdown under fort-c8y.\n\n## Cleanup\n\nOnce the new tickets are created, close `fort-89e` and all its children as `wontdo` (superseded by this epic).","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-07T06:33:33.079010531Z","updated_at":"2026-01-08T04:00:05.325465083Z","closed_at":"2026-01-08T04:00:05.325465083Z","close_reason":"Closed","dependencies":[{"issue_id":"fort-c8y.1","depends_on_id":"fort-c8y","type":"parent-child","created_at":"2026-01-07T06:33:33.081388586Z","created_by":"dev"}]}
{"id":"fort-c8y.10","title":"Phase 3: Add provider state management","description":"Persist provider state for async capabilities.\n\n## Design\n\nFile: /var/lib/fort/provider-state.json\nSchema: {capability -\u003e {origin:need -\u003e {request, response?, updated_at}}}\n\n## Tasks\n\n- [ ] Create provider state file structure\n- [ ] Load state on fort-provider startup\n- [ ] Persist state after handler runs\n- [ ] Include request, response, and updated_at per origin:need\n\n## Acceptance Criteria\n\n- State persists across restarts\n- State correctly tracks all active requests per capability\n- State file is valid JSON","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-08T04:02:34.078581096Z","updated_at":"2026-01-11T17:48:56.910564208Z","closed_at":"2026-01-11T17:48:56.910564208Z","close_reason":"Closed","dependencies":[{"issue_id":"fort-c8y.10","depends_on_id":"fort-c8y","type":"parent-child","created_at":"2026-01-08T04:02:34.079744591Z","created_by":"dev"},{"issue_id":"fort-c8y.10","depends_on_id":"fort-c8y.9","type":"blocks","created_at":"2026-01-08T04:14:31.878116393Z","created_by":"dev"}]}
{"id":"fort-c8y.11","title":"Phase 3: Implement async handler invocation","description":"Invoke async handlers with aggregate request/response state.\n\n## Design\n\n- On new request: add to state, invoke handler with ALL requests, update responses\n- On trigger: invoke handler, compare responses, callback if changed\n- Handler input: {origin:need -\u003e {request, response}}\n- Handler output: {origin:need -\u003e response}\n\n## Tasks\n\n- [ ] Build aggregate input from provider state\n- [ ] Invoke handler with aggregate JSON on stdin\n- [ ] Parse aggregate output\n- [ ] Diff responses to detect changes\n- [ ] Update provider state with new responses\n\n## Acceptance Criteria\n\n- Handler receives all active requests\n- Handler can return responses for any/all requests\n- Changed responses detected for callback dispatch","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-08T04:02:47.538180977Z","updated_at":"2026-01-11T17:55:01.436165075Z","closed_at":"2026-01-11T17:55:01.436165075Z","close_reason":"Closed","dependencies":[{"issue_id":"fort-c8y.11","depends_on_id":"fort-c8y","type":"parent-child","created_at":"2026-01-08T04:02:47.539547729Z","created_by":"dev"},{"issue_id":"fort-c8y.11","depends_on_id":"fort-c8y.10","type":"blocks","created_at":"2026-01-08T04:14:37.06215066Z","created_by":"dev"}]}
{"id":"fort-c8y.12","title":"Phase 3: Implement callback dispatch","description":"Push responses to consumer callback endpoints.\n\n## Design\n\n- After handler returns, POST responses to consumer callback endpoints\n- Fire-and-forget (ignore response status)\n- Return 202 to original request for async capabilities\n\n## Tasks\n\n- [ ] For each changed response, POST to consumer /fort/needs/\u003ctype\u003e/\u003cid\u003e\n- [ ] Use fort (CLI) or direct HTTP for callbacks\n- [ ] Fire-and-forget - log failures but do not retry\n- [ ] Return 202 Accepted for async capability requests\n\n## Acceptance Criteria\n\n- Changed responses trigger callbacks to consumers\n- Callbacks are fire-and-forget\n- Original request gets 202 for async capabilities","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-08T04:02:59.834830268Z","updated_at":"2026-01-11T18:04:03.636181125Z","closed_at":"2026-01-11T18:04:03.636181125Z","close_reason":"Closed","dependencies":[{"issue_id":"fort-c8y.12","depends_on_id":"fort-c8y","type":"parent-child","created_at":"2026-01-08T04:02:59.835944249Z","created_by":"dev"},{"issue_id":"fort-c8y.12","depends_on_id":"fort-c8y.11","type":"blocks","created_at":"2026-01-08T04:14:42.241587006Z","created_by":"dev"},{"issue_id":"fort-c8y.12","depends_on_id":"fort-c8y.5","type":"blocks","created_at":"2026-01-08T04:14:47.423965694Z","created_by":"dev"}]}
{"id":"fort-c8y.13","title":"Phase 3: Add boot-time initialization","description":"Run async handlers on boot for triggers.initialize capabilities.\n\n## Design\n\n- If triggers.initialize = true, run handler on service start\n- Load persisted state, invoke handler, dispatch callbacks\n\n## Tasks\n\n- [ ] Check triggers.initialize for each capability on startup\n- [ ] Load existing provider state\n- [ ] Invoke handler with current state\n- [ ] Dispatch callbacks for any responses\n\n## Acceptance Criteria\n\n- Handlers with triggers.initialize run on fort-provider start\n- Existing state is loaded and passed to handler\n- Callbacks dispatched for initial responses","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-08T04:03:11.543653816Z","updated_at":"2026-01-11T18:47:57.020326911Z","closed_at":"2026-01-11T18:47:57.020326911Z","close_reason":"Closed","dependencies":[{"issue_id":"fort-c8y.13","depends_on_id":"fort-c8y","type":"parent-child","created_at":"2026-01-08T04:03:11.544836552Z","created_by":"dev"},{"issue_id":"fort-c8y.13","depends_on_id":"fort-c8y.11","type":"blocks","created_at":"2026-01-08T04:14:52.603230277Z","created_by":"dev"}]}
{"id":"fort-c8y.14","title":"Phase 3: Add systemd triggers","description":"Re-run handlers when specified systemd units complete.\n\n## Design\n\n- For each triggers.systemd unit, watch for completion\n- On trigger: re-invoke handler, diff responses, callback changes\n- Fires after unit succeeds\n\n## Tasks\n\n- [ ] Generate systemd path/service units for each trigger\n- [ ] Watch for unit success (not just activation)\n- [ ] Invoke handler on trigger\n- [ ] Diff and dispatch callbacks for changes\n\n## Acceptance Criteria\n\n- Handler re-runs when trigger unit succeeds\n- Only changed responses trigger callbacks\n- Works for ACME renewal trigger","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-08T04:03:25.969143276Z","updated_at":"2026-01-11T20:52:46.007588746Z","closed_at":"2026-01-11T20:52:46.007588746Z","close_reason":"Closed","dependencies":[{"issue_id":"fort-c8y.14","depends_on_id":"fort-c8y","type":"parent-child","created_at":"2026-01-08T04:03:25.970304039Z","created_by":"dev"},{"issue_id":"fort-c8y.14","depends_on_id":"fort-c8y.11","type":"blocks","created_at":"2026-01-08T04:14:57.786573943Z","created_by":"dev"}]}
{"id":"fort-c8y.15","title":"Phase 4: Implement GC sweep (fort-provider-gc)","description":"Garbage collect orphaned state from provider.\n\n## Design\n\n- Periodic service (e.g., 1h interval)\n- For each async capability:\n  - For each origin in provider state\n  - Call POST /fort/needs on origin\n  - If need not in response (and host reachable): remove from state\n  - Invoke handler with updated state (for cleanup)\n\n## Positive-absence rules\n\n- Only delete on 200 + absence\n- Network failures = assume still in use\n- Host removed from cluster = immediate cleanup\n\n## Tasks\n\n- [ ] Create fort-provider-gc service and timer\n- [ ] Query each origin for its needs list\n- [ ] Compare against provider state\n- [ ] Remove orphaned entries\n- [ ] Invoke handler with cleaned state\n\n## Acceptance Criteria\n\n- Orphaned state cleaned up within GC interval\n- Network failures do not cause premature cleanup\n- Handler notified of removed consumers","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-08T04:03:41.12476294Z","updated_at":"2026-01-11T22:44:57.394923034Z","closed_at":"2026-01-11T22:44:57.394923034Z","close_reason":"Closed","dependencies":[{"issue_id":"fort-c8y.15","depends_on_id":"fort-c8y","type":"parent-child","created_at":"2026-01-08T04:03:41.125875087Z","created_by":"dev"},{"issue_id":"fort-c8y.15","depends_on_id":"fort-c8y.6","type":"blocks","created_at":"2026-01-08T04:15:13.183379505Z","created_by":"dev"},{"issue_id":"fort-c8y.15","depends_on_id":"fort-c8y.11","type":"blocks","created_at":"2026-01-08T04:15:18.364456875Z","created_by":"dev"}]}
{"id":"fort-c8y.16","title":"Phase 5: Migrate git-token to new schema","description":"First migration - validates new infrastructure on working capability.\n\n## Current State\n\n- git-token capability working (RPC style)\n- gitops aspect: fort.host.needs.git-token.default (RO)\n- dev-sandbox aspect: fort.host.needs.git-token.dev (RW)\n- Uses old schema: providers, store, transform\n\n## Target State\n\n- New schema: from, handler\n- Write handler scripts for gitops and dev-sandbox\n- Remove store/transform usage\n\n## Tasks\n\n- [ ] Write git-token handler for gitops (stores token, no restart needed)\n- [ ] Write git-token handler for dev-sandbox (stores token)\n- [ ] Migrate gitops need declaration to new schema\n- [ ] Migrate dev-sandbox need declaration to new schema\n- [ ] Test both RO and RW token flows\n\n## Acceptance Criteria\n\n- git clone/push still works after migration\n- No regressions in comin pulls\n- Dev sandbox push access works","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-08T04:03:55.967891437Z","updated_at":"2026-01-11T06:40:19.046384709Z","closed_at":"2026-01-11T06:40:19.046384709Z","close_reason":"Closed","dependencies":[{"issue_id":"fort-c8y.16","depends_on_id":"fort-c8y","type":"parent-child","created_at":"2026-01-08T04:03:55.96909831Z","created_by":"dev"},{"issue_id":"fort-c8y.16","depends_on_id":"fort-c8y.8","type":"blocks","created_at":"2026-01-08T04:15:53.990144501Z","created_by":"dev"},{"issue_id":"fort-c8y.16","depends_on_id":"fort-c8y.12","type":"blocks","created_at":"2026-01-08T04:15:59.177643498Z","created_by":"dev"}]}
{"id":"fort-c8y.17","title":"Phase 5: Migrate SSL certificates to control plane","description":"Replace acme-sync rsync timer with control plane callbacks.\n\n## Current State\n\n- ssl-cert capability handler exists (wildcard only)\n- acme-sync timer rsyncs certs to all hosts\n- No consumers declared\n\n## Target State\n\n- Hosts declare fort.host.needs.ssl-cert.default\n- ssl-cert capability uses async mode with triggers.systemd\n- Callbacks push certs to consumers\n- Remove acme-sync timer\n\n## Tasks\n\n- [ ] Convert ssl-cert capability to async mode\n- [ ] Add triggers.systemd for ACME renewal unit\n- [ ] Write ssl-cert consumer handler (stores certs, reloads nginx)\n- [ ] Add fort.host.needs.ssl-cert.default to hosts\n- [ ] Test cert delivery and nginx reload\n- [ ] Remove acme-sync timer from certificate-broker\n\n## Acceptance Criteria\n\n- All hosts receive certs via control plane\n- nginx reloads after cert delivery\n- Cert rotation works (ACME renewal triggers callback)","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-08T04:04:10.013204212Z","updated_at":"2026-01-12T02:52:06.074092942Z","closed_at":"2026-01-12T02:52:06.074092942Z","close_reason":"Closed","dependencies":[{"issue_id":"fort-c8y.17","depends_on_id":"fort-c8y","type":"parent-child","created_at":"2026-01-08T04:04:10.014399673Z","created_by":"dev"},{"issue_id":"fort-c8y.17","depends_on_id":"fort-c8y.16","type":"blocks","created_at":"2026-01-08T04:16:50.082643091Z","created_by":"dev"}]}
{"id":"fort-c8y.18","title":"Phase 5: Migrate Attic cache tokens to control plane","description":"Replace attic-key-sync SSH push with control plane.\n\n## Current State\n\n- attic-key-sync timer pushes cache config + tokens via SSH\n- No capability defined\n\n## Target State\n\n- attic-token capability on attic host\n- Consumers declare fort.host.needs.attic-token.default\n- Callback delivers token\n- Remove attic-key-sync timer\n\n## Tasks\n\n- [ ] Create attic-token capability handler\n- [ ] Write consumer handler (stores token + cache config)\n- [ ] Add fort.host.needs.attic-token.default to hosts\n- [ ] Test cache push from gitops hosts\n- [ ] Remove attic-key-sync timer\n\n## Acceptance Criteria\n\n- All hosts receive attic tokens via control plane\n- Cache push works from gitops post-deploy","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-08T04:04:25.221798775Z","updated_at":"2026-01-12T03:20:53.842145568Z","closed_at":"2026-01-12T03:20:53.842145568Z","close_reason":"Closed","dependencies":[{"issue_id":"fort-c8y.18","depends_on_id":"fort-c8y","type":"parent-child","created_at":"2026-01-08T04:04:25.222907496Z","created_by":"dev"},{"issue_id":"fort-c8y.18","depends_on_id":"fort-c8y.16","type":"blocks","created_at":"2026-01-08T04:16:55.259727942Z","created_by":"dev"}]}
{"id":"fort-c8y.19","title":"Phase 5: Migrate OIDC registration to control plane","description":"Replace service-registry OIDC client management with control plane.\n\n## Current State\n\n- service-registry creates/deletes pocket-id clients\n- Based on exposed services with SSO enabled\n\n## Target State\n\n- oidc-register capability on identity provider (drhorrible)\n- Consumers declare fort.host.needs.oidc.\u003cservicename\u003e\n- Aggregate handler manages all clients\n- Callback delivers client_id + client_secret\n- GC removes clients when need disappears\n\n## Implementation Status (IN PROGRESS)\n\n### Completed\n- [x] Create oidc-register capability handler in pocket-id app\n- [x] Add auto-generation of oidc needs in fort.nix for SSO services\n- [x] Remove OIDC management from service-registry\n- [x] Deploy all hosts (drhorrible, q, ratched, joker, lordhenry, minos, ursula, raishan)\n\n### Bug Fixes Applied\n- Fixed jq array concatenation in handler (commit 8d51d26)\n- Fixed jq select expression to properly return null\n\n### Pending\n- [ ] Wait for nag interval to expire (~5 min from last check at 03:50 UTC)\n- [ ] Verify OIDC registration succeeds on q (has outline, silverbullet, termix)\n- [ ] Verify on ratched (has flatnotes, vdirsyncer-auth)\n- [ ] Re-enable GC in handler (currently commented out for safe rollout)\n- [ ] Redeploy drhorrible with GC enabled\n- [ ] Close ticket\n\n### GC Code Location\nFile: apps/pocket-id/default.nix, lines 175-186\nThe GC code is commented out with `# TODO(fort-c8y.19)` marker.\nUncomment to enable deletion of all unaccounted-for OIDC clients.\n\n### Key Files Changed\n- apps/pocket-id/default.nix - oidc-register capability handler\n- common/fort.nix - auto-generates oidc needs for SSO services  \n- aspects/service-registry/registry.rb - removed OIDC management\n\n### Testing Command\n```bash\nfort q restart '{\"unit\": \"fort-consumer\"}'\nfort q journal '{\"unit\": \"fort-consumer\", \"lines\": 30, \"since\": \"1 min ago\"}'\nfort q status  # Check consumer_state for oidc-register-* entries\n```\n\n## Acceptance Criteria\n\n- OIDC clients created via control plane\n- Credentials delivered to consumers\n- Orphaned clients cleaned up by GC\n- This is the canonical aggregate capability example\n\nDepends on (2):\n  → fort-c8y: Runtime control plane v2 [P2]\n  → fort-c8y.16: Phase 5: Migrate git-token to new schema [P2]\n\nBlocks (1):\n  ← fort-c8y.21: Phase 5: Remove legacy mechanisms [P3 - open]","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-08T04:04:39.178299072Z","updated_at":"2026-01-12T04:09:59.328856253Z","closed_at":"2026-01-12T04:09:59.328856253Z","close_reason":"Closed","dependencies":[{"issue_id":"fort-c8y.19","depends_on_id":"fort-c8y","type":"parent-child","created_at":"2026-01-08T04:04:39.179396951Z","created_by":"dev"},{"issue_id":"fort-c8y.19","depends_on_id":"fort-c8y.16","type":"blocks","created_at":"2026-01-08T04:17:00.436062276Z","created_by":"dev"}]}
{"id":"fort-c8y.2","title":"Phase 1: Rename packages (fort-agent-* to fort-*)","description":"Rename existing packages to target naming convention.\n\n## Tasks\n\n- [ ] pkgs/fort-agent-wrapper/ -\u003e pkgs/fort-provider/\n- [ ] pkgs/fort-agent-call/ -\u003e pkgs/fort/\n- [ ] Update CLI to make request arg optional (default {})\n- [ ] Update all references in common/fort-agent.nix\n\n## Acceptance Criteria\n\n- fort drhorrible status works (no empty {} required)\n- fort-provider package builds and works identically to old wrapper\n- No functional changes, just naming","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-08T04:00:44.412792483Z","updated_at":"2026-01-10T16:27:07.535284948Z","closed_at":"2026-01-10T16:27:07.535284948Z","close_reason":"Closed","dependencies":[{"issue_id":"fort-c8y.2","depends_on_id":"fort-c8y","type":"parent-child","created_at":"2026-01-08T04:00:44.413883466Z","created_by":"dev"}]}
{"id":"fort-c8y.20","title":"Phase 5: Migrate proxy configuration to control plane","description":"Replace service-registry nginx vhost management with control plane.\n\n## Current State\n\n- service-registry generates nginx vhost configs for public services\n- Pushes to beacon via SSH\n\n## Target State\n\n- proxy-configure capability on beacon\n- Consumers declare fort.host.needs.proxy.\u003cservicename\u003e\n- Aggregate handler manages vhost configs\n- GC removes vhosts when need disappears\n\n## Tasks\n\n- [ ] Create proxy-configure capability handler (aggregate)\n- [ ] Handler generates and applies nginx vhost configs\n- [ ] Write consumer handler (just confirms receipt)\n- [ ] Add fort.host.needs.proxy.* declarations for public services\n- [ ] Test public service routing\n- [ ] Remove proxy management from service-registry\n\n## Acceptance Criteria\n\n- Beacon nginx configs managed via control plane\n- Public services accessible\n- Orphaned vhosts cleaned up by GC","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-08T04:04:54.29699454Z","updated_at":"2026-01-12T05:41:58.841521572Z","closed_at":"2026-01-12T05:41:58.841521572Z","close_reason":"Closed","dependencies":[{"issue_id":"fort-c8y.20","depends_on_id":"fort-c8y","type":"parent-child","created_at":"2026-01-08T04:04:54.299827858Z","created_by":"dev"},{"issue_id":"fort-c8y.20","depends_on_id":"fort-c8y.16","type":"blocks","created_at":"2026-01-08T04:17:05.618605134Z","created_by":"dev"}]}
{"id":"fort-c8y.21","title":"Phase 5: Remove legacy mechanisms","description":"Final cleanup after all migrations complete.\n\n## Legacy Mechanisms to Remove\n\n- acme-sync timer (replaced by ssl-cert callbacks)\n- attic-key-sync timer (replaced by attic-token capability)\n- service-registry aspect - decomposed into:\n  - DNS: evaluate if should move to control plane or remain centralized\n  - OIDC: replaced by oidc-register capability\n  - Proxy: replaced by proxy-configure capability\n\n## Tasks\n\n- [ ] Remove acme-sync timer from certificate-broker\n- [ ] Remove attic-key-sync timer from attic app\n- [ ] Remove OIDC client management from service-registry\n- [ ] Remove nginx vhost management from service-registry\n- [ ] Evaluate DNS management (may remain in service-registry)\n- [ ] Remove or simplify service-registry aspect\n\n## Acceptance Criteria\n\n- No SSH-based push mechanisms remain (except possibly DNS)\n- All credential/config distribution via control plane\n- Clean separation of concerns","status":"closed","priority":3,"issue_type":"task","created_at":"2026-01-08T04:05:07.839901036Z","updated_at":"2026-01-13T04:15:51.193603278Z","closed_at":"2026-01-13T04:15:51.193603278Z","close_reason":"Closed","dependencies":[{"issue_id":"fort-c8y.21","depends_on_id":"fort-c8y","type":"parent-child","created_at":"2026-01-08T04:05:07.841115804Z","created_by":"dev"},{"issue_id":"fort-c8y.21","depends_on_id":"fort-c8y.15","type":"blocks","created_at":"2026-01-08T04:17:20.273694078Z","created_by":"dev"},{"issue_id":"fort-c8y.21","depends_on_id":"fort-c8y.17","type":"blocks","created_at":"2026-01-08T04:17:25.450844033Z","created_by":"dev"},{"issue_id":"fort-c8y.21","depends_on_id":"fort-c8y.18","type":"blocks","created_at":"2026-01-08T04:17:30.633263772Z","created_by":"dev"},{"issue_id":"fort-c8y.21","depends_on_id":"fort-c8y.19","type":"blocks","created_at":"2026-01-08T04:17:35.817284767Z","created_by":"dev"},{"issue_id":"fort-c8y.21","depends_on_id":"fort-c8y.20","type":"blocks","created_at":"2026-01-08T04:17:41.001894249Z","created_by":"dev"},{"issue_id":"fort-c8y.21","depends_on_id":"fort-c8y.31","type":"blocks","created_at":"2026-01-12T13:02:43.187769108Z","created_by":"dev"},{"issue_id":"fort-c8y.21","depends_on_id":"fort-c8y.32","type":"blocks","created_at":"2026-01-12T13:02:48.369413329Z","created_by":"dev"}]}
{"id":"fort-c8y.22","title":"Remove deprecated /agent/ nginx location","description":"After fort-c8y.3 (rename paths/services) is deployed to all hosts, the /agent/ nginx location can be removed.\n\nCurrently we have dual paths (/fort/ and /agent/) for transition compatibility. Once all hosts are confirmed running the new paths, remove the deprecated /agent/ location.\n\nPrereq: Verify all hosts are running fort-c8y.3 or later.","status":"closed","priority":3,"issue_type":"task","created_at":"2026-01-10T16:50:58.24932653Z","updated_at":"2026-01-11T05:33:21.525448819Z","closed_at":"2026-01-11T05:33:21.525448819Z","close_reason":"Closed","dependencies":[{"issue_id":"fort-c8y.22","depends_on_id":"fort-c8y","type":"parent-child","created_at":"2026-01-10T16:50:58.250847835Z","created_by":"dev"}]}
{"id":"fort-c8y.23","title":"Investigate joker 'need to reboot' state","description":"comin status shows 'Need to reboot: yes' after deploy. Investigate why and whether this is expected or indicates a problem.","status":"open","priority":4,"issue_type":"task","created_at":"2026-01-11T03:38:56.8480935Z","updated_at":"2026-01-11T05:23:45.240725203Z"}
{"id":"fort-c8y.24","title":"Socket-activated services don't restart on binary change","description":"fort-provider.service (socket-activated) doesn't restart when the Go binary changes during NixOS activation. The socket stays active with the old process.\n\nDiscovered during fort-c8y.3 rollout - all auto-deploy hosts needed manual `systemctl restart fort-provider.service`.\n\nFix: Add `restartTriggers` to the service definition referencing the package, or investigate why socket-activated services skip the restart logic.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-11T04:17:42.069695163Z","updated_at":"2026-01-11T05:16:41.370008442Z","closed_at":"2026-01-11T05:16:41.370008442Z","close_reason":"Closed","dependencies":[{"issue_id":"fort-c8y.24","depends_on_id":"fort-c8y","type":"parent-child","created_at":"2026-01-11T04:17:42.070952343Z","created_by":"dev"}]}
{"id":"fort-c8y.25","title":"Move RW git token out of needs flow","description":"The dev-sandbox RW git token is currently served via the needs schema, but this is awkward:\n\n- Needs are host-scoped, but RW access is a user/principal concern\n- We worked around this by allowing ratched host to request RW (fort-c8y.8)\n- Better model: dev user makes direct `fort` calls for privileged operations\n\n## Current State\n\n- `fort.host.needs.git-token.dev` on ratched requests RW token at boot\n- Token stored at `/var/lib/fort-git/dev-token`\n- Works, but conceptually wrong - host shouldn't \"need\" user-level credentials\n\n## Target State\n\nOptions to consider:\n1. On-demand: dev user runs `fort drhorrible git-token '{\"access\":\"rw\"}'` when needed\n2. Session-based: generate token on SSH login, invalidate on logout\n3. Keep as-is if the complexity isn't worth it\n\n## Acceptance Criteria\n\n- Dev can push to forge without boot-time token distribution\n- Token lifecycle tied to user session, not host boot","status":"closed","priority":4,"issue_type":"task","created_at":"2026-01-11T06:41:53.848000895Z","updated_at":"2026-01-14T23:29:51.076333544Z","closed_at":"2026-01-14T23:29:51.076333544Z","close_reason":"Good enough with TTL rotation","dependencies":[{"issue_id":"fort-c8y.25","depends_on_id":"fort-c8y","type":"parent-child","created_at":"2026-01-11T06:41:53.849160607Z","created_by":"dev"}]}
{"id":"fort-c8y.26","title":"Review git-token TTL for provider-side GC","description":"The git-token capability previously had a 30-day TTL (ttl = 86400 * 30) for client-side enforcement. With the move to mode-based schema (fort-c8y.9), it now uses the default 24h TTL derived from async mode.\n\n## Context\n\n- Old: explicit `ttl = 86400 * 30` (30 days)\n- New: derived from `mode = \"async\"` → ttl = 86400 (24h default)\n\n## Questions to resolve\n\n- Is 30-day TTL important for git tokens, or is 24h fine?\n- How does this interact with provider-side GC?\n- Should TTL be configurable per-capability for async mode, or is a cluster-wide default sufficient?\n\n## Acceptance criteria\n\n- Decide on appropriate TTL for git-token handles\n- Implement if different from default","status":"closed","priority":3,"issue_type":"task","created_at":"2026-01-11T17:41:58.83987654Z","updated_at":"2026-01-13T16:11:10.676679301Z","closed_at":"2026-01-13T16:11:10.676679301Z","close_reason":"Closed","dependencies":[{"issue_id":"fort-c8y.26","depends_on_id":"fort-c8y","type":"parent-child","created_at":"2026-01-11T17:41:58.840996019Z","created_by":"dev"}],"comments":[{"id":1,"issue_id":"fort-c8y.26","author":"dev","text":"Investigation revealed deeper architectural issues with async capability flow. The TTL question is moot until callbacks work properly. Superseded by:\n\n- fort-rfv: Implement sendCallback\n- fort-vqo: Validate e2e callback flow\n- fort-le9: Periodic handler invocation per TTL\n- fort-vqv: Forgejo handler revocation/rotation\n- fort-rh6: Remove sync response from async requests","created_at":"2026-01-13T16:11:10Z"}]}
{"id":"fort-c8y.27","title":"Handle principal GC pattern for async capabilities","description":"Principals (like dev-sandbox) can request async capabilities but can't be queried for their needs since they don't have a fort-agent endpoint.\n\nCurrently GC correctly skips them on \"network failure\" due to positive-absence rules, but this means principal entries never get cleaned up.\n\n## Options\n\n1. **Prohibit async for principals**: Only allow RPC-mode capabilities for principal callers\n2. **TTL-based expiry for principals**: Use handle TTL expiry instead of needs-based GC\n3. **Separate principal tracking**: Different GC mechanism (e.g., principal must periodically refresh)\n4. **Build-time cleanup**: When principal removed from cluster, clean up at next deploy\n\n## Current behavior\n\n- Principal requests async capability → entry added to provider state\n- GC runs → tries to query principal's /fort/needs → fails (no endpoint)\n- Positive-absence rule → skip, don't delete\n- Entry stays forever\n\n## Context\n\nDiscovered during fort-c8y.15 implementation. The dev-sandbox principal has a git-token entry that will never be GC'd under current rules.","status":"closed","priority":3,"issue_type":"task","created_at":"2026-01-11T23:16:49.342730893Z","updated_at":"2026-01-14T23:29:51.026576307Z","closed_at":"2026-01-14T23:29:51.026576307Z","close_reason":"Folded into le9 - principals use TTL expiry instead of needs-based GC","dependencies":[{"issue_id":"fort-c8y.27","depends_on_id":"fort-c8y","type":"parent-child","created_at":"2026-01-11T23:16:49.343888658Z","created_by":"dev"}]}
{"id":"fort-c8y.28","title":"Add force-nag capability for immediate retry","description":"## Context\n\nDuring fort-c8y.19 debugging, had to wait ~15 minutes for the nag interval to expire before a fixed handler would be retried. This is friction for operational debugging.\n\n## Proposal\n\nAdd a mechanism to force immediate retry of unsatisfied needs, bypassing the nag interval. Options:\n\n1. **New capability**: `fort \u003chost\u003e nag '{\"need\": \"oidc-register-outline\"}'` or `fort \u003chost\u003e nag '{}'` for all\n2. **Extend restart**: `fort \u003chost\u003e restart '{\"unit\": \"fort-consumer\", \"clear_nag\": true}'`\n3. **State file manipulation**: Clear the nag timestamp file directly\n\nThe first option feels cleanest - a dedicated `nag` capability on fort-consumer that clears nag state and triggers immediate retry.\n\n## Acceptance\n\n- Can force immediate retry of a specific need or all needs\n- Works from dev-sandbox via `fort \u003chost\u003e nag`","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-12T04:13:55.56992409Z","updated_at":"2026-01-12T07:32:28.988211013Z","closed_at":"2026-01-12T07:32:28.988211013Z","close_reason":"Closed","dependencies":[{"issue_id":"fort-c8y.28","depends_on_id":"fort-c8y","type":"parent-child","created_at":"2026-01-12T04:13:55.571299368Z","created_by":"dev"}]}
{"id":"fort-c8y.29","title":"Restart fort-consumer-retry timer when needs change on deploy","description":"When new needs are added during a deploy, the fort-consumer-retry timer doesn't immediately trigger. This means new needs wait for the nag interval before first attempting fulfillment.\n\n## Current Behavior\n\n- Timer fires every 5 minutes (OnUnitActiveSec)\n- New needs get last_sought = now on first timer run, then wait for nag interval\n- If nag = 1h, new needs wait up to 1h before first attempt\n\n## Desired Behavior\n\n- On activation (deploy), if needs.json changed, reset fulfillment state for new needs\n- Or: trigger fort-consumer-retry immediately after activation\n\n## Implementation Options\n\n1. Add activation script that compares new vs old needs.json\n2. Use systemd Conflicts= to restart timer on fort-consumer.service\n3. Clear fulfillment state on activation when needs change","status":"closed","priority":3,"issue_type":"task","created_at":"2026-01-12T05:37:12.506926653Z","updated_at":"2026-01-14T02:53:02.681117475Z","closed_at":"2026-01-14T02:53:02.681117475Z","close_reason":"Closed","dependencies":[{"issue_id":"fort-c8y.29","depends_on_id":"fort-c8y","type":"parent-child","created_at":"2026-01-12T05:37:12.508271177Z","created_by":"dev"}]}
{"id":"fort-c8y.3","title":"Phase 1: Rename paths and services","description":"Rename paths and systemd services to target convention.\n\n## Tasks\n\n- [ ] /agent/* -\u003e /fort/* (nginx location in common/fort-agent.nix)\n- [ ] /etc/fort-agent/ -\u003e /etc/fort/\n- [ ] /var/lib/fort-agent/ -\u003e /var/lib/fort/ (consolidate)\n- [ ] fort-fulfill.service -\u003e fort-consumer.service\n- [ ] fort-fulfill-retry.timer -\u003e fort-consumer-retry.timer\n- [ ] common/fort-agent.nix -\u003e common/fort.nix (or merge)\n\n## Acceptance Criteria\n\n- All hosts deploy successfully with new paths\n- Services start and function identically\n- No /agent/ or fort-agent references remain in active config","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-08T04:00:57.58952355Z","updated_at":"2026-01-11T04:19:15.546228644Z","closed_at":"2026-01-11T04:19:15.546228644Z","close_reason":"Closed","dependencies":[{"issue_id":"fort-c8y.3","depends_on_id":"fort-c8y","type":"parent-child","created_at":"2026-01-08T04:00:57.590618791Z","created_by":"dev"},{"issue_id":"fort-c8y.3","depends_on_id":"fort-c8y.2","type":"blocks","created_at":"2026-01-08T04:11:13.958201271Z","created_by":"dev"}]}
{"id":"fort-c8y.30","title":"Phase 5: Migrate DNS configuration to control plane","description":"The service-registry aspect still manages DNS records via SSH push. This needs to migrate to control plane capabilities before fort-c8y.21 (legacy cleanup) can complete.\n\n## Current State (registry.rb)\n\n**Headscale DNS** (lines 56-64):\n- Collects VPN IPs for all services\n- SSHes to beacon, writes `/var/lib/headscale/extra-records.json`\n- Enables `service.domain` resolution over tailnet\n\n**CoreDNS/LAN DNS** (lines 66-71):\n- Filters to non-vpn services\n- SSHes to forge, writes `/var/lib/coredns/custom.conf`\n- Enables `service.domain` resolution on LAN\n\n## Target State\n\nTwo new capabilities:\n\n1. **`dns-headscale`** on beacon (raishan)\n   - Accepts: `{fqdn, ip}` records from hosts with exposed services\n   - Writes: `/var/lib/headscale/extra-records.json`\n   - Mode: async (like proxy-configure)\n\n2. **`dns-coredns`** on forge (drhorrible)\n   - Accepts: `{fqdn, ip}` records from hosts with non-vpn services\n   - Writes: `/var/lib/coredns/custom.conf`\n   - Mode: async\n\nAlternatively, could be a single `dns-configure` capability if one host should own all DNS, but current architecture has them split.\n\n## Implementation Notes\n\n- Each host with `fort.cluster.services` would declare needs for DNS registration\n- Similar pattern to `proxy-configure`: host manifest includes service info, needs trigger registration\n- GC sweep cleans up stale records when services removed\n\n## Acceptance Criteria\n\n- [ ] DNS records managed via control plane, not SSH push\n- [ ] Headscale extra-records populated from capability responses\n- [ ] CoreDNS custom.conf populated from capability responses\n- [ ] fort-c8y.21 unblocked\n","status":"tombstone","priority":2,"issue_type":"task","created_at":"2026-01-12T13:01:48.577882912Z","updated_at":"2026-01-12T13:02:06.448765702Z","deleted_at":"2026-01-12T13:02:06.448765702Z","deleted_by":"dev","delete_reason":"manual delete","original_type":"task"}
{"id":"fort-c8y.31","title":"Phase 5: Migrate Headscale DNS to control plane","description":"Migrate Headscale extra-records management from SSH push to control plane capability.\n\n## Current State (service-registry/registry.rb lines 56-64)\n\n- Collects VPN IPs for all services across cluster\n- SSHes to beacon (raishan), writes `/var/lib/headscale/extra-records.json`\n- Enables `service.domain` resolution over tailnet\n\n## Target State\n\n**`dns-headscale` capability** on beacon (raishan):\n- Mode: async\n- Accepts: `{fqdn, ip}` records from hosts with exposed services  \n- Writes: `/var/lib/headscale/extra-records.json`\n- Triggers: systemd (on headscale restart)\n\n**Consumer side** (hosts with `fort.cluster.services`):\n- Declare `fort.host.needs.dns-headscale.\u003cservice\u003e` for each exposed service\n- Request includes: fqdn, vpn_ip\n\n## Implementation Notes\n\n- Similar pattern to `proxy-configure`\n- GC sweep cleans up stale records when services removed\n- Handler aggregates all active requests into single extra-records.json\n\n## Acceptance Criteria\n\n- [ ] `dns-headscale` capability on beacon\n- [ ] Hosts with services declare dns-headscale needs\n- [ ] extra-records.json populated from capability state\n- [ ] SSH push removed from service-registry for this path\n","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-12T13:02:20.747886438Z","updated_at":"2026-01-12T14:52:28.765746957Z","closed_at":"2026-01-12T14:52:28.765746957Z","close_reason":"Closed","dependencies":[{"issue_id":"fort-c8y.31","depends_on_id":"fort-c8y","type":"parent-child","created_at":"2026-01-12T13:02:20.749309674Z","created_by":"dev"}]}
{"id":"fort-c8y.32","title":"Phase 5: Migrate CoreDNS to control plane","description":"Migrate CoreDNS custom.conf management from SSH push to control plane capability.\n\n## Current State (service-registry/registry.rb lines 66-71)\n\n- Filters to non-vpn visibility services\n- SSHes to forge (drhorrible), writes `/var/lib/coredns/custom.conf`\n- Enables `service.domain` resolution on LAN\n\n## Target State\n\n**`dns-coredns` capability** on forge (drhorrible):\n- Mode: async\n- Accepts: `{fqdn, lan_ip}` records from hosts with non-vpn services\n- Writes: `/var/lib/coredns/custom.conf`\n- Triggers: systemd (on coredns restart)\n\n**Consumer side** (hosts with non-vpn `fort.cluster.services`):\n- Declare `fort.host.needs.dns-coredns.\u003cservice\u003e` for each non-vpn service\n- Request includes: fqdn, lan_ip\n\n## Implementation Notes\n\n- Similar pattern to `proxy-configure`\n- Only services with `visibility != \"vpn\"` need LAN DNS\n- GC sweep cleans up stale records when services removed\n- Handler aggregates all active requests into hosts-style custom.conf\n\n## Acceptance Criteria\n\n- [ ] `dns-coredns` capability on forge\n- [ ] Hosts with non-vpn services declare dns-coredns needs\n- [ ] custom.conf populated from capability state\n- [ ] SSH push removed from service-registry for this path\n","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-12T13:02:32.975590473Z","updated_at":"2026-01-12T17:05:24.06933288Z","closed_at":"2026-01-12T17:05:24.06933288Z","close_reason":"Closed","dependencies":[{"issue_id":"fort-c8y.32","depends_on_id":"fort-c8y","type":"parent-child","created_at":"2026-01-12T13:02:32.976909605Z","created_by":"dev"}]}
{"id":"fort-c8y.33","title":"Remove holdings and handles from control plane","description":"Holdings and handles are artifacts of the old way of tracking things and shouldn't be needed anymore.\n\nRemove:\n- `holdings` capability from hosts\n- Any handle-related GC logic\n- Related agent code and documentation","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-13T02:43:22.564835225Z","updated_at":"2026-01-13T06:35:33.079904218Z","closed_at":"2026-01-13T06:35:33.079904218Z","close_reason":"Closed"}
{"id":"fort-c8y.34","title":"Termix OIDC self-jailing recovery","description":"**Problem**: The termix bootstrap disables password login after configuring OIDC. If OIDC credentials get rotated/invalidated (like when beads nuked the repo state), we can't log back in to reconfigure OIDC because:\n1. Password login is disabled\n2. OIDC is broken\n3. Can't disable registration without also breaking OIDC login\n\nCurrent fix requires nuking `/var/lib/termix/*` and losing all state.\n\n**Ideas to investigate**:\n- Patch termix to keep the admin user functional even with password login disabled\n- Use a pocket-id service account to authenticate the bootstrap script instead of a local admin user\n- Add a \"backdoor\" admin endpoint that accepts a local secret file for emergency reconfiguration\n- Store OIDC config externally and inject it into termix on startup (bypass the API entirely)","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-13T02:44:41.321448465Z","updated_at":"2026-01-13T15:32:29.603736298Z","closed_at":"2026-01-13T15:32:29.603736298Z","close_reason":"Closed"}
{"id":"fort-c8y.4","title":"Phase 1: Update AGENTS.md and documentation","description":"Update documentation to reflect new naming.\n\n## Tasks\n\n- [ ] Update AGENTS.md fort-agent-call references to fort\n- [ ] Update usage examples\n- [ ] Update any other docs referencing old names\n\n## Acceptance Criteria\n\n- AGENTS.md shows fort \u003chost\u003e \u003ccapability\u003e usage\n- No fort-agent-call references in active documentation","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-08T04:01:08.608605147Z","updated_at":"2026-01-11T05:47:09.789818481Z","closed_at":"2026-01-11T05:47:09.789818481Z","close_reason":"Closed","dependencies":[{"issue_id":"fort-c8y.4","depends_on_id":"fort-c8y","type":"parent-child","created_at":"2026-01-08T04:01:08.609696441Z","created_by":"dev"},{"issue_id":"fort-c8y.4","depends_on_id":"fort-c8y.3","type":"blocks","created_at":"2026-01-08T04:11:19.146194188Z","created_by":"dev"}]}
{"id":"fort-c8y.5","title":"Phase 2: Add callback endpoint to fort-provider","description":"## Summary\n\nAdd endpoint for providers to push responses to consumers, completing the fulfillment loop.\n\n## Design\n\nRoute: `POST /fort/needs/\u003ctype\u003e/\u003cid\u003e`\nAuth: Verify caller matches declared \"from\" provider in `/etc/fort/needs.json`\n\n## Flow\n\n1. Receive `POST /fort/needs/\u003ctype\u003e/\u003cid\u003e` with payload\n2. Verify `X-Fort-Origin` matches the `from` field for this need\n3. Look up need in `/etc/fort/needs.json` to find handler (if any)\n4. Invoke handler with payload on stdin (or interpret payload directly if no handler)\n5. Update `/var/lib/fort/fulfillment-state.json` based on result\n\n## State Update Logic\n\n**With handler specified:**\n- Handler exits 0 → set `satisfied = true`\n- Handler exits non-zero → `satisfied` stays false (nag will retry)\n\n**Without handler (side-effect-only needs):**\n- Non-empty response (e.g., \"OK\") → set `satisfied = true`\n- Empty response → set `satisfied = false` (revocation, triggers re-request after nag interval)\n\n## Tasks\n\n- [ ] Add `/fort/needs/\u003ctype\u003e/\u003cid\u003e` route handling in fort-agent\n- [ ] Verify caller is the declared provider for this need\n- [ ] Locate handler script from needs.json (may be null)\n- [ ] Invoke handler with payload on stdin, or interpret payload directly\n- [ ] Update fulfillment-state.json based on handler exit code or payload content\n\n## Acceptance Criteria\n\n- Provider can POST to consumer callback endpoint\n- Handler receives payload on stdin\n- Consumer state updated correctly for all cases (handler success/failure, no-handler with/without payload)\n- Auth rejects calls from non-providers","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-08T04:01:22.174429825Z","updated_at":"2026-01-11T17:29:42.338365152Z","closed_at":"2026-01-11T17:29:42.338365152Z","close_reason":"Closed","dependencies":[{"issue_id":"fort-c8y.5","depends_on_id":"fort-c8y","type":"parent-child","created_at":"2026-01-08T04:01:22.175575926Z","created_by":"dev"},{"issue_id":"fort-c8y.5","depends_on_id":"fort-c8y.4","type":"blocks","created_at":"2026-01-08T04:12:15.45741692Z","created_by":"dev"},{"issue_id":"fort-c8y.5","depends_on_id":"fort-c8y.7","type":"blocks","created_at":"2026-01-08T04:13:07.972318219Z","created_by":"dev"}]}
{"id":"fort-c8y.6","title":"Phase 2: Add needs enumeration endpoint","description":"Add endpoint for GC to query what needs a host declares.\n\n## Design\n\nRoute: POST /fort/needs\nResponse: {\"needs\": [\"type/id\", ...]}\nSource: build-time generated list (static, no runtime file read)\n\n## Tasks\n\n- [ ] Generate static needs list at build time in Nix\n- [ ] Add /fort/needs route to fort-provider\n- [ ] Return JSON list of declared needs\n\n## Acceptance Criteria\n\n- POST /fort/needs returns list of all declared needs\n- Response is static (no runtime file dependency)\n- Works for hosts with no needs (empty list)","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-08T04:01:37.087993272Z","updated_at":"2026-01-11T07:21:28.841440954Z","closed_at":"2026-01-11T07:21:28.841440954Z","close_reason":"Closed","dependencies":[{"issue_id":"fort-c8y.6","depends_on_id":"fort-c8y","type":"parent-child","created_at":"2026-01-08T04:01:37.089214941Z","created_by":"dev"},{"issue_id":"fort-c8y.6","depends_on_id":"fort-c8y.4","type":"blocks","created_at":"2026-01-08T04:12:20.634866335Z","created_by":"dev"}]}
{"id":"fort-c8y.7","title":"Phase 2: Add consumer state tracking","description":"Track fulfillment state for nag-based retry.\n\n## Design\n\nFile: /var/lib/fort/consumer-state.json\nSchema: {need_id -\u003e {satisfied: bool, last_sought: timestamp}}\n\n## Tasks\n\n- [ ] Create consumer state file on first run\n- [ ] Update fort-consumer to check satisfied before requesting\n- [ ] Track last_sought timestamp\n- [ ] Implement nag-based retry (only request if unsatisfied AND past nag interval)\n- [ ] Mark satisfied = true when callback received\n\n## Acceptance Criteria\n\n- Consumer only requests unsatisfied needs\n- Nag interval (default 15m) respected\n- State persists across restarts","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-08T04:01:52.56273974Z","updated_at":"2026-01-11T17:01:31.004605354Z","closed_at":"2026-01-11T17:01:31.004605354Z","close_reason":"Closed","dependencies":[{"issue_id":"fort-c8y.7","depends_on_id":"fort-c8y","type":"parent-child","created_at":"2026-01-08T04:01:52.563904317Z","created_by":"dev"},{"issue_id":"fort-c8y.7","depends_on_id":"fort-c8y.4","type":"blocks","created_at":"2026-01-08T04:12:25.819162373Z","created_by":"dev"}]}
{"id":"fort-c8y.8","title":"Phase 2: Simplify need options schema","description":"Replace current need options with handler-based model.\n\n## Current Schema\n- providers (list)\n- store, transform, restart, reload\n- identity\n\n## Target Schema\n- from (single provider)\n- handler (script)\n- nag (duration, default 15m)\n- request\n\n## Tasks\n\n- [ ] Add \"from\" option (single string, not list)\n- [ ] Add \"handler\" option (path to script)\n- [ ] Add \"nag\" option (duration string -\u003e seconds, default 15m)\n- [ ] Remove store/transform/restart/reload/identity\n- [ ] Update option type definitions in fort.nix\n\n## Acceptance Criteria\n\n- New schema accepted by Nix module\n- Old schema rejected with clear error\n- Handler invoked with response on stdin","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-08T04:02:08.626862093Z","updated_at":"2026-01-11T06:39:57.199869797Z","closed_at":"2026-01-11T06:39:57.199869797Z","close_reason":"Closed","dependencies":[{"issue_id":"fort-c8y.8","depends_on_id":"fort-c8y","type":"parent-child","created_at":"2026-01-08T04:02:08.628005598Z","created_by":"dev"},{"issue_id":"fort-c8y.8","depends_on_id":"fort-c8y.4","type":"blocks","created_at":"2026-01-08T04:12:31.00511036Z","created_by":"dev"}]}
{"id":"fort-c8y.9","title":"Phase 3: Update capability options schema","description":"Add async mode and trigger options to capabilities.\n\n## New Options\n\n- mode = \"rpc\" for synchronous (default is async)\n- cacheResponse - persist responses for handler to reuse\n- triggers.initialize - run on boot\n- triggers.systemd - list of units that trigger re-run\n\n## Tasks\n\n- [ ] Add mode option (default async, explicit \"rpc\" for sync)\n- [ ] Add cacheResponse option\n- [ ] Add triggers.initialize option\n- [ ] Add triggers.systemd option (list of unit names)\n- [ ] Remove needsGC and ttl (inferred from mode)\n- [ ] Update mandatory capabilities (status, manifest, needs) to mode = \"rpc\"\n\n## Acceptance Criteria\n\n- Existing RPC capabilities still work\n- New options accepted by Nix module\n- Default mode is async","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-08T04:02:22.348111875Z","updated_at":"2026-01-11T17:39:46.605648792Z","closed_at":"2026-01-11T17:39:46.605648792Z","close_reason":"Closed","dependencies":[{"issue_id":"fort-c8y.9","depends_on_id":"fort-c8y","type":"parent-child","created_at":"2026-01-08T04:02:22.349310308Z","created_by":"dev"},{"issue_id":"fort-c8y.9","depends_on_id":"fort-c8y.8","type":"blocks","created_at":"2026-01-08T04:13:42.071494122Z","created_by":"dev"}]}
{"id":"fort-c9d","title":"Set up SilverBullet for exocortex PKM","description":"Deploy SilverBullet instance at exocortex.gisi.network fronting ~/Projects/exocortex on ratched dev-sandbox.\n\nRequirements:\n- SilverBullet with gatekeeper SSO mode\n- Serves markdown from ~/Projects/exocortex (git-tracked)\n- File permissions: both SilverBullet and dev user need r/w access\n- Same markdown files are:\n  - PKM content for SilverBullet\n  - Git-tracked context for Claude Code to read/manipulate\n\nChallenges:\n- Shared file permissions between silverbullet service user and dev user\n- May need group-based permissions or bind mount with appropriate ownership\n- Ensuring git operations don't break silverbullet and vice versa\n\nThis is the 'markdown as shared context' workflow experiment.","status":"closed","priority":2,"issue_type":"feature","created_at":"2026-01-04T06:18:58.208878801Z","updated_at":"2026-01-04T15:52:12.460894016Z","closed_at":"2026-01-04T15:52:12.460894016Z","close_reason":"Closed","dependencies":[{"issue_id":"fort-c9d","depends_on_id":"fort-mcs","type":"related","created_at":"2026-01-04T06:35:51.830257924Z","created_by":"daemon"}]}
{"id":"fort-cb7","title":"Move Hugo blog off beacon host","description":"The blog (catdevurandom.com) is currently hosted on raishan (beacon). Non-essential services shouldn't be on hosts that prohibit autodeploy. Move to a different host once exposedServices supports external domains.","status":"open","priority":3,"issue_type":"task","created_at":"2026-01-04T19:55:58.093284806Z","updated_at":"2026-01-04T19:55:58.093284806Z","dependencies":[{"issue_id":"fort-cb7","depends_on_id":"fort-1hd","type":"blocks","created_at":"2026-01-13T16:30:59.951184352Z","created_by":"daemon"},{"issue_id":"fort-cb7","depends_on_id":"fort-vpc","type":"blocks","created_at":"2026-01-13T16:31:00.007669505Z","created_by":"daemon"}]}
{"id":"fort-cbu","title":"Rename Justfile to lowercase justfile","description":"The filename is 'Justfile' but agents frequently try to read 'justfile'. Rename for consistency with common conventions.","status":"closed","priority":4,"issue_type":"task","created_at":"2025-12-28T14:59:53.442404902Z","updated_at":"2025-12-30T04:42:50.560226941Z","closed_at":"2025-12-30T04:42:50.560226941Z","close_reason":"Closed"}
{"id":"fort-cpg","title":"Add just to dev sandbox environment","description":"The dev sandbox (ratched) is missing `just` in PATH. This is needed for running common development commands like `just test` and `just deploy`.\n\nShould be added to the dev sandbox zshrc or nix environment.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-28T00:34:29.983542511Z","updated_at":"2025-12-28T06:51:36.089217499Z","closed_at":"2025-12-28T06:51:36.089217499Z","close_reason":"Closed"}
{"id":"fort-cy6","title":"CI/CD and GitOps Pipeline","description":"Implement a complete CI/CD and GitOps pipeline for fort-nix using Forgejo, Comin, and Attic.\n\n## Motivation\n\n- Enable Claude Code (on ratched devbox) to trigger deployments without root SSH access to production hosts\n- Separate secret authorship (editors) from secret decryption (production hosts)\n- Eliminate the need for a master deploy key\n- Add binary caching for faster deployments across heterogeneous architectures\n\n## Architecture Overview\n\n### Components\n- **Forgejo**: Self-hosted git forge on drhorrible (forge role) with Actions CI\n- **Comin**: Pull-based GitOps - hosts poll git and self-deploy\n- **Attic**: Binary cache - build once, substitute everywhere\n\n### Two-Branch Secrets Model\n- `main` branch: secrets keyed for editors only (laptop, ratched, forge)\n- `release` branch: CI re-keys secrets for actual host recipients\n- Hosts run comin against `release` branch\n\n### Key Design Decisions\n- Forge (drhorrible) stays on manual deploy-rs - it's critical infrastructure\n- All hosts can read AND write to Attic cache (content-addressing makes poisoning infeasible)\n- CI builds all hosts best-effort, targets fill cache gaps via post-deploy hook\n- deploy-rs kept as escape hatch for high-risk changes\n\n## Reference\nSee recommendation.md for full details.","status":"closed","priority":1,"issue_type":"epic","created_at":"2025-12-27T23:48:47.590858803Z","updated_at":"2025-12-30T04:35:43.956655665Z","closed_at":"2025-12-30T04:35:43.956655665Z","close_reason":"Closed"}
{"id":"fort-cy6.1","title":"Add Forgejo app to forge role","description":"Create a Forgejo app module and add it to the forge role on drhorrible.\n\n## Context\nForgejo is a self-hosted git forge (Gitea fork) that will serve as our CI/CD hub. It runs on drhorrible alongside existing forge services (CoreDNS, Zot, Prometheus/Grafana/Loki).\n\n## Implementation\n\n### Create the app module\nCreate `apps/forgejo/default.nix`:\n\n```nix\n{ config, lib, pkgs, ... }:\nlet\n  cfg = config.fort;\n  domain = cfg.clusterManifest.domain;\nin {\n  services.forgejo = {\n    enable = true;\n    database.type = \"sqlite3\";\n    settings = {\n      server = {\n        DOMAIN = \"git.${domain}\";\n        ROOT_URL = \"https://git.${domain}/\";\n        HTTP_PORT = 3001;  # Avoid conflict with Grafana on 3000\n      };\n      service = {\n        DISABLE_REGISTRATION = true;  # SSO only\n      };\n      # Session/security settings TBD based on SSO integration\n    };\n  };\n\n  # Expose via fortCluster.exposedServices\n  fort.exposedServices.forgejo = {\n    port = 3001;\n    subdomain = \"git\";\n    visibility = \"vpn\";  # or \"public\" if needed externally\n    sso = \"oidc\";  # integrate with pocket-id\n  };\n}\n```\n\n### Add to forge role\nUpdate `roles/forge.nix` to include forgejo in the apps list.\n\n### Persistence\nForgejo data should persist at `/persist/system/var/lib/forgejo` (impermanence pattern).\n\n## Acceptance Criteria\n- [ ] Forgejo service starts on drhorrible\n- [ ] Web UI accessible at git.fort.gisi.network (via mesh)\n- [ ] Data persists across reboots\n\n## Notes\n- SSO configuration will be handled in a follow-up ticket\n- Runner setup is also a separate ticket","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-27T23:49:13.162017945Z","updated_at":"2025-12-28T02:02:16.93298591Z","closed_at":"2025-12-28T02:02:16.93298591Z","close_reason":"Closed","labels":["forgejo","phase-1"],"dependencies":[{"issue_id":"fort-cy6.1","depends_on_id":"fort-cy6","type":"parent-child","created_at":"2025-12-27T23:49:13.172306649Z","created_by":"daemon"}]}
{"id":"fort-cy6.10","title":"Configure hosts to use Attic cache","description":"Configure all hosts to substitute from the Attic binary cache.\n\n## Context\nWith Attic running on forge, all hosts should be configured to pull pre-built derivations from the cache instead of building locally.\n\n**Critical for CI**: The Forgejo runner is the primary cache consumer AND producer. When CI runs `nix flake check`, it:\n1. Pulls existing derivations from cache (avoids hammering cache.nixos.org)\n2. Builds anything missing\n3. Pushes new builds back to cache\n\nThis means CI naturally warms the cache - a separate \"build for cache\" step is unnecessary.\n\n## Implementation\n\n### Create shared cache configuration\nAdd to common configuration (e.g., in `common/host.nix` or a new aspect):\n\n```nix\n{ config, lib, pkgs, ... }:\nlet\n  cfg = config.fort;\n  domain = cfg.clusterManifest.domain;\nin {\n  nix.settings = {\n    substituters = [\n      \"https://cache.nixos.org\"\n      \"https://cache.${domain}\"\n    ];\n    \n    trusted-public-keys = [\n      \"cache.nixos.org-1:6NCHdD59X431o0gWypbMrAURkbJ16ZPMQFGspcDShjY=\"\n      \"cache.${domain}-1:XXXXX...\"  # Replace with actual key from fort-cy6.9\n    ];\n    \n    trusted-users = [ \"root\" \"@wheel\" ];\n  };\n}\n```\n\n### CI Runner Configuration\nThe forge host runs the CI runner, so it gets cache config automatically. Additionally, configure the runner's nix to push builds:\n\n```nix\n# In forgejo app or forge role\nnix.settings.post-build-hook = pkgs.writeScript \"upload-to-cache\" ''\n  #!${pkgs.bash}/bin/bash\n  set -euf\n  if [ -n \"${OUT_PATHS:-}\" ]; then\n    ${pkgs.attic-client}/bin/attic push fort-cache $OUT_PATHS\n  fi\n'';\n```\n\nThis makes every CI build automatically populate the cache.\n\n### Push capability for other hosts\nFor the multi-writer model, hosts also need push tokens (stored in agenix).\n\n## Acceptance Criteria\n- [ ] All hosts have cache.gisi.network in substituters\n- [ ] CI runner pulls from cache (verify cache hits in logs)\n- [ ] CI runner pushes builds to cache (post-build hook)\n- [ ] Subsequent CI runs are faster due to cache hits\n\n## Dependencies\n- fort-cy6.9: Attic must be deployed and cache created\n\n## Notes\n- The public key will be known after Attic setup\n- CI becomes the primary cache warmer - no separate build job needed\n- Monitor cache hit rates via Attic metrics","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-27T23:54:29.467595091Z","updated_at":"2025-12-28T20:42:20.242964522Z","closed_at":"2025-12-28T20:42:20.242964522Z","close_reason":"Closed","labels":["attic","cache","phase-3"],"dependencies":[{"issue_id":"fort-cy6.10","depends_on_id":"fort-cy6","type":"parent-child","created_at":"2025-12-27T23:54:29.47729488Z","created_by":"daemon"},{"issue_id":"fort-cy6.10","depends_on_id":"fort-cy6.9","type":"blocks","created_at":"2025-12-27T23:54:29.481056235Z","created_by":"daemon"}]}
{"id":"fort-cy6.11","title":"Configure CI runner to use Attic cache","description":"Configure the Forgejo Actions runner to both pull from and push to the Attic binary cache.\n\n## Context\nThe CI runner is the ideal place to warm the cache. Every `nix flake check` run:\n1. Evaluates all host configurations\n2. Builds derivations (with `--no-build` we skip this, but release builds will)\n3. Should pull from our cache first (avoid hammering cache.nixos.org)\n4. Should push new builds back to cache\n\n## Implementation\n\n### 1. Configure Nix substituters\nThe forge host already gets this from cy6.10 common config, but verify the runner inherits it.\n\n### 2. Add post-build hook for cache push\nIn the forgejo app module, add a post-build hook that pushes to Attic.\n\n### 3. Configure Attic credentials for runner\nThe runner needs a push token stored in agenix.\n\n### 4. Add attic-client to runner PATH\nUpdate the runner config.yml PATH to include attic-client.\n\n## Acceptance Criteria\n- [ ] CI pulls from Attic cache (check logs for cache hits)\n- [ ] CI pushes successful builds to Attic\n- [ ] Subsequent CI runs show improved cache hit rate\n- [ ] cache.nixos.org requests reduced\n\n## Dependencies\n- fort-cy6.9: Attic must be deployed\n- fort-cy6.5: CI workflow must exist\n\n## Notes\n- The post-build hook runs for ALL nix builds on forge, not just CI\n- This is actually desirable - any build on forge warms the cache","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-27T23:55:18.201841246Z","updated_at":"2025-12-28T20:42:20.292305072Z","closed_at":"2025-12-28T20:42:20.292305072Z","close_reason":"Closed","labels":["attic","ci","phase-3"],"dependencies":[{"issue_id":"fort-cy6.11","depends_on_id":"fort-cy6","type":"parent-child","created_at":"2025-12-27T23:55:18.213848738Z","created_by":"daemon"},{"issue_id":"fort-cy6.11","depends_on_id":"fort-cy6.9","type":"blocks","created_at":"2025-12-27T23:55:18.218122686Z","created_by":"daemon"},{"issue_id":"fort-cy6.11","depends_on_id":"fort-cy6.7","type":"blocks","created_at":"2025-12-27T23:55:18.219737614Z","created_by":"daemon"}]}
{"id":"fort-cy6.12","title":"Test binary cache flow","description":"Verify the binary cache is working correctly before enabling GitOps.\n\n## Context\nBefore hosts auto-deploy via comin, we need to confirm:\n1. CI builds push to cache\n2. Hosts can substitute from cache\n3. Cache hits are logged/observable\n\n## Test Cases\n\n### Test 1: CI pushes to cache\n1. Push a change to main\n2. Watch release workflow\n3. Verify build step completes\n4. Check Attic UI/CLI for new store paths\n\n```bash\nattic cache info fort-cache\n# Should show increased path count after CI run\n```\n\n### Test 2: Host substitutes from cache\nOn a test host (e.g., ratched):\n\n```bash\n# Clear local store of a known-cached path (carefully!)\n# Or just try building something that CI already built\n\n# Check substitution\nnix build /nix/store/xxx... --dry-run\n# Should show \"will be fetched from https://cache.fort.gisi.network\"\n\n# Actually build\nnix build ./path/to/something\n# Should download from cache, not build locally\n```\n\n### Test 3: Cache miss builds locally\n1. Find a derivation that CI didn't build (e.g., different arch)\n2. Build on target host\n3. Verify it builds locally\n4. (Phase 4 will add push-back to cache)\n\n### Test 4: Monitor cache metrics\nIf Attic exposes metrics:\n- Check Prometheus for cache hit/miss rates\n- Verify Grafana dashboard shows cache usage\n\n## Acceptance Criteria\n- [ ] CI workflow pushes at least one host build to cache\n- [ ] At least one host successfully substitutes from cache\n- [ ] Cache hit shows in nix logs (--print-build-logs or similar)\n- [ ] No errors in Attic service logs\n\n## Dependencies\n- fort-cy6.10: Hosts configured with substituters\n- fort-cy6.11: CI pushes to cache\n\n## Notes\n- This is the gate before Phase 4 (GitOps)\n- Document cache hit rates for future reference\n- Consider adding cache monitoring to observability stack","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-27T23:55:35.35082255Z","updated_at":"2025-12-28T22:45:46.009696312Z","closed_at":"2025-12-28T22:45:46.009696312Z","close_reason":"Closed","labels":["attic","phase-3","testing"],"dependencies":[{"issue_id":"fort-cy6.12","depends_on_id":"fort-cy6","type":"parent-child","created_at":"2025-12-27T23:55:35.360227142Z","created_by":"daemon"},{"issue_id":"fort-cy6.12","depends_on_id":"fort-cy6.10","type":"blocks","created_at":"2025-12-27T23:55:35.363639816Z","created_by":"daemon"},{"issue_id":"fort-cy6.12","depends_on_id":"fort-cy6.11","type":"blocks","created_at":"2025-12-27T23:55:35.365071817Z","created_by":"daemon"}]}
{"id":"fort-cy6.13","title":"Create comin GitOps aspect","description":"Create a comin aspect for pull-based GitOps deployment.\n\n## Context\nComin is a NixOS deployment tool that operates in pull mode - hosts poll a git repo and deploy their own configuration. This eliminates the need for a central deployer with SSH access to all hosts.\n\n## Implementation\n\n### Add comin to flake inputs\nUpdate root `flake.nix`:\n\n```nix\n{\n  inputs = {\n    # ... existing inputs ...\n    comin = {\n      url = \"github:nlewo/comin\";\n      inputs.nixpkgs.follows = \"nixpkgs\";\n    };\n  };\n  \n  outputs = { self, nixpkgs, comin, ... }: {\n    # ... existing outputs ...\n  };\n}\n```\n\n### Create aspect\nCreate `aspects/gitops/default.nix`:\n\n```nix\n{ config, lib, pkgs, inputs, ... }:\nlet\n  cfg = config.fort;\n  domain = cfg.clusterManifest.domain;\nin {\n  imports = [ inputs.comin.nixosModules.comin ];\n\n  services.comin = {\n    enable = true;\n    \n    remotes = [{\n      name = \"origin\";\n      url = \"https://git.${domain}/infra/fort-nix.git\";\n      \n      branches.main = {\n        name = \"release\";  # Pull from release branch, not main\n      };\n      \n      # Optional: testing branch for safe experimentation\n      # branches.testing = {\n      #   name = \"testing\";\n      # };\n    }];\n    \n    # Poll interval\n    # interval = 60;  # seconds, default is 60\n    \n    # Machine identification\n    # By default, uses hostname to find its config\n  };\n}\n```\n\n### Flake structure for comin\nComin expects `nixosConfigurations.\u003chostname\u003e` at the flake root or a specified path. Our current structure has configs in host subflakes. Options:\n\n1. **Export from root flake**: Add `nixosConfigurations` to root flake that re-exports from host flakes\n2. **Configure comin path**: Point comin to the host-specific flake\n\nOption 1 is cleaner for comin:\n\n```nix\n# In root flake.nix\noutputs = { ... }: {\n  nixosConfigurations = {\n    ratched = (import ./clusters/bedlam/hosts/ratched/flake.nix).nixosConfigurations.ratched;\n    # ... other hosts\n  };\n};\n```\n\n### Authentication\nComin needs to fetch from Forgejo. Options:\n- Public read access to release branch\n- Deploy key with read-only access\n- Token-based authentication\n\nFor simplicity, consider making the release branch publicly readable (contains no secrets that hosts can't decrypt anyway).\n\n## Acceptance Criteria\n- [ ] Comin aspect module created\n- [ ] Can be added to host manifest\n- [ ] Comin service starts without errors\n- [ ] Comin can fetch from Forgejo (may need auth setup)\n\n## Dependencies\n- fort-cy6.12: Cache should be working first\n\n## Notes\n- Start with ratched (dev sandbox) for testing\n- Do NOT add to forge (drhorrible) - it stays on manual deploy\n- See recommendation.md for full rationale","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-27T23:56:13.512683383Z","updated_at":"2025-12-29T04:05:15.107224624Z","closed_at":"2025-12-29T04:05:15.107224624Z","close_reason":"Closed","labels":["comin","gitops","phase-4"],"dependencies":[{"issue_id":"fort-cy6.13","depends_on_id":"fort-cy6","type":"parent-child","created_at":"2025-12-27T23:56:13.518647597Z","created_by":"daemon"},{"issue_id":"fort-cy6.13","depends_on_id":"fort-cy6.12","type":"blocks","created_at":"2025-12-27T23:56:13.522693099Z","created_by":"daemon"}]}
{"id":"fort-cy6.14","title":"Deploy comin to joker (test host)","description":"Deploy comin to ratched as the first GitOps-enabled host.\n\n## Context\nRatched is the dev sandbox - low risk, easy to recover if something breaks. It's the ideal first host for GitOps.\n\n## Implementation\n\n### Update ratched manifest\nEdit `clusters/bedlam/hosts/ratched/manifest.nix`:\n\n```nix\n{\n  hostName = \"ratched\";\n  deviceUuid = \"...\";\n  \n  roles = [];\n  \n  apps = [];\n  \n  aspects = [\n    \"mesh\"\n    \"observable\"\n    \"dev-sandbox\"\n    \"gitops\"  # Add this\n  ];\n}\n```\n\n### Initial deploy via deploy-rs\nThe first deploy must be via deploy-rs (chicken-and-egg: comin isn't installed yet):\n\n```bash\njust deploy ratched\n```\n\n### Verify comin is running\nAfter deploy:\n\n```bash\nssh root@ratched.fort.gisi.network\n\n# Check service status\nsystemctl status comin\n\n# Check logs\njournalctl -u comin -f\n\n# Comin should be polling and showing \"no changes\" or similar\n```\n\n### Test a change\n1. Make a small change to ratched's config (e.g., add a comment)\n2. Push to main\n3. Wait for CI to update release branch\n4. Watch comin logs on ratched\n5. Verify the change is applied\n\n## Acceptance Criteria\n- [ ] Comin service running on ratched\n- [ ] Comin successfully polls Forgejo\n- [ ] Comin detects and applies a test change\n- [ ] No manual intervention needed for the test change\n\n## Dependencies\n- fort-cy6.13: Comin aspect must be created\n\n## Notes\n- If comin breaks ratched, we can still SSH in and fix manually\n- Or use deploy-rs as fallback\n- This validates the entire pipeline before rolling out to other hosts","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-27T23:56:30.002650682Z","updated_at":"2025-12-29T04:05:15.159188871Z","closed_at":"2025-12-29T04:05:15.159188871Z","close_reason":"Closed","labels":["comin","gitops","phase-4"],"dependencies":[{"issue_id":"fort-cy6.14","depends_on_id":"fort-cy6","type":"parent-child","created_at":"2025-12-27T23:56:30.012473934Z","created_by":"daemon"},{"issue_id":"fort-cy6.14","depends_on_id":"fort-cy6.13","type":"blocks","created_at":"2025-12-27T23:56:30.01606146Z","created_by":"daemon"}]}
{"id":"fort-cy6.15","title":"Add post-build cache push hook","description":"Configure hosts to push their builds back to Attic cache after successful deployment.\n\n## Context\nFor heterogeneous architectures, CI may not be able to build everything. Hosts that build locally should push their results to cache so future builds (on any host of that arch) get cache hits.\n\n## Implementation\n\n### Option A: Comin post-deploy hook\nComin supports post-deploy commands:\n\n```nix\nservices.comin = {\n  # ... existing config ...\n\n  postBuildCommand = '\n    ${pkgs.attic-client}/bin/attic push fort-cache \"$out\"\n  ';\n};\n```\n\nThis pushes the built system closure after successful activation.\n\n### Option B: Nix post-build-hook\nGlobal hook for ALL nix builds:\n\n```nix\nnix.settings.post-build-hook = pkgs.writeScript \"upload-to-cache\" '\n  #\\!/bin/bash\n  set -euf\n\n  if [ -n \"${OUT_PATHS:-}\" ]; then\n    echo \"Pushing to cache: $OUT_PATHS\"\n    attic push fort-cache $OUT_PATHS || true\n  fi\n';\n```\n\n### Authentication\nEach host needs a push token for Attic. Options:\n\n1. **Shared write token**: All hosts use the same token (simpler)\n2. **Per-host tokens**: Each host has its own token (more auditable)\n\nFor homelab, Option 1 is fine. Store token in agenix and configure attic client.\n\n### Recommendation\nUse Option A (comin hook) for GitOps hosts - only pushes system builds, not every random build.\n\nKeep Option B available as an aspect for hosts that do a lot of local building.\n\n## Acceptance Criteria\n- [ ] Hosts can push to Attic cache\n- [ ] After comin deploys, built paths appear in cache\n- [ ] Subsequent builds on other hosts of same arch get cache hits\n\n## Dependencies\n- fort-cy6.13: Comin aspect needed\n- fort-cy6.10: Hosts need Attic client configured\n\n## Notes\n- This completes the multi-writer cache model\n- First ARM host to deploy will populate ARM cache for all ARM hosts\n- Monitor cache growth - may need garbage collection tuning","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-27T23:58:46.149670638Z","updated_at":"2025-12-29T19:23:02.926041198Z","closed_at":"2025-12-29T19:23:02.926041198Z","close_reason":"Closed","labels":["attic","comin","phase-4"],"dependencies":[{"issue_id":"fort-cy6.15","depends_on_id":"fort-cy6","type":"parent-child","created_at":"2025-12-27T23:58:46.159878071Z","created_by":"daemon"},{"issue_id":"fort-cy6.15","depends_on_id":"fort-cy6.13","type":"blocks","created_at":"2025-12-27T23:58:46.163700898Z","created_by":"daemon"},{"issue_id":"fort-cy6.15","depends_on_id":"fort-cy6.10","type":"blocks","created_at":"2025-12-27T23:58:46.165341456Z","created_by":"daemon"}]}
{"id":"fort-cy6.16","title":"Test GitOps end-to-end flow","description":"Comprehensive test of the complete GitOps pipeline before rolling out to more hosts.\n\n## Context\nBefore enabling comin on production hosts, verify the entire flow works:\n1. Push to main\n2. CI builds + re-keys → release branch\n3. Comin pulls release on ratched\n4. Ratched deploys and pushes to cache\n\n## Test Scenario\n\n### Make a visible change\nAdd something observable to ratched's config:\n\n```nix\n# In ratched's config, add:\nenvironment.etc.\"gitops-test\".text = \"deployed at: ${builtins.currentTime}\";\n# Or simpler: add a systemd service, a package, etc.\n```\n\n### Execute the test\n\n1. **Commit and push to main**\n   ```bash\n   git add .\n   git commit -m \"Test GitOps flow\"\n   git push origin main\n   ```\n\n2. **Watch CI** (Forgejo Actions)\n   - Flake check passes\n   - Build step runs (may use cache)\n   - Re-key step runs\n   - Release branch updated\n\n3. **Watch comin on ratched**\n   ```bash\n   ssh root@ratched.fort.gisi.network\n   journalctl -u comin -f\n   # Should see: pull, evaluate, build, activate\n   ```\n\n4. **Verify change applied**\n   ```bash\n   cat /etc/gitops-test\n   # Should show the file we added\n   ```\n\n5. **Verify cache populated**\n   ```bash\n   attic cache info fort-cache\n   # Should show ratched's paths\n   ```\n\n### Timing\nNote the time from push to activation. This is your deployment latency:\n- CI time: ~X minutes\n- Comin poll interval: up to 60s\n- Build time: depends on cache hits\n- Total: ~Y minutes\n\n## Failure Scenarios to Test\n\n### Test: What if CI fails?\n- Push a change that breaks flake check\n- Verify release branch is NOT updated\n- Verify ratched stays on previous config\n\n### Test: What if comin build fails?\n- Push a change that evaluates but fails to build\n- Verify ratched stays on previous config\n- Check comin logs for error\n\n### Test: Manual rollback\n- If needed, how to manually fix ratched?\n- `nixos-rebuild switch` or deploy-rs should still work\n\n## Acceptance Criteria\n- [ ] Change flows from push → ratched without manual intervention\n- [ ] Deployment time is acceptable (\u003c 10 minutes?)\n- [ ] Failed changes don't break ratched\n- [ ] Manual recovery path confirmed working\n\n## Dependencies\n- fort-cy6.14: Comin deployed to ratched\n- fort-cy6.15: Post-build hook configured\n\n## Notes\n- This is the gate before production rollout\n- Document any issues for future reference\n- Consider adding alerting for comin failures","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-27T23:59:08.283025585Z","updated_at":"2025-12-29T04:17:48.761245696Z","closed_at":"2025-12-29T04:17:48.761245696Z","close_reason":"Closed","labels":["gitops","phase-4","testing"],"dependencies":[{"issue_id":"fort-cy6.16","depends_on_id":"fort-cy6","type":"parent-child","created_at":"2025-12-27T23:59:08.293134677Z","created_by":"daemon"},{"issue_id":"fort-cy6.16","depends_on_id":"fort-cy6.14","type":"blocks","created_at":"2025-12-27T23:59:08.296807129Z","created_by":"daemon"},{"issue_id":"fort-cy6.16","depends_on_id":"fort-cy6.15","type":"blocks","created_at":"2025-12-27T23:59:08.298362491Z","created_by":"daemon"}]}
{"id":"fort-cy6.17","title":"Roll out comin to remaining hosts","description":"Enable GitOps on remaining hosts (except forge).\n\n## Context\nAfter validating GitOps on ratched, roll out to other hosts incrementally.\n\n## Hosts to Enable\n\nEnable comin on (in suggested order):\n1. **joker** - minimal node, low risk\n2. **ursula** - media server\n3. **lordhenry** - LLM stack\n4. **minos** - IoT hub (more complex, test carefully)\n5. **q** - ingest machine (many services)\n6. **raishan** - beacon (public-facing, do last among these)\n\n## DO NOT Enable\n- **drhorrible (forge)** - stays on manual deploy-rs (critical infrastructure)\n\n## Rollout Process\n\nFor each host:\n\n1. **Update manifest**\n   ```nix\n   aspects = [\n     # ... existing aspects ...\n     \"gitops\"\n   ];\n   ```\n\n2. **Deploy via deploy-rs** (first time)\n   ```bash\n   just deploy \u003chostname\u003e\n   ```\n\n3. **Verify comin running**\n   ```bash\n   ssh root@\u003chostname\u003e.fort.gisi.network\n   systemctl status comin\n   ```\n\n4. **Test a change**\n   Push a small change, verify it propagates\n\n5. **Monitor for issues**\n   Check logs, verify services healthy\n\n### Rollback Plan\nIf a host has issues:\n1. SSH in (should still work)\n2. `systemctl stop comin` to prevent further changes\n3. Fix via `nixos-rebuild switch` or deploy-rs\n4. Investigate root cause before re-enabling\n\n## Acceptance Criteria\n- [ ] All hosts (except forge) running comin\n- [ ] All hosts successfully deploying from release branch\n- [ ] No manual intervention needed for normal changes\n- [ ] Forge confirmed to still work with deploy-rs\n\n## Dependencies\n- fort-cy6.16: E2E test must pass first\n\n## Notes\n- Take it slow - one host at a time\n- Have a rollback plan ready\n- Monitor for a few days before considering complete\n- Update documentation with new deployment workflow","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-27T23:59:26.664584628Z","updated_at":"2025-12-29T21:46:07.493628169Z","closed_at":"2025-12-29T21:46:07.493628169Z","close_reason":"Closed","labels":["comin","gitops","phase-4"],"dependencies":[{"issue_id":"fort-cy6.17","depends_on_id":"fort-cy6","type":"parent-child","created_at":"2025-12-27T23:59:26.674063868Z","created_by":"daemon"},{"issue_id":"fort-cy6.17","depends_on_id":"fort-cy6.16","type":"blocks","created_at":"2025-12-27T23:59:26.677698788Z","created_by":"daemon"}]}
{"id":"fort-cy6.18","title":"Grant Claude Code push access to Forgejo","description":"Configure Claude Code on ratched to push to Forgejo without exposing sensitive credentials.\n\n## Context\nThe goal of this entire initiative: Claude Code can trigger deployments by pushing to git, without having SSH access to production hosts or the ability to decrypt production secrets.\n\n## Implementation\n\n### Authentication Options\n\n1. **Personal Access Token**\n   - Create a Forgejo PAT for \"claude-code\" user\n   - Store on ratched (can be in plain file - it only grants git push)\n   - Configure git credential helper\n\n2. **SSH Deploy Key**\n   - Generate SSH key on ratched\n   - Add public key to Forgejo as deploy key with write access\n   - Configure git to use this key\n\n3. **OIDC-based auth** (if supported)\n   - More complex but more secure\n\n### Recommended: Personal Access Token\n\nOn ratched:\n```bash\n# Store token (created via Forgejo UI)\necho \"token_here\" \u003e /persist/system/home/dev/.forgejo-token\nchmod 600 /persist/system/home/dev/.forgejo-token\n\n# Configure git\ngit config --global credential.helper store\n# Or use a credential helper that reads from file\n```\n\nIn NixOS config:\n```nix\n# Ensure git is configured for the dev user\nhome-manager.users.dev = {\n  programs.git = {\n    enable = true;\n    extraConfig = {\n      credential.helper = \"store --file=/persist/system/home/dev/.forgejo-token\";\n    };\n  };\n};\n```\n\n### Create Forgejo User\n- Create \"claude-code\" user in Forgejo\n- Grant write access to fort-nix repo\n- Generate PAT with repo write scope\n\n### Permissions Audit\nVerify Claude Code on ratched has:\n- [x] Push access to Forgejo (this ticket)\n- [x] Editor key for main branch secrets\n- [ ] NO SSH access to other hosts\n- [ ] NO access to production secrets (only editor-keyed)\n- [ ] NO access to FORGE_AGE_KEY\n\n## Acceptance Criteria\n- [ ] Claude Code can `git push` to Forgejo\n- [ ] Push triggers CI workflow\n- [ ] No additional credentials exposed to Claude Code\n\n## Dependencies\n- fort-cy6.17: GitOps must be working first\n\n## Notes\n- This is the final piece that enables autonomous Claude Code deployments\n- Token should be rotatable without system changes\n- Consider audit logging of pushes","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-27T23:59:55.004948365Z","updated_at":"2025-12-29T22:15:06.973476772Z","closed_at":"2025-12-29T22:15:06.973476772Z","close_reason":"Closed","labels":["phase-5","security"],"dependencies":[{"issue_id":"fort-cy6.18","depends_on_id":"fort-cy6","type":"parent-child","created_at":"2025-12-27T23:59:55.014227504Z","created_by":"daemon"},{"issue_id":"fort-cy6.18","depends_on_id":"fort-cy6.17","type":"blocks","created_at":"2025-12-27T23:59:55.017562567Z","created_by":"daemon"}]}
{"id":"fort-cy6.19","title":"Document deployment workflows","description":"Document the new deployment workflows and when to use each.\n\n## Context\nWith GitOps enabled, there are now multiple ways to deploy. Document when to use each.\n\n## Documentation to Create/Update\n\n### Update README.md or create DEPLOYING.md\n\n```markdown\n# Deployment Workflows\n\n## GitOps (Default)\nMost hosts auto-deploy via comin when changes are pushed to main.\n\n1. Make changes\n2. Push to main\n3. CI validates and updates release branch\n4. Hosts pull and deploy automatically\n\nAffected hosts: all except drhorrible (forge)\n\n## Manual Deploy (Forge Only)\nForge (drhorrible) requires manual deployment:\n\n\\`\\`\\`bash\njust deploy drhorrible\n\\`\\`\\`\n\n## Manual Deploy (Emergency/Override)\nFor any host, you can still use deploy-rs:\n\n\\`\\`\\`bash\njust deploy \u003chostname\u003e\n\\`\\`\\`\n\nUse this when:\n- GitOps is broken\n- You need immediate deployment (don't want to wait for CI)\n- Testing changes before committing\n\n## High-Risk Changes\nFor changes that might break SSH/network:\n\n1. Use comin's testing branch feature\n2. Or deploy manually with deploy-rs (has rollback)\n3. Have console access ready\n\n## Monitoring Deployments\n- Forgejo Actions: https://git.fort.gisi.network/infra/fort-nix/actions\n- Comin logs: \\`journalctl -u comin -f\\` on each host\n- Grafana: deployment metrics (if configured)\n```\n\n### Update Justfile comments\nAdd comments explaining when to use `just deploy` vs GitOps.\n\n### Update recommendation.md\nMark as implemented, add lessons learned.\n\n## Acceptance Criteria\n- [ ] Deployment documentation exists\n- [ ] Clear guidance on when to use each method\n- [ ] Emergency procedures documented\n- [ ] Team knows about the new workflow\n\n## Dependencies\n- fort-cy6.17: GitOps must be rolled out first\n\n## Notes\n- Keep it concise - developers won't read walls of text\n- Include troubleshooting tips\n- Link to relevant logs/dashboards","status":"closed","priority":3,"issue_type":"task","created_at":"2025-12-28T00:00:13.749704412Z","updated_at":"2025-12-29T22:31:55.671371074Z","closed_at":"2025-12-29T22:31:55.671371074Z","close_reason":"Closed","labels":["docs","phase-5"],"dependencies":[{"issue_id":"fort-cy6.19","depends_on_id":"fort-cy6","type":"parent-child","created_at":"2025-12-28T00:00:13.758309846Z","created_by":"daemon"},{"issue_id":"fort-cy6.19","depends_on_id":"fort-cy6.17","type":"blocks","created_at":"2025-12-28T00:00:13.761624549Z","created_by":"daemon"}]}
{"id":"fort-cy6.2","title":"Configure Forgejo SSO via pocket-id","description":"Configure Forgejo to authenticate via pocket-id (the existing OIDC provider).\n\n## Context\npocket-id is already deployed on drhorrible and provides OIDC authentication backed by LLDAP. Forgejo needs to be configured as an OIDC client.\n\n## Implementation\n\n### Register Forgejo as OIDC client in pocket-id\nAdd Forgejo client configuration to pocket-id. The redirect URI will be:\n`https://git.gisi.network/user/oauth2/pocket-id/callback`\n\n### Configure Forgejo OIDC\nUpdate `apps/forgejo/default.nix` to add OIDC settings:\n\n```nix\nservices.forgejo.settings = {\n  # ... existing settings ...\n  \n  oauth2_client = {\n    ENABLE_AUTO_REGISTRATION = true;\n    USERNAME = \"preferred_username\";  # or \"email\"\n    ACCOUNT_LINKING = \"auto\";\n  };\n};\n```\n\nThe actual OIDC provider configuration may need to be done via Forgejo admin UI or database seeding, since it includes secrets (client_id, client_secret).\n\n### Secrets\n- Create `apps/forgejo/oidc-secret.age` with the OIDC client secret\n\n## Acceptance Criteria\n- [ ] Users can log in to Forgejo via pocket-id\n- [ ] No local registration (SSO-only)\n- [ ] User accounts auto-created on first login\n\n## Notes\n- May need to configure group-based authorization later\n- Consider admin user bootstrapping strategy\n\nLabels: [forgejo phase-1 sso]","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-27T23:49:27.524565897Z","updated_at":"2025-12-28T03:27:02.065472508Z","closed_at":"2025-12-28T03:27:02.065472508Z","close_reason":"Closed","labels":["forgejo","phase-1","sso"],"dependencies":[{"issue_id":"fort-cy6.2","depends_on_id":"fort-cy6","type":"parent-child","created_at":"2025-12-27T23:49:27.533675547Z","created_by":"daemon"},{"issue_id":"fort-cy6.2","depends_on_id":"fort-cy6.1","type":"blocks","created_at":"2025-12-27T23:49:27.537271627Z","created_by":"daemon"}]}
{"id":"fort-cy6.20","title":"Verify deploy-rs still works as escape hatch","description":"Confirm deploy-rs remains functional as an escape hatch for manual deployments.\n\n## Context\nEven with GitOps, we need deploy-rs to work for:\n- Forge (drhorrible) - always manual\n- Emergency overrides on any host\n- High-risk changes where you want rollback protection\n\n## Verification\n\n### Test on forge\n```bash\njust deploy drhorrible\n```\n- Should work as before\n- This is the primary deployment method for forge\n\n### Test on a GitOps host\n```bash\njust deploy ratched\n```\n- Should still work\n- Comin and deploy-rs can coexist\n- deploy-rs does immediate push, comin will eventually converge\n\n### Verify rollback still works\n1. Deploy a change that breaks SSH (test carefully!)\n2. Verify deploy-rs rolls back automatically\n3. Host should remain accessible\n\n### Check for conflicts\n- Comin polling shouldn't interfere with deploy-rs\n- If both try to activate simultaneously, one should win cleanly\n\n## Acceptance Criteria\n- [ ] `just deploy drhorrible` works\n- [ ] `just deploy \u003cgitops-host\u003e` works\n- [ ] No conflicts between comin and deploy-rs\n- [ ] Rollback protection functional on deploy-rs deploys\n\n## Dependencies\n- fort-cy6.17: GitOps must be rolled out first\n\n## Notes\n- deploy-rs uses SSH, comin uses git pull - different mechanisms\n- Keep Justfile deploy commands working\n- This is insurance against GitOps failures","status":"closed","priority":3,"issue_type":"task","created_at":"2025-12-28T00:00:33.63959393Z","updated_at":"2025-12-29T22:26:33.604104361Z","closed_at":"2025-12-29T22:26:33.604104361Z","close_reason":"Closed","labels":["phase-5","testing"],"dependencies":[{"issue_id":"fort-cy6.20","depends_on_id":"fort-cy6","type":"parent-child","created_at":"2025-12-28T00:00:33.648712989Z","created_by":"daemon"},{"issue_id":"fort-cy6.20","depends_on_id":"fort-cy6.17","type":"blocks","created_at":"2025-12-28T00:00:33.651953456Z","created_by":"daemon"}]}
{"id":"fort-cy6.21","title":"Support comin test branches with secret re-keying","description":"## Context\n\nComin supports a pattern where pushing to a `host-test` branch lets the host pull and run the config without updating the boot generation. This is useful for testing changes before committing to them.\n\nHowever, with the two-branch secrets model, `main` (and branches off main) are keyed for editors only. Hosts can't decrypt those secrets directly.\n\n## Proposed Solution\n\nCreate a workflow that:\n1. Watches for pushes to `*-test-rekey` branches (e.g., `minos-test-rekey`)\n2. Re-keys secrets for the target host\n3. Pushes to the corresponding `*-test` branch (e.g., `minos-test`)\n\nThis lets developers push experimental changes to `host-test-rekey` and have CI produce a decryptable `host-test` branch for comin to pull.\n\n## Alternatives Considered\n\n- Could require devs to manually re-key for test deploys (annoying)\n- Could have a single `test-rekey` branch that CI fans out to per-host test branches (complex)\n\n## Dependencies\n- fort-cy6.7: Release workflow (establishes the re-keying pattern)\n- fort-cy6.13: Comin aspect (hosts need comin to use test branches)\n\n## Notes\n- Naming TBD: `host-test-rekey` vs `test/host` vs something else\n- May want to auto-cleanup stale test branches","status":"closed","priority":3,"issue_type":"task","created_at":"2025-12-28T07:21:46.574313228Z","updated_at":"2025-12-29T23:24:28.991432291Z","closed_at":"2025-12-29T23:24:28.991432291Z","close_reason":"Closed","dependencies":[{"issue_id":"fort-cy6.21","depends_on_id":"fort-cy6","type":"parent-child","created_at":"2025-12-28T07:21:46.575809539Z","created_by":"daemon"}]}
{"id":"fort-cy6.22","title":"Consolidate key definitions in cluster manifest","description":"## Context\n\nThe cluster manifest has overlapping key definitions that evolved organically:\n\n- `sshKey.publicKey` - primary laptop deploy key\n- `authorizedDeployKeys` - additional deploy keys (added to root authorized_keys AND secrets)\n- `privilegedKeys` - keys that can decrypt secrets on main branch\n- `ciAgeKey` - CI-specific key\n\n## Cleanup Options\n\n1. Merge `sshKey.publicKey` into `privilegedKeys` (already duplicated there)\n2. Clarify purpose of `authorizedDeployKeys` vs `privilegedKeys`\n3. Consider whether `authorizedDeployKeys` should only affect SSH access, not secret decryption\n\n## Notes\nLow priority cleanup after CI/CD pipeline is stable.","status":"closed","priority":3,"issue_type":"task","created_at":"2025-12-28T08:09:57.615598545Z","updated_at":"2025-12-30T04:01:38.260941252Z","closed_at":"2025-12-30T04:01:38.260948426Z","dependencies":[{"issue_id":"fort-cy6.22","depends_on_id":"fort-cy6","type":"parent-child","created_at":"2025-12-28T08:09:57.616975844Z","created_by":"daemon"}]}
{"id":"fort-cy6.22.1","title":"Update manifest.nix with principals structure","status":"closed","priority":3,"issue_type":"task","created_at":"2025-12-30T03:47:15.694679037Z","updated_at":"2025-12-30T03:48:23.675872115Z","closed_at":"2025-12-30T03:48:23.675876784Z","dependencies":[{"issue_id":"fort-cy6.22.1","depends_on_id":"fort-cy6.22","type":"parent-child","created_at":"2025-12-30T03:47:15.704752326Z","created_by":"daemon"}]}
{"id":"fort-cy6.22.2","title":"Update secrets.nix to derive from principals","status":"closed","priority":3,"issue_type":"task","created_at":"2025-12-30T03:47:15.890745828Z","updated_at":"2025-12-30T03:50:09.03872924Z","closed_at":"2025-12-30T03:50:09.038733478Z","dependencies":[{"issue_id":"fort-cy6.22.2","depends_on_id":"fort-cy6.22","type":"parent-child","created_at":"2025-12-30T03:47:15.892171597Z","created_by":"daemon"}]}
{"id":"fort-cy6.22.3","title":"Update host.nix to derive root authorized_keys","status":"closed","priority":3,"issue_type":"task","created_at":"2025-12-30T03:47:16.091349425Z","updated_at":"2025-12-30T03:51:11.272209655Z","closed_at":"2025-12-30T03:51:11.272214154Z","dependencies":[{"issue_id":"fort-cy6.22.3","depends_on_id":"fort-cy6.22","type":"parent-child","created_at":"2025-12-30T03:47:16.09221078Z","created_by":"daemon"}]}
{"id":"fort-cy6.22.4","title":"Update dev-sandbox and remove private key","status":"closed","priority":3,"issue_type":"task","created_at":"2025-12-30T03:47:16.278835838Z","updated_at":"2025-12-30T03:55:24.757372876Z","closed_at":"2025-12-30T03:55:24.757377134Z","dependencies":[{"issue_id":"fort-cy6.22.4","depends_on_id":"fort-cy6.22","type":"parent-child","created_at":"2025-12-30T03:47:16.279750808Z","created_by":"daemon"}]}
{"id":"fort-cy6.22.5","title":"Update Justfile to derive from admin principal","status":"closed","priority":3,"issue_type":"task","created_at":"2025-12-30T03:47:16.478081197Z","updated_at":"2025-12-30T03:55:46.080042561Z","closed_at":"2025-12-30T03:55:46.080048744Z","dependencies":[{"issue_id":"fort-cy6.22.5","depends_on_id":"fort-cy6.22","type":"parent-child","created_at":"2025-12-30T03:47:16.479370891Z","created_by":"daemon"}]}
{"id":"fort-cy6.22.6","title":"Remove obsolete key fields from manifest","status":"closed","priority":3,"issue_type":"task","created_at":"2025-12-30T03:47:16.69196144Z","updated_at":"2025-12-30T04:01:38.206569996Z","closed_at":"2025-12-30T04:01:38.206575096Z","dependencies":[{"issue_id":"fort-cy6.22.6","depends_on_id":"fort-cy6.22","type":"parent-child","created_at":"2025-12-30T03:47:16.692839367Z","created_by":"daemon"}]}
{"id":"fort-cy6.23","title":"Exhaustive cache sanity check","description":"Full end-to-end validation of binary cache flow after GitOps rollout is complete.\n\n## Checks\n- Verify CI pushes for all host architectures\n- Confirm substitution logs on multiple hosts\n- Check Attic metrics/logs for error rates\n- Document observed cache hit rates\n\n## Context\nDeferred from fort-cy6.12 - adjacent logs during development gave confidence, but worth a thorough check once everything is stable.","status":"closed","priority":3,"issue_type":"task","created_at":"2025-12-28T22:45:39.128879389Z","updated_at":"2025-12-30T03:02:01.32131697Z","closed_at":"2025-12-30T03:02:01.32131697Z","close_reason":"Closed","dependencies":[{"issue_id":"fort-cy6.23","depends_on_id":"fort-cy6","type":"parent-child","created_at":"2025-12-28T22:45:39.129938712Z","created_by":"daemon"}]}
{"id":"fort-cy6.24","title":"Optimize release workflow: only re-key changed secrets","description":"Currently the release workflow re-keys ALL secrets on every commit. Since age uses random nonces, this produces different ciphertext every time, causing all hosts to see 'changes' and re-evaluate even when their config hasn't changed.\n\n## Optimization\n\n1. Build map of secret path → recipients (already done)\n2. Store this map in the release branch (e.g., `.release-recipients.json`)\n3. On next run, compare current map to stored map\n4. Only re-key secrets whose recipients have changed\n5. Update stored map\n\n## Benefits\n\n- Hosts only see derivation changes when their secrets actually changed\n- Reduces unnecessary evaluation/build cycles\n- More efficient use of cache (unchanged derivations stay cached)\n\n## Notes\n\n- Low priority - current overhead is minimal (eval is fast, builds are cached)\n- But easy win for cleaner semantics","status":"closed","priority":3,"issue_type":"task","created_at":"2025-12-29T04:16:37.21179604Z","updated_at":"2025-12-29T23:00:48.149199633Z","closed_at":"2025-12-29T23:00:48.149199633Z","close_reason":"Closed","dependencies":[{"issue_id":"fort-cy6.24","depends_on_id":"fort-cy6","type":"parent-child","created_at":"2025-12-29T04:16:37.221050799Z","created_by":"daemon"}]}
{"id":"fort-cy6.25","title":"Consolidate host-manifest.json and services.json","description":"The new host-manifest.json includes apps, aspects, and roles. We should extend it to also include exposedServices and update service-registry to read from this single manifest instead of having two redundant files (services.json and host-manifest.json).","status":"closed","priority":3,"issue_type":"task","created_at":"2025-12-29T21:57:53.04155595Z","updated_at":"2025-12-30T04:35:35.475662186Z","closed_at":"2025-12-30T04:35:35.475662186Z","close_reason":"Closed","dependencies":[{"issue_id":"fort-cy6.25","depends_on_id":"fort-cy6","type":"parent-child","created_at":"2025-12-29T21:57:53.050913199Z","created_by":"daemon"}]}
{"id":"fort-cy6.26","title":"Reorder CI: validate before rekey","description":"Currently the CI workflow has 'rekey for release branch' and 'validate the build' as separate jobs. The rekey should only happen after validation passes - no point rekeying secrets for a build that won't work.","status":"closed","priority":3,"issue_type":"task","created_at":"2025-12-29T22:00:13.915937453Z","updated_at":"2025-12-29T22:59:30.762870608Z","closed_at":"2025-12-29T22:59:30.762870608Z","close_reason":"Closed","dependencies":[{"issue_id":"fort-cy6.26","depends_on_id":"fort-cy6","type":"parent-child","created_at":"2025-12-29T22:00:13.92610488Z","created_by":"daemon"}]}
{"id":"fort-cy6.3","title":"Mirror fort-nix repo to Forgejo","description":"Set up fort-nix repository in Forgejo, either as a mirror of the existing remote or as the new primary.\n\n## Context\nfort-nix currently lives on an external git remote. We need it in Forgejo to enable CI/CD via Forgejo Actions.\n\n## Options\n\n### Option A: Mirror (recommended for transition)\n- Create repo in Forgejo\n- Set up as mirror of existing remote\n- Keeps external remote as source of truth during transition\n- Forgejo Actions can run on push events\n\n### Option B: Primary (eventual goal)\n- Create repo in Forgejo as primary\n- Update all developers' remotes\n- External remote becomes backup/mirror\n\n## Implementation\n\n### Create repository\nVia Forgejo web UI or API:\n- Organization: `infra` (create if needed)\n- Repository: `fort-nix`\n- Visibility: Private (VPN-only access anyway)\n\n### Configure as mirror (Option A)\n```bash\n# In Forgejo admin: Settings → Mirror → Add mirror\n# Source: existing git remote URL\n# Sync interval: e.g., every 10 minutes\n```\n\n### Or configure as primary (Option B)\n```bash\ngit remote set-url origin https://git.gisi.network/infra/fort-nix.git\n# Or add as additional remote during transition:\ngit remote add forgejo https://git.gisi.network/infra/fort-nix.git\n```\n\n### Authentication\n- Push access requires authentication\n- Options: SSH keys, personal access tokens, or OIDC-integrated git credential helper\n\n## Acceptance Criteria\n- [ ] fort-nix repo exists in Forgejo\n- [ ] Repository contains current main branch\n- [ ] Can push to Forgejo (at least from laptop/ratched)\n\n## Dependencies\n- fort-cy6.2: SSO should be configured so users can authenticate\n\n## Notes\n- Consider setting up a deploy key for CI operations\n- Branch protection rules can be configured later\n\nLabels: [forgejo phase-1]","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-27T23:49:47.492597329Z","updated_at":"2025-12-28T05:10:34.388950209Z","closed_at":"2025-12-28T05:10:34.388950209Z","close_reason":"Closed","labels":["forgejo","phase-1"],"dependencies":[{"issue_id":"fort-cy6.3","depends_on_id":"fort-cy6","type":"parent-child","created_at":"2025-12-27T23:49:47.501796394Z","created_by":"daemon"},{"issue_id":"fort-cy6.3","depends_on_id":"fort-cy6.2","type":"blocks","created_at":"2025-12-27T23:49:47.504977102Z","created_by":"daemon"}]}
{"id":"fort-cy6.4","title":"Set up Forgejo Actions runner","description":"Configure a Forgejo Actions runner on drhorrible to execute CI workflows.\n\n## Context\nForgejo Actions is GitHub Actions-compatible CI. It requires a runner daemon to execute workflows. The runner will run on drhorrible (forge) alongside Forgejo itself.\n\n## Implementation\n\n### Enable Actions in Forgejo\nUpdate `apps/forgejo/default.nix`:\n\n```nix\nservices.forgejo.settings = {\n  # ... existing settings ...\n  actions = {\n    ENABLED = true;\n  };\n};\n```\n\n### Configure Runner\nUse the NixOS gitea-actions-runner service with forgejo-runner package:\n\n```nix\nservices.gitea-actions-runner = {\n  package = pkgs.forgejo-runner;\n  instances.default = {\n    enable = true;\n    name = \"forge-runner\";\n    url = \"https://git.gisi.network\";\n    tokenFile = config.age.secrets.forgejo-runner-token.path;\n    labels = [\n      \"nixos:host\"          # Native NixOS runner\n      \"x86_64-linux:host\"   # Architecture label\n    ];\n    settings = {\n      # Runner settings\n      capacity = 2;  # Parallel jobs\n    };\n  };\n};\n```\n\n### Secrets\n- Create runner registration token via Forgejo admin UI\n- Store in `apps/forgejo/runner-token.age`\n- Add to agenix configuration\n\n### Nix in Runner Environment\nThe runner needs Nix available for CI jobs. Options:\n1. Use `nixos:host` label and run directly on host (has Nix already)\n2. Use container with Nix installed\n\nOption 1 is simpler for our use case since we're building NixOS configs.\n\n### Runner Capabilities\nThe runner will need:\n- Nix with flakes enabled\n- Network access to cache.nixos.org and our Attic cache (once set up)\n- Ability to run `nix build`, `nix flake check`, etc.\n\n## Acceptance Criteria\n- [ ] Runner appears in Forgejo admin → Actions → Runners\n- [ ] Runner shows as online\n- [ ] Test workflow executes successfully (e.g., simple `nix flake check`)\n\n## Dependencies\n- fort-cy6.1: Forgejo must be deployed first\n\n## Notes\n- Runner token needs to be generated after Forgejo is running\n- Consider adding more runners later for parallelism or different arches\n- Docker/Podman not required if using host labels\n\nLabels: [ci forgejo phase-1]","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-27T23:50:05.470953583Z","updated_at":"2025-12-28T05:59:46.152522046Z","closed_at":"2025-12-28T05:59:46.152522046Z","close_reason":"Closed","labels":["ci","forgejo","phase-1"],"dependencies":[{"issue_id":"fort-cy6.4","depends_on_id":"fort-cy6","type":"parent-child","created_at":"2025-12-27T23:50:05.484171456Z","created_by":"daemon"},{"issue_id":"fort-cy6.4","depends_on_id":"fort-cy6.1","type":"blocks","created_at":"2025-12-27T23:50:05.487740544Z","created_by":"daemon"}]}
{"id":"fort-cy6.5","title":"Create flake check CI workflow","description":"Create a Forgejo Actions workflow that runs `nix flake check` on pull requests and pushes.\n\n## Context\nThis is the basic CI validation workflow. It ensures the flake evaluates correctly before changes are merged.\n\n## Implementation\n\n### Create workflow file\nCreate `.forgejo/workflows/check.yml`:\n\n```yaml\nname: Flake Check\n\non:\n  push:\n    branches: [main]\n  pull_request:\n    branches: [main]\n\njobs:\n  check:\n    runs-on: nixos\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Check root flake\n        run: nix flake check --no-build\n\n      - name: Check host flakes\n        run: |\n          for host_dir in clusters/bedlam/hosts/*/; do\n            host=$(basename \"$host_dir\")\n            echo \"::group::Checking host: $host\"\n            nix flake check \"$host_dir\" --no-build\n            echo \"::endgroup::\"\n          done\n\n      - name: Check device flakes\n        run: |\n          for device_dir in clusters/bedlam/devices/*/; do\n            device=$(basename \"$device_dir\")\n            echo \"::group::Checking device: $device\"\n            nix flake check \"$device_dir\" --no-build\n            echo \"::endgroup::\"\n          done\n```\n\n### Notes on `--no-build`\nUsing `--no-build` speeds up checks by only evaluating, not building. Full builds happen in the release workflow.\n\n### Runner Requirements\n- Runner must have Nix with flakes enabled\n- Uses `nixos` label to run on host runner\n\n## Acceptance Criteria\n- [ ] Workflow triggers on push to main\n- [ ] Workflow triggers on PRs to main\n- [ ] All flake checks pass on current main\n- [ ] Failed checks block PR merge (once branch protection configured)\n\n## Dependencies\n- fort-cy6.4: Runner must be set up\n- fort-cy6.3: Repo must be in Forgejo\n\n## Notes\n- This mirrors `just test` functionality\n- Consider caching Nix store between runs for speed","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-27T23:50:33.356495967Z","updated_at":"2025-12-28T06:05:19.457392982Z","closed_at":"2025-12-28T06:05:19.457392982Z","close_reason":"Closed","labels":["ci","phase-2"],"dependencies":[{"issue_id":"fort-cy6.5","depends_on_id":"fort-cy6","type":"parent-child","created_at":"2025-12-27T23:50:33.366873446Z","created_by":"daemon"},{"issue_id":"fort-cy6.5","depends_on_id":"fort-cy6.4","type":"blocks","created_at":"2025-12-27T23:50:33.370196133Z","created_by":"daemon"},{"issue_id":"fort-cy6.5","depends_on_id":"fort-cy6.3","type":"blocks","created_at":"2025-12-27T23:50:33.371588495Z","created_by":"daemon"}]}
{"id":"fort-cy6.6","title":"Refactor secrets.nix for editor-only keys","description":"Refactor secrets.nix to use editor-only keys on the main branch, enabling the two-branch secrets model.\n\n## Context\nCurrently, secrets are keyed for all devices and re-keyed at deploy time. For GitOps, we need:\n- **main branch**: secrets keyed for editors only (laptop, ratched, forge)\n- **release branch**: CI re-keys for actual host recipients\n\nThis separates secret authorship (editors can decrypt on main) from production access (only hosts can decrypt on release).\n\n## Current State\n`secrets.nix` currently defines `publicKeys` per-secret, including device keys when `KEYED_FOR_DEVICES=1`.\n\n## Implementation\n\n### Define editor keys\n```nix\nlet\n  # Keys that can decrypt secrets on main branch\n  editors = [\n    clusterManifest.sshPublicKey                    # Laptop (primary deploy key)\n    devices.\"\u003cratched-uuid\u003e\".sshPublicKey           # Dev sandbox\n    devices.\"\u003cdrhorrible-uuid\u003e\".sshPublicKey        # Forge (needs to re-key)\n  ];\nin {\n  # All secrets use editors only\n  \"aspects/mesh/auth-key.age\".publicKeys = editors;\n  \"apps/homeassistant/secrets.yaml.age\".publicKeys = editors;\n  # ... all other secrets\n}\n```\n\n### Remove KEYED_FOR_DEVICES logic\nThe deploy-time rekeying with `KEYED_FOR_DEVICES=1` is no longer needed for GitOps hosts. Keep deploy-rs functional for forge (manual deploys), but the release workflow handles rekeying for comin hosts.\n\n### Re-encrypt all secrets\nAfter updating secrets.nix:\n```bash\nagenix -r  # Re-key all secrets for new recipients\n```\n\n### Update .gitignore / pre-commit\nEnsure secrets are only committed with editor keys (never device keys on main).\n\n## Acceptance Criteria\n- [ ] All secrets in main branch keyed only for editors\n- [ ] Laptop can decrypt all secrets\n- [ ] Ratched can decrypt all secrets  \n- [ ] Forge (drhorrible) can decrypt all secrets\n- [ ] Production hosts CANNOT decrypt secrets on main branch\n- [ ] deploy-rs still works for forge (manual deploy)\n\n## Dependencies\n- fort-cy6.1: Forge's SSH key must be known\n\n## Security Considerations\n- This is a security-sensitive change\n- Verify editor list is correct before re-keying\n- Old commits in git history will still have device-keyed secrets (acceptable)\n\n## Notes\n- This is foundational for the release workflow\n- Claude Code on ratched gains ability to author/edit secrets\n- Claude Code still cannot decrypt production secrets (those only exist on release branch)","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-27T23:52:19.969746559Z","updated_at":"2025-12-28T07:58:47.347576258Z","closed_at":"2025-12-28T07:58:47.347576258Z","close_reason":"Closed","labels":["phase-2","secrets"],"dependencies":[{"issue_id":"fort-cy6.6","depends_on_id":"fort-cy6","type":"parent-child","created_at":"2025-12-27T23:52:19.979260963Z","created_by":"daemon"},{"issue_id":"fort-cy6.6","depends_on_id":"fort-cy6.1","type":"blocks","created_at":"2025-12-27T23:52:19.982437154Z","created_by":"daemon"}]}
{"id":"fort-cy6.7","title":"Create release workflow with secret re-keying","description":"Create the Forgejo Actions workflow that re-keys secrets for host recipients and pushes to the release branch.\n\n## Context\nThis is the core of the two-branch secrets model:\n1. Developer pushes to main (secrets keyed for editors)\n2. CI inspects each host's agenix config to determine which secrets it needs\n3. CI re-keys those secrets for the host's public key\n4. CI pushes to release branch\n5. Hosts (via comin) pull from release and can decrypt their secrets\n\n## Implementation\n\n### Create workflow file\nCreate `.forgejo/workflows/release.yml`:\n\n```yaml\nname: Build and Release\n\non:\n  push:\n    branches: [main]\n\njobs:\n  release:\n    runs-on: nixos\n    steps:\n      - uses: actions/checkout@v4\n        with:\n          fetch-depth: 0  # Need full history for branch operations\n\n      - name: Determine per-host secrets\n        run: |\n          mkdir -p /tmp/host-secrets\n          for host in $(nix eval .#hosts --json 2\u003e/dev/null | jq -r 'keys[]' || echo \"\"); do\n            echo \"Analyzing secrets for $host...\"\n            nix eval \".#nixosConfigurations.$host.config.age.secrets\" --json 2\u003e/dev/null \\\n              | jq -r 'to_entries[] | .value.file' \\\n              | sort -u \u003e \"/tmp/host-secrets/$host.txt\" || echo \"No secrets for $host\"\n          done\n\n      - name: Re-key secrets for recipients\n        env:\n          FORGE_AGE_KEY: ${{ secrets.FORGE_AGE_KEY }}\n        run: |\n          echo \"$FORGE_AGE_KEY\" \u003e /tmp/forge-key.txt\n          trap 'rm -f /tmp/forge-key.txt' EXIT\n          \n          for host in $(ls /tmp/host-secrets/); do\n            host=\"${host%.txt}\"\n            echo \"::group::Re-keying for $host\"\n            \n            hostKey=$(nix eval \".#hosts.$host.device.sshPublicKey\" --raw 2\u003e/dev/null || echo \"\")\n            if [ -z \"$hostKey\" ]; then\n              echo \"::warning::Could not get key for $host, skipping\"\n              continue\n            fi\n            \n            while IFS= read -r secret; do\n              [ -z \"$secret\" ] \u0026\u0026 continue\n              echo \"Re-keying: $secret\"\n              # Decrypt with forge key, re-encrypt for host\n              age -d -i /tmp/forge-key.txt \"$secret\" \\\n                | age -e -r \"$hostKey\" -o \"$secret.new\" \\\n                \u0026\u0026 mv \"$secret.new\" \"$secret\"\n            done \u003c \"/tmp/host-secrets/$host.txt\"\n            \n            echo \"::endgroup::\"\n          done\n\n      - name: Commit and push release branch\n        run: |\n          git config user.name \"Forge CI\"\n          git config user.email \"forge@fort.gisi.network\"\n          \n          git checkout -B release\n          git add -A\n          git commit -m \"Release: $(git rev-parse --short main) - $(date -Iseconds)\" \\\n            || echo \"No changes to commit\"\n          git push -f origin release\n```\n\n### Secrets Required\n- `FORGE_AGE_KEY`: Forge's age private key (for decrypting editor-keyed secrets)\n\nStore in Forgejo repository secrets (Settings → Secrets).\n\n### Getting Host List\nThe workflow assumes `.#hosts` exports a list of hosts. This may need adjustment based on flake structure. Alternative:\n```bash\nls clusters/bedlam/hosts/\n```\n\n### Getting Host Public Keys\nAssumes `.#hosts.$host.device.sshPublicKey` is accessible. May need to expose this in the flake or read from device manifest files.\n\n## Acceptance Criteria\n- [ ] Workflow triggers on push to main\n- [ ] Release branch is created/updated\n- [ ] Secrets in release branch are keyed for correct hosts\n- [ ] Forge can still decrypt secrets on main branch\n- [ ] Hosts can decrypt their secrets on release branch\n\n## Dependencies\n- fort-cy6.5: Check workflow should pass first\n- fort-cy6.6: Secrets must be refactored to editor-only\n\n## Security Considerations\n- FORGE_AGE_KEY is highly sensitive - it can decrypt all secrets\n- Stored only in Forgejo secrets, never in repo\n- Runner should not log secret contents\n\n## Notes\n- The exact nix eval paths may need adjustment based on flake structure\n- Consider adding the build step here once Attic is set up (Phase 3)\n- May want to add a \"dry-run\" mode for testing","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-27T23:53:05.370399023Z","updated_at":"2025-12-28T07:58:47.288229687Z","closed_at":"2025-12-28T07:58:47.288229687Z","close_reason":"Closed","labels":["ci","phase-2","secrets"],"dependencies":[{"issue_id":"fort-cy6.7","depends_on_id":"fort-cy6","type":"parent-child","created_at":"2025-12-27T23:53:05.380428876Z","created_by":"daemon"},{"issue_id":"fort-cy6.7","depends_on_id":"fort-cy6.5","type":"blocks","created_at":"2025-12-27T23:53:05.383967205Z","created_by":"daemon"},{"issue_id":"fort-cy6.7","depends_on_id":"fort-cy6.6","type":"blocks","created_at":"2025-12-27T23:53:05.385388765Z","created_by":"daemon"}]}
{"id":"fort-cy6.8","title":"Test CI pipeline end-to-end","description":"Verify the complete CI pipeline works correctly before proceeding to cache and GitOps phases.\n\n## Context\nBefore adding binary caching and comin, we need to verify:\n1. Check workflow runs on PRs\n2. Release workflow creates properly-keyed release branch\n3. Secrets are correctly re-keyed for each host\n\n## Test Cases\n\n### Test 1: Check workflow on PR\n1. Create a branch with a minor change\n2. Open PR against main\n3. Verify check workflow triggers and passes\n4. Verify checks block merge if they fail\n\n### Test 2: Release workflow on merge\n1. Merge the PR to main\n2. Verify release workflow triggers\n3. Verify release branch is created/updated\n\n### Test 3: Secret re-keying verification\nFor each host, verify the secrets in release branch are correctly keyed:\n\n```bash\n# Checkout release branch\ngit checkout release\n\n# For a test host (e.g., ratched), try decrypting a secret\n# This should FAIL on main branch (editor keys only)\n# This should SUCCEED on release branch (host key)\n\n# On the target host, or with the host's private key:\nage -d -i /path/to/host/key aspects/mesh/auth-key.age\n```\n\n### Test 4: Editor access on main\nVerify editors can still decrypt on main:\n```bash\ngit checkout main\nage -d -i ~/.ssh/id_ed25519 aspects/mesh/auth-key.age  # Should work\n```\n\n### Test 5: Host cannot decrypt main\nVerify hosts cannot decrypt main branch secrets:\n```bash\ngit checkout main\nage -d -i /path/to/host/key aspects/mesh/auth-key.age  # Should FAIL\n```\n\n## Acceptance Criteria\n- [ ] Check workflow passes on current main\n- [ ] Release branch exists after push to main\n- [ ] At least one host's secrets verified to be correctly keyed on release\n- [ ] Editors confirmed able to decrypt main branch secrets\n- [ ] Hosts confirmed unable to decrypt main branch secrets\n\n## Dependencies\n- fort-cy6.7: Release workflow must be implemented\n\n## Notes\n- Document any issues found for future reference\n- This is a gate before proceeding to Phase 3\n- Consider automating these tests as part of the pipeline","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-27T23:53:26.367461113Z","updated_at":"2025-12-28T14:55:38.457706872Z","closed_at":"2025-12-28T14:55:38.457706872Z","close_reason":"Closed","labels":["phase-2","testing"],"dependencies":[{"issue_id":"fort-cy6.8","depends_on_id":"fort-cy6","type":"parent-child","created_at":"2025-12-27T23:53:26.377412823Z","created_by":"daemon"},{"issue_id":"fort-cy6.8","depends_on_id":"fort-cy6.7","type":"blocks","created_at":"2025-12-27T23:53:26.380536572Z","created_by":"daemon"}]}
{"id":"fort-cy6.9","title":"Add Attic binary cache to forge","description":"Deploy Attic binary cache server on drhorrible (forge).\n\n## Status: IN PROGRESS - Deploy pending\n\n### Completed\n- [x] Add attic flake input to root flake.nix\n- [x] Update all host flakes to follow root/attic\n- [x] Update common/host.nix to import atticd NixOS module\n- [x] Create apps/attic/default.nix with services.atticd config\n- [x] Add attic to forge role\n- [x] Generate and encrypt server token secret\n- [x] Create bootstrap service for cache/token creation\n- [x] Update Justfile template for new hosts\n- [x] Document Service Initialization patterns in AGENTS.md\n- [x] Fix forgejo binary rename (gitea -\u003e forgejo) broken by nixpkgs update\n- [x] Add pkgs.attic-client to attic bootstrap path\n\n### Pending\n- [ ] Debug atticd startup failure (exit code 101)\n- [ ] Successful deploy to drhorrible\n- [ ] Verify cache accessible at cache.gisi.network\n- [ ] Test pushing a derivation to cache\n\n### Known Issues\nThe first deploy attempt failed with:\n1. **atticd.service exit 101** - Unknown cause, need to check logs after next deploy\n2. **forgejo-bootstrap gitea not found** - Fixed (binary renamed to `forgejo` in nixos-25.11)\n3. **Tailscale state corruption** - Side effect of failed deploy, fixed by clearing state\n\nThe nixpkgs was also updated from 2025-10-14 to 2025-12-06 as a side effect of adding the attic input (host lock files were fully updated rather than just adding new input).\n\n### Files Changed\n- flake.nix, flake.lock (attic input)\n- common/host.nix (import atticd module)\n- apps/attic/default.nix, apps/attic/attic-server-token.age (new)\n- apps/forgejo/default.nix (gitea -\u003e forgejo binary)\n- roles/forge.nix (add attic)\n- Justfile (template update)\n- AGENTS.md (Service Initialization docs)\n- secrets.nix (attic secret)\n- All host flake.nix and flake.lock files\n\n### Next Steps\n1. Deploy to drhorrible (user will do this)\n2. Check `journalctl -u atticd -n 50` for startup error\n3. Fix atticd config issue if needed\n4. Verify bootstrap creates cache and tokens","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-27T23:54:07.515511079Z","updated_at":"2025-12-28T20:07:28.034231214Z","closed_at":"2025-12-28T20:07:28.034231214Z","close_reason":"Closed","labels":["attic","cache","phase-3"],"dependencies":[{"issue_id":"fort-cy6.9","depends_on_id":"fort-cy6","type":"parent-child","created_at":"2025-12-27T23:54:07.527240706Z","created_by":"daemon"},{"issue_id":"fort-cy6.9","depends_on_id":"fort-cy6.8","type":"blocks","created_at":"2025-12-27T23:54:07.530875725Z","created_by":"daemon"}]}
{"id":"fort-d09","title":"Skill: add-app guided workflow","description":"## Context\nAdding a new app to fort-nix involves multiple steps: creating the module, choosing SSO mode, setting up tmpfiles, adding to host manifest. AGENTS.md documents the pattern but it's ~40 lines of always-loaded context.\n\n## Proposed skill: `add-app`\n\nProgressive disclosure skill that loads when adding new services.\n\n### Content to extract from AGENTS.md\n- \"Adding an App\" section (lines 42-60)\n- fortCluster.exposedServices boilerplate\n- Host manifest addition pattern\n\n### Skill structure\n```\n.claude/skills/add-app/\n├── SKILL.md              # Overview, when to use, high-level steps\n├── template.nix          # Base app template with placeholders\n└── examples.md           # Links to outline, pocket-id, forgejo\n```\n\n### Definition of done\n- [ ] Skill created in .claude/skills/add-app/\n- [ ] AGENTS.md \"Adding an App\" section reduced to 2-3 sentence overview pointing to skill\n- [ ] Tested: skill loads when discussing new app creation\n\n## Labels\ndx","status":"open","priority":3,"issue_type":"feature","created_at":"2025-12-31T04:20:42.131126189Z","updated_at":"2025-12-31T04:20:42.131126189Z","dependencies":[{"issue_id":"fort-d09","depends_on_id":"fort-4ge","type":"blocks","created_at":"2026-01-13T16:30:59.70303872Z","created_by":"daemon"}]}
{"id":"fort-d9u","title":"GitHub: Create wicket repo and update PAT","description":"Parent: fort-q3t\n\n## Manual steps (user)\n1. Create `gisikw/wicket` repo on GitHub (can be empty/private)\n2. Update the GitHub PAT used for fort-nix mirroring to also have access to wicket\n   - Or create a new PAT with access to both repos\n   - Needs `Contents: Read and write` permission on both repos\n\n## Notes\n- Current token is at `clusters/bedlam/github-mirror-token.age`\n- If creating new token, will need to re-encrypt with agenix\n- Could use a single token for all mirrors, or per-repo tokens (single is simpler)\n\n## Acceptance\n- [ ] wicket repo exists on GitHub\n- [ ] PAT has push access to both fort-nix and wicket","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-12T06:01:34.218728331Z","updated_at":"2026-01-12T06:23:05.319237624Z","closed_at":"2026-01-12T06:23:05.319237624Z","close_reason":"Closed"}
{"id":"fort-dfk","title":"Track and manage pinned package versions in pkgs/","description":"Custom derivations in pkgs/ (termix, zot, etc.) can go stale when upstream releases new versions.\n\nIdeas:\n- Inventory current pins and their upstream sources\n- Consider renovate/dependabot-style automation\n- At minimum, document how to check for updates\n- Maybe a simple script that compares pinned versions to latest releases","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-01T19:49:00.750342402Z","updated_at":"2026-01-01T19:49:00.750342402Z"}
{"id":"fort-e2w","title":"Design robust cluster backup solution","description":"## Goal\n\nDesign and implement a robust backup solution for the cluster that ensures critical data survives host failures, accidental deletion, and other data loss scenarios.\n\n## Context\n\n- Hosts with impermanence persist `/var/lib` to `/persist/system/var/lib`\n- Services like Outline store data in PostgreSQL and local filesystem\n- Recent data loss incident (fort-8i4) highlights the need for offsite/redundant backups\n- Control plane design (docs/control-plane-design.md) mentions `backup-accept` capability for NAS\n\n## Scope\n\n### Data to backup\n- PostgreSQL databases (outline, pocket-id, forgejo, etc.)\n- Service state directories (`/var/lib/\u003cservice\u003e/`)\n- Secrets (already encrypted with agenix, but should be backed up)\n- Configuration state (git repo is already backed up via GitHub mirror)\n\n### Hosts with critical data\n- **drhorrible** (forge): forgejo, pocket-id, OIDC state\n- **q**: outline, actualbudget, vikunja, *arr stack configs\n- **ursula**: home assistant state, zigbee2mqtt\n- **raishan** (beacon): headscale state\n\n## Design considerations\n\n1. **Backup tool**: restic vs borg vs custom\n   - restic: Good dedup, encryption, S3/B2 backends\n   - borg: Battle-tested, excellent dedup\n   - Custom: Agent-based using `backup-accept` capability\n\n2. **Backup destination**\n   - Offsite cloud (B2, S3, Wasabi)\n   - Local NAS (if available)\n   - Cross-host (backup to another cluster host)\n\n3. **Integration with agent architecture**\n   - `backup-accept` capability on NAS/backup host\n   - Hosts push backups via agent call\n   - Or: backup host pulls from other hosts\n\n4. **Scheduling**\n   - PostgreSQL: pg_dump on schedule, ship to backup\n   - Filesystem: incremental snapshots\n   - Retention policy\n\n5. **Recovery testing**\n   - How do we verify backups are usable?\n   - Periodic restore tests?\n\n## Deliverables\n\n1. Design doc with chosen approach\n2. Backup aspect or role for hosts\n3. Recovery runbook\n4. Monitoring/alerting for backup failures\n\n## References\n\n- docs/control-plane-design.md (mentions backup-accept capability)\n- fort-8i4 (Outline data loss investigation)","status":"in_progress","priority":2,"issue_type":"epic","created_at":"2026-01-03T15:33:54.357573256Z","updated_at":"2026-01-04T05:53:20.932985128Z"}
{"id":"fort-e2w.1","title":"Set up restic REST server on ursula","description":"Deploy restic REST server with append-only mode on ursula. This is the backup hub that receives backups from all other hosts.\n\nTasks:\n- Add backup-hub app or role to apps/\n- Configure REST server with append-only and private-repos\n- Expose via nginx (vpn visibility only)\n- Store repos on ZFS pool\n\nReference: docs/backup-design.md section 7.2","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-04T05:54:16.051730683Z","updated_at":"2026-01-04T05:54:16.051730683Z","dependencies":[{"issue_id":"fort-e2w.1","depends_on_id":"fort-e2w","type":"parent-child","created_at":"2026-01-04T05:54:16.060660393Z","created_by":"daemon"},{"issue_id":"fort-e2w.1","depends_on_id":"fort-e2w.3","type":"blocks","created_at":"2026-01-04T06:05:28.162389287Z","created_by":"daemon"}]}
{"id":"fort-e2w.10","title":"Implement backup verification tests","description":"Automated verification that backups are restorable.\n\nOptions:\n- restic check (repository integrity)\n- Periodic test restore to ephemeral VM\n- Smoke tests on restored data\n\nStart with restic check, consider automated restore testing later.\n\nReference: docs/backup-design.md section 9 (Periodic Recovery Testing)","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-04T06:04:37.87005573Z","updated_at":"2026-01-04T06:04:37.87005573Z","dependencies":[{"issue_id":"fort-e2w.10","depends_on_id":"fort-e2w","type":"parent-child","created_at":"2026-01-04T06:04:37.879122541Z","created_by":"daemon"},{"issue_id":"fort-e2w.10","depends_on_id":"fort-e2w.8","type":"blocks","created_at":"2026-01-04T06:05:40.272543734Z","created_by":"daemon"}]}
{"id":"fort-e2w.11","title":"Add backup agent capabilities","description":"Optional: expose backup operations via fort-agent API.\n\nCapabilities to consider:\n- backup-status: Last backup time, size, success/failure\n- backup-trigger: Manually trigger backup from dev-sandbox\n- backup-list: List available snapshots\n\nLives on backup hub, callable from dev-sandbox.\n\nReference: docs/backup-design.md section 7.3","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-04T06:04:40.662293434Z","updated_at":"2026-01-04T06:04:40.662293434Z","dependencies":[{"issue_id":"fort-e2w.11","depends_on_id":"fort-e2w","type":"parent-child","created_at":"2026-01-04T06:04:40.672457784Z","created_by":"daemon"}]}
{"id":"fort-e2w.12","title":"Set up peer backup exchange","description":"Configure offsite peer backup with trusted party.\n\nTasks:\n- Coordinate with peer on hosting arrangement\n- Deploy restic REST server on peer's Linux box (Docker or native)\n- Set up WireGuard/Tailscale tunnel for secure transport\n- Configure sync from backup hub to peer\n- Verify encrypted blobs land on peer\n- Document mutual backup arrangement\n\nThis provides a third copy (beyond local + cloud) with geographic diversity.\n\nReference: docs/backup-design.md section 6.2, Phase 5","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-04T06:04:54.189475849Z","updated_at":"2026-01-04T06:04:54.189475849Z","dependencies":[{"issue_id":"fort-e2w.12","depends_on_id":"fort-e2w","type":"parent-child","created_at":"2026-01-04T06:04:54.19934704Z","created_by":"daemon"},{"issue_id":"fort-e2w.12","depends_on_id":"fort-e2w.10","type":"blocks","created_at":"2026-01-04T06:05:40.321071446Z","created_by":"daemon"}]}
{"id":"fort-e2w.2","title":"Create backup-client aspect","description":"Create aspects/backup-client that configures hosts to push backups to the hub.\n\nTasks:\n- Install restic package\n- Configure services.restic.backups.system for /var/lib\n- Add PostgreSQL backup job (if postgres enabled)\n- Set up timer for daily backups with randomized delay\n\nReference: docs/backup-design.md section 7.1","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-04T05:54:21.323170286Z","updated_at":"2026-01-04T05:54:21.323170286Z","dependencies":[{"issue_id":"fort-e2w.2","depends_on_id":"fort-e2w","type":"parent-child","created_at":"2026-01-04T05:54:21.332455592Z","created_by":"daemon"},{"issue_id":"fort-e2w.2","depends_on_id":"fort-e2w.3","type":"blocks","created_at":"2026-01-04T06:05:28.215892618Z","created_by":"daemon"}]}
{"id":"fort-e2w.3","title":"Create restic password secret","description":"Create shared restic repository password in agenix.\n\nTasks:\n- Generate strong password\n- Encrypt as restic-password.age\n- Add to secrets.nix with appropriate host keys\n- Document paper backup recommendation\n\nReference: docs/backup-design.md section 3 (Encryption Model)","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-04T05:54:23.889235971Z","updated_at":"2026-01-04T05:54:23.889235971Z","dependencies":[{"issue_id":"fort-e2w.3","depends_on_id":"fort-e2w","type":"parent-child","created_at":"2026-01-04T05:54:23.900316311Z","created_by":"daemon"}]}
{"id":"fort-e2w.4","title":"Roll out backup-client to hosts","description":"Add backup-client aspect to hosts with persistent data.\n\nHosts to enable:\n- drhorrible (forgejo, pocket-id)\n- q (outline, actualbudget, vikunja, *arr configs)\n- ursula (jellyfin, audiobookshelf, calibre-web)\n- raishan (headscale)\n\nVerify backups complete successfully for each host.","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-04T05:58:33.795835387Z","updated_at":"2026-01-04T05:58:33.795835387Z","dependencies":[{"issue_id":"fort-e2w.4","depends_on_id":"fort-e2w","type":"parent-child","created_at":"2026-01-04T05:58:33.806273558Z","created_by":"daemon"},{"issue_id":"fort-e2w.4","depends_on_id":"fort-e2w.2","type":"blocks","created_at":"2026-01-04T06:05:28.107515621Z","created_by":"daemon"},{"issue_id":"fort-e2w.4","depends_on_id":"fort-e2w.1","type":"blocks","created_at":"2026-01-04T06:05:28.266096273Z","created_by":"daemon"}]}
{"id":"fort-e2w.5","title":"Set up Backblaze B2 cloud storage","description":"Configure cloud offsite backup destination.\n\nTasks:\n- Create Backblaze B2 bucket (or Cloudflare R2 if \u003c10GB)\n- Generate application key with write access\n- Store credentials as agenix secret\n- Configure rclone remote on backup hub\n\nReference: docs/backup-design.md section 6.1","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-04T05:58:54.242316083Z","updated_at":"2026-01-04T05:58:54.242316083Z","dependencies":[{"issue_id":"fort-e2w.5","depends_on_id":"fort-e2w","type":"parent-child","created_at":"2026-01-04T05:58:54.251722936Z","created_by":"daemon"},{"issue_id":"fort-e2w.5","depends_on_id":"fort-e2w.4","type":"blocks","created_at":"2026-01-04T06:05:40.06977194Z","created_by":"daemon"}]}
{"id":"fort-e2w.6","title":"Configure nightly cloud sync","description":"Set up automated sync from backup hub to cloud storage.\n\nTasks:\n- Create systemd service for rclone sync\n- Add timer (2 AM suggested)\n- Verify encrypted blobs land in B2\n- Test restore from cloud\n\nReference: docs/backup-design.md section 7.2","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-04T05:58:57.681439177Z","updated_at":"2026-01-04T05:58:57.681439177Z","dependencies":[{"issue_id":"fort-e2w.6","depends_on_id":"fort-e2w","type":"parent-child","created_at":"2026-01-04T05:58:57.690994812Z","created_by":"daemon"},{"issue_id":"fort-e2w.6","depends_on_id":"fort-e2w.5","type":"blocks","created_at":"2026-01-04T06:05:40.127366224Z","created_by":"daemon"}]}
{"id":"fort-e2w.7","title":"Add backup metrics to observability stack","description":"Integrate backup health into fort-observability.\n\nTasks:\n- Export Prometheus metrics: backup age, size, duration per host\n- Create backup health check service on hub\n- Add Grafana dashboard for backup status\n- Consider restic-exporter or custom metrics\n\nReference: docs/backup-design.md section 8","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-04T06:04:13.131606209Z","updated_at":"2026-01-04T06:04:13.131606209Z","dependencies":[{"issue_id":"fort-e2w.7","depends_on_id":"fort-e2w","type":"parent-child","created_at":"2026-01-04T06:04:13.141526182Z","created_by":"daemon"},{"issue_id":"fort-e2w.7","depends_on_id":"fort-e2w.6","type":"blocks","created_at":"2026-01-04T06:05:40.176991935Z","created_by":"daemon"}]}
{"id":"fort-e2w.8","title":"Create backup failure alerts","description":"Set up AlertManager rules for backup health.\n\nAlerts to create:\n- Backup older than 48 hours\n- Backup job failed\n- Cloud sync failed\n- Repository integrity check failed\n\nReference: docs/backup-design.md section 8","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-04T06:04:17.701949646Z","updated_at":"2026-01-04T06:04:17.701949646Z","dependencies":[{"issue_id":"fort-e2w.8","depends_on_id":"fort-e2w","type":"parent-child","created_at":"2026-01-04T06:04:17.712986394Z","created_by":"daemon"},{"issue_id":"fort-e2w.8","depends_on_id":"fort-e2w.7","type":"blocks","created_at":"2026-01-04T06:05:40.227387078Z","created_by":"daemon"}]}
{"id":"fort-e2w.9","title":"Document backup recovery procedures","description":"Write operational runbook for restores.\n\nDocumentation:\n- Full host recovery procedure\n- Single service recovery\n- Point-in-time restore\n- Cloud-only recovery (if hub unavailable)\n- Password recovery (paper backup guidance)\n\nReference: docs/backup-design.md section 9","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-04T06:04:22.481949468Z","updated_at":"2026-01-04T06:04:22.481949468Z","dependencies":[{"issue_id":"fort-e2w.9","depends_on_id":"fort-e2w","type":"parent-child","created_at":"2026-01-04T06:04:22.491567085Z","created_by":"daemon"}]}
{"id":"fort-eax","title":"Increase Nix download-buffer-size for hosts","description":"During the attic deploy, download buffer was noted as full causing slow downloads. Consider adding nix.settings.download-buffer-size to hosts to improve large package downloads.","status":"closed","priority":4,"issue_type":"task","created_at":"2025-12-28T15:38:02.099786945Z","updated_at":"2025-12-30T04:41:24.191470535Z","closed_at":"2025-12-30T04:41:24.191470535Z","close_reason":"Closed"}
{"id":"fort-ee2","title":"Create claude-users LDAP group in lldap","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-21T21:15:50.927493-06:00","updated_at":"2025-12-21T22:20:30.130349-06:00","closed_at":"2025-12-21T22:20:30.130349-06:00","close_reason":"User added claude-users group to ldap-groups.age"}
{"id":"fort-efv","title":"Remove claude-code-ui internal auth once groups restriction works","description":"Once fort-040 (oauth2-proxy groups claim) is fixed, disable or bypass the internal username/password auth in claude-code-ui. VPN + gatekeeper + groups should be sufficient.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-22T00:30:13.582813-06:00","updated_at":"2025-12-27T19:31:29.179784772Z","closed_at":"2025-12-27T19:31:29.179784772Z","close_reason":"Won't do - claude-code-ui being deprovisioned entirely","dependencies":[{"issue_id":"fort-efv","depends_on_id":"fort-040","type":"blocks","created_at":"2025-12-22T00:30:18.458525-06:00","created_by":"daemon"}]}
{"id":"fort-eir","title":"Ensure claude-code-ui user has access to git","description":"The claude-code-ui service user needs git access to work with repositories.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-22T01:22:05.465709-06:00","updated_at":"2025-12-22T01:30:13.080958-06:00","closed_at":"2025-12-22T01:30:13.080958-06:00","close_reason":"Already implemented - git is in the service path at line 76"}
{"id":"fort-fh7","title":"Deploy Claude Code UI to q host","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-21T21:08:48.459214-06:00","updated_at":"2025-12-22T01:01:28.249789-06:00","closed_at":"2025-12-22T01:01:28.249789-06:00","close_reason":"Claude Code UI fully deployed - VPN + gatekeeper auth + internal auth working, Claude OAuth complete"}
{"id":"fort-gel","title":"Add Termix as hosted app on q","description":"Deploy Termix (github.com/Termix-SSH/Termix) on q host. Fast-moving project, likely needs custom derivation. SSH-based terminal sharing tool.","notes":"## Resolution: VPN-only + Internal Auth\n\nAfter investigating Termix OIDC (v1.9.0):\n- OIDC still requires Admin UI configuration (no env var support)\n- No reverse proxy auth header support (no X-Forwarded-User, etc.)\n- Internal auth cannot be disabled\n\nDecision: Deploy behind Tailscale VPN (visibility: vpn, which is default), using Termix's built-in authentication.\n\n### Implementation\n- OCI container: ghcr.io/lukegus/termix:latest (via zot proxy)\n- Port 8080\n- Data persistence: /var/lib/termix:/data\n- Service: termix. (VPN-only access)","status":"closed","priority":2,"issue_type":"feature","created_at":"2025-12-21T11:41:07.941134-06:00","updated_at":"2025-12-22T23:00:23.77283-06:00","closed_at":"2025-12-22T23:00:23.77283-06:00","close_reason":"Implemented with VPN-only visibility + internal auth approach","dependencies":[{"issue_id":"fort-gel","depends_on_id":"fort-ax1","type":"blocks","created_at":"2025-12-21T11:41:22.692611-06:00","created_by":"daemon"}]}
{"id":"fort-j6c","title":"Skill: add-pkg custom derivation","description":"## Context\nCustom derivations in pkgs/ are for external projects not in nixpkgs or too fast-moving. The pattern involves fetchurl, autoPatchelfHook, and proper installation. AGENTS.md has a template but could be more comprehensive.\n\n## Proposed skill: `add-pkg`\n\nProgressive disclosure skill for creating custom derivations.\n\n### Content to extract from AGENTS.md\n- \"Custom Derivations\" section (lines 112-147)\n- pkgs/ vs apps/ distinction\n\n### Skill structure\n```\n.claude/skills/add-pkg/\n├── SKILL.md              # When to use pkgs/, structure overview\n├── binary-template.nix   # For pre-built binaries\n├── source-template.nix   # For building from source\n└── examples.md           # Links to zot, other pkgs/\n```\n\n### Definition of done\n- [ ] Skill created in .claude/skills/add-pkg/\n- [ ] AGENTS.md custom derivations section reduced to brief pointer\n- [ ] Tested: skill loads when packaging external software\n\n## Labels\ndx","status":"open","priority":3,"issue_type":"feature","created_at":"2025-12-31T04:25:11.970543926Z","updated_at":"2025-12-31T04:25:11.970543926Z","dependencies":[{"issue_id":"fort-j6c","depends_on_id":"fort-4ge","type":"blocks","created_at":"2026-01-13T16:30:59.807183926Z","created_by":"daemon"}]}
{"id":"fort-jro","title":"Add Hugo app for static blog hosting","description":"Add Hugo as an app to host a static blog, replacing the S3-hosted blog.\n\nRequirements:\n- Hugo with a theme (TBD which theme)\n- Build static site from content in repo (see fort-xxx for monorepo content)\n- Expose via nginx\n- Automatic rebuild on content changes (git push triggers rebuild)\n\nThis deprecates the existing S3 blog setup.","status":"closed","priority":2,"issue_type":"feature","created_at":"2026-01-04T06:16:21.541457608Z","updated_at":"2026-01-04T17:52:17.115525253Z","closed_at":"2026-01-04T17:52:17.115525253Z","close_reason":"Closed","dependencies":[{"issue_id":"fort-jro","depends_on_id":"fort-19c","type":"related","created_at":"2026-01-04T06:35:51.775429074Z","created_by":"daemon"}]}
{"id":"fort-kfb","title":"Consolidate /var/lib/fort-* directories under /var/lib/fort/","description":"## Context\n\nControl plane state is currently scattered across multiple directories:\n- /var/lib/fort/ (provider state, fulfillment state, handles, status)\n- /var/lib/fort-auth/\u003cservice\u003e/ (OIDC credentials per service)\n- /var/lib/fort-git/ (git credentials)\n\n## Proposal\n\nConsolidate under /var/lib/fort/:\n- /var/lib/fort/auth/\u003cservice\u003e/client-id, client-secret\n- /var/lib/fort/git/token, ...\n- /var/lib/fort/ssl/\u003cdomain\u003e/...\n- /var/lib/fort/state/ (provider-state.json, fulfillment-state.json)\n\n## Benefits\n\n- Single parent directory to manage permissions\n- Clearer ownership (all fort-related state in one place)\n- Easier backup/restore of control plane state\n\n## Migration\n\n- Update handlers to write to new paths\n- Update consumers to read from new paths\n- Add migration in activation script for existing hosts","status":"open","priority":3,"issue_type":"task","created_at":"2026-01-15T14:26:05.698440206Z","updated_at":"2026-01-15T14:26:05.698440206Z","labels":["cleanup","control-plane"]}
{"id":"fort-kl1","title":"Age files must be committed before deploy to avoid revert","description":"When age files are modified but not committed, the deploy process may revert them due to rekeying logic. This is a footgun for operators.","status":"closed","priority":2,"issue_type":"bug","created_at":"2025-12-21T22:20:30.351213-06:00","updated_at":"2025-12-30T07:10:50.781189701Z","closed_at":"2025-12-30T07:10:50.781189701Z","close_reason":"Closed"}
{"id":"fort-le9","title":"Ensure handlers are periodically invoked per GC/TTL rules","description":"Provider GC should not just remove orphaned entries - it should also proactively invoke handlers for credentials approaching expiry so they can be rotated before they lapse.\n\n## Current state\n- GC runs hourly via fort-provider-gc timer\n- GC queries origins for declared needs, removes orphaned entries\n- GC invokes handler with surviving state (FORT_TRIGGER=gc)\n- TTL is stored in .meta files but never checked\n\n## Required behavior\nGC should:\n1. Check expiry times in .meta files\n2. For entries approaching expiry (e.g., within 2x GC interval), re-invoke handler\n3. Handler returns fresh credentials\n4. sendCallback fires to push new credentials to consumers\n\n## Implementation notes\n- Need to decide threshold for \"approaching expiry\" (e.g., TTL/2, or 2*GC_interval)\n- Handler invocation should include context about why it's being called (rotation vs initial)\n- This enables provider-driven rotation without consumer polling\n\nPart of fort-c8y epic.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-13T16:10:04.842814978Z","updated_at":"2026-01-14T23:29:50.923572414Z","closed_at":"2026-01-14T23:29:50.923572414Z","close_reason":"Implemented TTL rotation in GC","dependencies":[{"issue_id":"fort-le9","depends_on_id":"fort-rfv","type":"blocks","created_at":"2026-01-13T16:10:08.508774219Z","created_by":"daemon"},{"issue_id":"fort-le9","depends_on_id":"fort-c8y","type":"blocks","created_at":"2026-01-13T16:10:08.567337222Z","created_by":"daemon"},{"issue_id":"fort-le9","depends_on_id":"fort-9gl","type":"parent-child","created_at":"2026-01-14T17:20:01.039216892Z","created_by":"daemon"}]}
{"id":"fort-lr4","title":"Verify Exo calendar workflow","description":"End-to-end verification that Exo can use the calendar:\n\n1. Exo can list calendars in khal (`khal list`)\n2. Exo can create events on personal calendar (`khal new ...`)\n3. Events sync to Radicale after vdirsyncer runs\n4. Events are visible in external calendar client (Apple Calendar)\n5. Document the workflow in AGENTS.md or create a skill\n\n**Success criteria**: Exo can timeblock focus time without touching work calendar.\n\nDepends on: fort-vdirsyncer-radicale ticket\n\nParent epic: fort-zye","status":"closed","priority":3,"issue_type":"task","created_at":"2026-01-15T14:53:44.115406151Z","updated_at":"2026-01-15T15:56:11.596138435Z","closed_at":"2026-01-15T15:56:11.596138435Z","close_reason":"Closed","dependencies":[{"issue_id":"fort-lr4","depends_on_id":"fort-2nc","type":"blocks","created_at":"2026-01-15T14:53:49.403269859Z","created_by":"daemon"},{"issue_id":"fort-lr4","depends_on_id":"fort-zye","type":"parent-child","created_at":"2026-01-15T14:55:28.006389027Z","created_by":"daemon"}]}
{"id":"fort-mcs","title":"Tech debt: Apps should accept subdomain parameter","description":"Apps that expose a fort service should accept a subdomain argument that adds/overrides the subdomain in exposedServices.\n\nContext: fortCluster.exposedServices already supports separate name and subdomain fields (e.g., pocket-id uses name=\"pocket\" but subdomain=\"id\" → id.gisi.network).\n\nProposed pattern:\n```nix\n# App module accepts subdomain override from host manifest\n{ subdomain ? null }:\n...\nfortCluster.exposedServices = [{\n  name = \"silverbullet\";\n  subdomain = subdomain;  # null uses default, or override from manifest\n  ...\n}];\n```\n\nIn host manifest:\n```nix\napps = [\n  { name = \"silverbullet\"; subdomain = \"exocortex\"; }\n  { name = \"silverbullet\"; subdomain = \"notes\"; }\n];\n```\n\nThis enables multiple instances of the same app on different subdomains without duplicating the entire app module.\n\nAudit existing apps and add subdomain parameter where multi-instance makes sense.","status":"closed","priority":2,"issue_type":"chore","created_at":"2026-01-04T06:35:36.389991149Z","updated_at":"2026-01-04T07:18:03.240645228Z","closed_at":"2026-01-04T07:18:03.240645228Z","close_reason":"Closed"}
{"id":"fort-n7v","title":"Deduplicate 'Waiting for comin' deploy output","description":"The `just deploy` command outputs repeated 'Waiting for comin to fetch...' and 'Waiting for activation...' lines during GitOps deploys. These burn tokens when running in agent context.\n\nSuggested fix: Only output on state changes, or collapse repeated lines into a single updating line (if terminal supports it), or just silence the repeated attempts entirely and only show the final result.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-02T06:02:13.206931244Z","updated_at":"2026-01-09T13:01:13.561831732Z","closed_at":"2026-01-09T13:01:13.561831732Z","close_reason":"Closed"}
{"id":"fort-nrx","title":"Install beads in claude-code-ui context","description":"Install beads (https://github.com/steveyegge/beads) in the claude-code-ui context. This will likely require creating a custom Go derivation and making it available to the service user.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-22T01:22:05.746585-06:00","updated_at":"2025-12-27T19:31:17.891936161Z","closed_at":"2025-12-27T19:31:17.891936161Z","close_reason":"Won't do - claude-code-ui approach abandoned in favor of direct ratched dev environment"}
{"id":"fort-nyj","title":"Create radicale app module","description":"Create `apps/radicale/default.nix` with:\n\n- `services.radicale` enabled with filesystem storage at `/var/lib/radicale`\n- htpasswd authentication (Radicale's built-in auth for CalDAV client compatibility)\n- `fort.cluster.services` with `visibility = \"public\"` and `sso.mode = \"none\"` (Radicale handles its own auth)\n- systemd.tmpfiles.rules for data directory\n\nReference: `apps/vikunja/default.nix` for simple nixpkgs service pattern.\n\n**Auth approach**: CalDAV clients (Apple Calendar, etc.) require HTTP Basic Auth - they don't support OIDC. So we use Radicale's native htpasswd auth with credentials stored in agenix.\n\nParent epic: fort-zye","status":"closed","priority":3,"issue_type":"task","created_at":"2026-01-15T14:53:19.971607493Z","updated_at":"2026-01-15T15:14:13.311434979Z","closed_at":"2026-01-15T15:14:13.311434979Z","close_reason":"Closed","dependencies":[{"issue_id":"fort-nyj","depends_on_id":"fort-zye","type":"parent-child","created_at":"2026-01-15T14:55:27.796360143Z","created_by":"daemon"}]}
{"id":"fort-nz6","title":"Add dual-mode deploy to justfile (GitOps + deploy-rs)","description":"Reduce friction for autonomous deploys by making just deploy auto-detect the appropriate flow based on master key presence.\n\nChanges:\n- Add deploy.pending field to host-status (shows SHA comin has fetched but not activated)\n- Update justfile deploy to use GitOps flow when master key absent, deploy-rs when present\n- GitOps flow polls status until pending matches, triggers deploy, waits for activation","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-01T03:19:28.795667868Z","updated_at":"2026-01-01T03:48:18.051556816Z","closed_at":"2026-01-01T03:48:18.051556816Z","close_reason":"Closed"}
{"id":"fort-ok4","title":"Split-gatekeeper: network-aware auth bypass","description":"Add a new SSO mode or fort.cluster.services option that allows services to have different auth behavior based on request origin:\n\n- Public internet → oauth2-proxy (OIDC required)\n- VPN/tailnet → skip auth entirely (or reduced auth)\n\nThis enables the \"gatekeeper\" pattern where auth is a function of where you're coming from, not what service you're hitting.\n\nUse cases:\n- Monitoring dashboard: require login from public, but VPN users can view directly\n- Internal tools: public access locked down, VPN access open\n\nImplementation options:\n1. New sso.mode = \"gatekeeper\" with sub-options for public vs vpn behavior\n2. Extend existing modes with a vpnBypass = true option\n3. nginx location-based routing checking source IP ranges\n\nRelated: docs/beacon-monitoring-exploration.md discusses this pattern.","status":"closed","priority":3,"issue_type":"feature","created_at":"2026-01-13T19:52:16.447069691Z","updated_at":"2026-01-15T02:51:40.065421602Z","closed_at":"2026-01-15T02:51:40.065421602Z","close_reason":"Closed"}
{"id":"fort-okx","title":"Set up Gitea for self-hosted git","description":"Deploy Gitea (or similar) for self-hosted git. Prereq for CI/CD pipelines.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-27T19:43:36.513587843Z","updated_at":"2025-12-28T02:15:42.546528778Z","closed_at":"2025-12-28T02:15:42.546528778Z","close_reason":"Closed"}
{"id":"fort-ox2","title":"Stand up SilverBullet instance on q","description":"Deploy SilverBullet (note-taking/PKM app) on host q for testing purposes. Will need to create the app module and add it to q's manifest.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-01T21:10:40.770394636Z","updated_at":"2026-01-01T21:24:05.722165932Z","closed_at":"2026-01-01T21:24:05.722165932Z","close_reason":"Closed"}
{"id":"fort-oze","title":"Add wicket repo to forge config","description":"Parent: fort-q3t\nDepends: fort-0ij (multi-repo support), fort-d9u (GitHub setup)\n\n## Changes\nAdd to `clusters/bedlam/manifest.nix` forge.repos:\n```nix\n\"wicket\" = {\n  mirrors = {\n    github = {\n      remote = \"github.com/gisikw/wicket\";\n      tokenFile = ./github-mirror-token.age;  # Same token if PAT covers both\n    };\n  };\n};\n```\n\n## Deploy\n1. Commit and push\n2. `just deploy drhorrible`\n3. Verify wicket repo created in Forgejo\n4. Verify push mirror configured\n\n## Testing\n- Push a commit to wicket in Forgejo\n- Confirm it appears on GitHub\n\n## Acceptance\n- [ ] wicket exists in Forgejo under infra org\n- [ ] Push mirror to gisikw/wicket working","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-12T06:02:02.706603821Z","updated_at":"2026-01-12T06:26:53.691595234Z","closed_at":"2026-01-12T06:26:53.691595234Z","close_reason":"Closed"}
{"id":"fort-pkh","title":"Audit apps for duplicate nginx proxy headers","description":"NixOS nginx includes recommended proxy headers by default (Host, X-Forwarded-*, etc). Apps that also set these in extraConfig cause duplicate headers.\n\nHeadscale was broken by this - duplicate Host header is invalid HTTP/1.1.\n\nAudit other apps and document the pattern in AGENTS.md.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-01T17:51:38.868113768Z","updated_at":"2026-01-14T17:06:00.838868738Z","closed_at":"2026-01-14T17:06:00.838868738Z","close_reason":"Closed"}
{"id":"fort-pq8","title":"Integrate home-manager in dev-sandbox using home-config input","description":"## Context\n\nfort-67y added cluster-level flakes with home-config input. The input is now available in dev-sandbox via extraInputs.home-config, but nothing uses it yet.\n\n## Work Needed\n\n1. Import home-manager's NixOS module in dev-sandbox aspect\n2. Configure home-manager.users.dev to use the home-config flake\n3. Determine how the github:gisikw/config flake exports its config and wire it up appropriately\n\n## Files\n- aspects/dev-sandbox/default.nix (main changes)\n- Possibly clusters/bedlam/hosts/ratched/flake.nix if home-manager module needs to be passed through","status":"closed","priority":3,"issue_type":"task","created_at":"2025-12-31T07:20:34.162211912Z","updated_at":"2025-12-31T07:33:33.19622674Z","closed_at":"2025-12-31T07:33:33.19622674Z","close_reason":"Closed"}
{"id":"fort-puv","title":"Audit container apps for 0.0.0.0 port exposure","description":"SillyTavern was found to expose port 8000 on 0.0.0.0, allowing direct access from the VPN bypassing oauth2-proxy. This was fixed by binding to 127.0.0.1:8000:8000.\n\n## Task\n\n1. Audit all container-based apps for similar exposure:\n   - Check `ports = [ \"XXXX:XXXX\" ]` declarations in apps/*/default.nix\n   - Services behind nginx/oauth2-proxy should bind to localhost only\n   - Services that need direct VPN access (rare) can stay on 0.0.0.0\n\n2. Fix any exposed services by changing to `\"127.0.0.1:PORT:PORT\"`\n\n3. Update documentation:\n   - Add guidance to AGENTS.md about container port binding\n   - Consider updating sso-guide skill if SSO-specific\n\n## Apps to check\n\n- apps/*/default.nix files with `virtualisation.oci-containers`\n- Any service using `ports = [` without explicit localhost binding\n\n## Context\n\nWhen a container binds to 0.0.0.0, it's accessible directly on the VPN, bypassing any reverse proxy auth layer. This defeats SSO protection.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-14T04:56:15.016770535Z","updated_at":"2026-01-14T07:50:58.143562822Z","closed_at":"2026-01-14T07:50:58.143562822Z","close_reason":"Closed"}
{"id":"fort-pwm","title":"Add CalDAV/CardDAV stack (radicale, khal, vdirsyncer)","description":"Add calendar and contacts sync infrastructure:\n- radicale: CalDAV/CardDAV server\n- khal: Calendar CLI client\n- vdirsyncer: Sync tool for calendars/contacts\n\nResearch which components are needed and how they integrate. Consider SSO integration for radicale.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-01T19:49:00.061975909Z","updated_at":"2026-01-13T16:31:00.197851215Z","closed_at":"2026-01-13T16:31:00.197851215Z","close_reason":"completed (khal/vdirsyncer done, radicale tracked in fort-zye)"}
{"id":"fort-q14","title":"Add SSH key for dev-sandbox principal","description":"Add an SSH public key to the dev-sandbox principal for direct SSH access to dev-sandbox hosts.\n\nCurrently dev-sandbox only has an age key (for secrets) and an agentKey (for fort-agent-call signing). Adding an SSH key would allow SSH access to hosts with the dev-sandbox aspect, useful for:\n- Termix integration\n- Direct terminal access without reusing the admin key\n\nUpdate clusters/bedlam/manifest.nix to add publicKey to dev-sandbox principal.","status":"closed","priority":3,"issue_type":"task","created_at":"2025-12-31T21:33:47.227104621Z","updated_at":"2025-12-31T21:53:01.769449602Z","closed_at":"2025-12-31T21:53:01.769449602Z","close_reason":"Closed"}
{"id":"fort-q3t","title":"Multi-repo forge support + wicket mirror","description":"## Goal\nExtend the forge configuration to support multiple repos with push mirrors, then use it to add the `wicket` repo.\n\n## Context\nCurrently `forge` config in cluster manifest is single-repo:\n```nix\nforge = { org = \"infra\"; repo = \"fort-nix\"; mirrors = {...}; };\n```\n\nNeed to support:\n```nix\nforge = {\n  org = \"infra\";\n  repos = {\n    \"fort-nix\" = { mirrors = { github = {...}; }; };\n    \"wicket\" = { mirrors = { github = {...}; }; };\n  };\n};\n```\n\n## Architecture decision\nKeep in cluster manifest for now. Moving to `fort.cluster.forge` mkOption would be cleaner but is a bigger refactor - not worth it for 2 repos. Revisit if we hit 5+.\n\n## Acceptance criteria\n- [ ] wicket repo exists in Forgejo under infra org\n- [ ] Push mirror to gisikw/wicket works\n- [ ] Existing fort-nix mirror still works\n- [ ] Bootstrap is idempotent (re-running doesn't break anything)","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-12T06:01:19.903869792Z","updated_at":"2026-01-12T06:26:58.851039182Z","closed_at":"2026-01-12T06:26:58.851039182Z","close_reason":"Closed"}
{"id":"fort-qg0","title":"Complete attic bootstrap script","description":"The attic server runs, but the bootstrap script (ExecStartPost) that creates admin/CI tokens and the cache needs work. Issues encountered:\n- atticadm needs a config file with all required fields (chunking, etc.)\n- Config must match what atticd uses\n- Consider using the same config generation approach as the nixpkgs module\n\nReference: apps/attic/default.nix ExecStartPost section","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-28T19:59:39.31863417Z","updated_at":"2025-12-28T20:26:02.407846254Z","closed_at":"2025-12-28T20:26:02.407846254Z","close_reason":"Closed"}
{"id":"fort-qm9","title":"Add docker.io user images to zot sync","description":"The zot registry only syncs from docker.io/library (official images) and ghcr.io. User images like dullage/flatnotes need to be pulled directly from Docker Hub.\n\nAdd a sync config for docker.io user namespace images so containers can be pulled through the local registry like other apps (e.g., sillytavern uses ghcr.io through local registry).\n\nReference: apps/zot/default.nix sync config","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-10T01:13:32.490727227Z","updated_at":"2026-01-10T01:13:32.490727227Z"}
{"id":"fort-qmg","title":"Persist tmux session across disconnects in dev-sandbox","description":"When reconnecting to the dev sandbox, automatically reattach to the last active tmux session instead of starting fresh.\n\nTrack last connected session so disconnects/reconnects come back to the same place. Consider:\n- Session naming convention\n- Auto-attach on SSH login\n- Cleanup of stale sessions","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-01T19:49:01.446572019Z","updated_at":"2026-01-01T19:59:59.78729275Z","closed_at":"2026-01-01T19:59:59.78729275Z","close_reason":"Closed"}
{"id":"fort-qz3","title":"Add 'just diagnose \u003chost\u003e' helper","description":"Create a non-interactive diagnostic command that pulls common debug info from a host: failed systemd units, recent journal errors, disk space, memory, etc. Useful for agents who can't use interactive SSH.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-21T12:18:02.974999-06:00","updated_at":"2025-12-30T07:13:52.295155608Z","closed_at":"2025-12-30T07:13:52.295155608Z","close_reason":"Less useful now that most hosts use GitOps autodeploy, and agents lack SSH credentials for journal access anyway."}
{"id":"fort-r65","title":"Migrate Gatus monitoring to split-gatekeeper auth","description":"Once fort-ok4 (split-gatekeeper) is implemented, update the Gatus monitoring instance on beacon to use it:\n\n- Public access: require OIDC login via oauth2-proxy\n- VPN access: skip auth, direct access\n\nFor now, Gatus is VPN-only (visibility = \"vpn\"). This ticket tracks the migration to split-gatekeeper once that pattern exists.\n\nDepends on: fort-ok4\n\nRelated: docs/beacon-monitoring-exploration.md","status":"closed","priority":3,"issue_type":"task","created_at":"2026-01-13T19:52:23.296101622Z","updated_at":"2026-01-15T03:02:15.491265513Z","closed_at":"2026-01-15T03:02:15.491265513Z","close_reason":"Closed","dependencies":[{"issue_id":"fort-r65","depends_on_id":"fort-ok4","type":"blocks","created_at":"2026-01-13T19:52:46.948841133Z","created_by":"daemon"}]}
{"id":"fort-rfv","title":"Implement sendCallback for async capability flow","description":"The async capability flow is supposed to be fire-and-forget with callback delivery, but sendCallback is currently a stub that just logs.\n\n## Current state\n```go\nfunc (h *AgentHandler) sendCallback(origin, path string, response json.RawMessage) {\n    fmt.Fprintf(os.Stderr, \"[callback] would POST to %s%s: %s\\n\", origin, path, string(response))\n    // TODO: Implement actual callback dispatch using fort CLI or direct HTTP\n}\n```\n\n## Required behavior\nWhen handler is invoked with aggregate input and returns responses:\n- If a key was previously unfulfilled and now has a response → fire callback\n- If a key's response changed → fire callback\n- If a key was previously fulfilled but is now absent from handler output → fire callback with empty payload (revocation)\n- Callback POSTs to `https://\u003corigin\u003e/fort/needs/\u003ccapability\u003e/\u003cname\u003e`\n\n## Test cases\nGiven handler takes `{cred1: unfulfilled, cred2: existing, cred3: existing, cred4: existing}` and returns `{cred1: new, cred2: same, cred3: updated}` (cred4 absent):\n1. Verify sendCallback fires for cred1 (newly fulfilled)\n2. Verify sendCallback fires for cred3 (changed)\n3. Verify sendCallback fires for cred4 with empty payload (removed/revoked)\n4. Verify sendCallback does NOT fire for cred2 (unchanged)\n\n## Implementation notes\n- Use fort CLI for signing, or implement direct HTTP with proper auth headers\n- Fire-and-forget (goroutine) is fine, but should log failures\n- Empty payload on callback = revocation signal to consumer\n\nPart of fort-c8y epic.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-13T16:09:36.42229431Z","updated_at":"2026-01-14T17:57:51.34680368Z","closed_at":"2026-01-14T17:57:51.34680368Z","close_reason":"Implemented - validation in fort-vqo","dependencies":[{"issue_id":"fort-rfv","depends_on_id":"fort-c8y","type":"blocks","created_at":"2026-01-13T16:09:40.988586746Z","created_by":"daemon"},{"issue_id":"fort-rfv","depends_on_id":"fort-9gl","type":"parent-child","created_at":"2026-01-14T17:20:00.888019824Z","created_by":"daemon"}]}
{"id":"fort-rh6","title":"Remove sync response payload from async capability requests","description":"Once the callback flow is validated end-to-end, remove the payload from 202 responses for async capabilities.\n\n## Current state (accidental)\nProvider returns 202 with full response payload in body. Consumer fulfill script parses this and uses it directly, bypassing the callback flow entirely.\n\n## Target state\n- Provider returns 202 with empty body (or just ACK metadata)\n- Consumer fulfill script ignores 202 body for async capabilities\n- All credential delivery goes through callbacks\n\n## Migration safety\nThis is the final step after:\n1. fort-rfv: sendCallback implemented\n2. fort-vqo: e2e validation complete\n3. fort-le9: periodic invocation working\n\nOnly flip this switch once confident callbacks work reliably.\n\n## Implementation\n1. Provider: change `w.Write(triggerResponse)` to `w.Write([]byte(\"{}\"))` for async mode\n2. Consumer: update fulfill script to not parse body for async capabilities (just mark \"requested, waiting for callback\")\n\nPart of fort-c8y epic.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-13T16:10:35.14230052Z","updated_at":"2026-01-15T00:24:44.725918621Z","closed_at":"2026-01-15T00:24:44.725918621Z","close_reason":"Implemented - sync fallback removed, consumers wait for callbacks","dependencies":[{"issue_id":"fort-rh6","depends_on_id":"fort-vqo","type":"blocks","created_at":"2026-01-13T16:10:39.259663175Z","created_by":"daemon"},{"issue_id":"fort-rh6","depends_on_id":"fort-c8y","type":"blocks","created_at":"2026-01-13T16:10:39.313159108Z","created_by":"daemon"},{"issue_id":"fort-rh6","depends_on_id":"fort-9gl","type":"parent-child","created_at":"2026-01-14T17:20:00.991926283Z","created_by":"daemon"}]}
{"id":"fort-rp3","title":"Auto-provision Termix OIDC client","description":"Set up OIDC authentication for Termix without clickops. Options:\n\n1. Bootstrap service that creates admin user on first run, stores credentials in /var/lib, then uses admin API to configure OIDC\n2. Direct DB manipulation (less ideal)\n\nTermix's pattern is 'first user is admin', so bootstrap would need to:\n- Create admin user if not exists\n- Store admin credentials to /var/lib/termix/\n- Use those credentials for subsequent OIDC configuration\n\nMay need to clear existing app state if admin already exists from clickops setup.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-02T15:25:55.571493109Z","updated_at":"2026-01-02T16:49:15.753861261Z","closed_at":"2026-01-02T16:49:15.753861261Z","close_reason":"Closed","dependencies":[{"issue_id":"fort-rp3","depends_on_id":"fort-0by","type":"blocks","created_at":"2026-01-02T15:26:13.891902236Z","created_by":"daemon"}]}
{"id":"fort-s2f","title":"Improve Home Assistant dashboard usability","description":"Current HA dashboard is a mess - everything exposed alphabetically with device IDs everywhere.\n\nInspiration:\n- Map-based floorplan dashboards (reddit examples)\n- Pokemon spritesheet style\n- Stardew Valley aesthetic\n- Basically anything visual/spatial rather than list-based\n\nResearch needed:\n- What HA dashboard tools/cards support this (custom:floorplan, picture-elements, etc.)\n- How to create/source appropriate sprites/floorplan images\n- Wife approval on aesthetic direction\n\nGoal: Dashboard that's actually pleasant to use and shows spatial context.","status":"open","priority":2,"issue_type":"feature","created_at":"2026-01-04T06:16:40.152359342Z","updated_at":"2026-01-04T06:16:40.152359342Z"}
{"id":"fort-twh","title":"Add Monokai Pro Spectrum theme and ProggyClean Nerd Font to Termix","description":"Update termix app to patch in custom theme and font on container boot. Theme: Monokai Pro Spectrum (from ghostty/iTerm2 color schemes). Font: ProggyClean Nerd Font v3.4.0.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-08T05:17:19.378099274Z","updated_at":"2026-01-08T12:43:40.975069679Z","closed_at":"2026-01-08T12:43:40.975069679Z","close_reason":"Closed"}
{"id":"fort-ucf","title":"Make Termix public behind OIDC","description":"Once OIDC is configured, change Termix visibility from 'local' to 'public' so it's accessible from outside the VPN (e.g., work laptop).","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-02T15:25:55.749490871Z","updated_at":"2026-01-02T16:55:56.834954296Z","closed_at":"2026-01-02T16:55:56.834954296Z","close_reason":"Closed","dependencies":[{"issue_id":"fort-ucf","depends_on_id":"fort-rp3","type":"blocks","created_at":"2026-01-02T15:26:13.945626487Z","created_by":"daemon"}]}
{"id":"fort-ucz","title":"Add default nginx vhost with deploy status endpoint","description":"Add a default nginx virtual host on port 80 that serves a simple status page showing:\n\n- Deploy timestamp\n- Git SHA / version\n- Host name\n- Maybe uptime or other quick health indicators\n\nThis provides a fast way to verify deploy status without authentication (VPN-gated only).\n\n## Implementation notes\n\n- May require moving some services off port 80 to a different port (nginx can proxy them)\n- Should be part of the base host config (common/ or an aspect)\n- Could be a static JSON file generated at activation time, or a simple systemd service\n- `visibility: local` so it's VPN-only\n\n## Example output\n\n```json\n{\n  \"host\": \"joker\",\n  \"deployed_at\": \"2025-12-29T10:30:00Z\",\n  \"git_sha\": \"abc123f\",\n  \"uptime_seconds\": 3600\n}\n```","status":"closed","priority":3,"issue_type":"feature","created_at":"2025-12-29T22:40:29.065471823Z","updated_at":"2025-12-30T07:38:37.238580539Z","closed_at":"2025-12-30T07:38:37.238580539Z","close_reason":"Closed"}
{"id":"fort-uqd","title":"Identify and implement a notification solution","description":"Add notification service (ntfy or gotify) to the beacon for alerting. Integrate with Gatus for status alerts.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-14T02:15:03.406659828Z","updated_at":"2026-01-16T02:22:55.913929838Z","closed_at":"2026-01-16T02:22:55.913929838Z","close_reason":"Closed"}
{"id":"fort-vpc","title":"Expand certificate broker for arbitrary domain certs","description":"Currently certificate-broker only generates wildcard certs for the cluster domain. Expand to support generating certs for arbitrary external domains via DNS-01 challenge (configurable in host manifest). Eventually expose via fort-agent-call instead of SSH dropoff.","status":"open","priority":3,"issue_type":"feature","created_at":"2026-01-04T19:55:57.692785922Z","updated_at":"2026-01-04T19:55:57.692785922Z"}
{"id":"fort-vqo","title":"Validate end-to-end sendCallback with consumers","description":"Once sendCallback is implemented (fort-rfv), validate the full flow works end-to-end before migrating away from sync responses.\n\n## Test scenario\n1. Consumer declares a need for a capability\n2. Consumer calls provider (gets ACK only, no payload in response)\n3. Provider processes async, fires sendCallback\n4. Consumer receives callback at /fort/needs/\u003ccap\u003e/\u003cid\u003e\n5. Consumer handler is invoked with payload\n6. Consumer marks need as satisfied\n\n## Validation points\n- Consumer callback endpoint auth works (provider can POST)\n- Consumer handler receives correct payload\n- Fulfillment state updates correctly\n- Retry logic works if callback fails (consumer re-requests on nag interval)\n\n## Dependencies\nRequires fort-rfv (sendCallback implementation) to be complete first.\n\nPart of fort-c8y epic.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-13T16:09:50.376883937Z","updated_at":"2026-01-14T18:26:59.715761632Z","closed_at":"2026-01-14T18:26:59.715761632Z","close_reason":"Validated: callbacks dispatch successfully, consumer endpoint authenticates providers correctly","dependencies":[{"issue_id":"fort-vqo","depends_on_id":"fort-rfv","type":"blocks","created_at":"2026-01-13T16:09:54.150398215Z","created_by":"daemon"},{"issue_id":"fort-vqo","depends_on_id":"fort-c8y","type":"blocks","created_at":"2026-01-13T16:09:54.20875726Z","created_by":"daemon"},{"issue_id":"fort-vqo","depends_on_id":"fort-9gl","type":"parent-child","created_at":"2026-01-14T17:20:00.939037257Z","created_by":"daemon"}]}
{"id":"fort-vqv","title":"Update forgejo git-token handler for revocation and TTL-based reissue","description":"The forgejo git-token handler needs to support the full credential lifecycle: issuance, rotation, and revocation.\n\n## Current state\n- Handler creates tokens idempotently (reuses existing if present)\n- Tokens stored in /var/lib/forgejo-deploy-tokens/\u003corigin\u003e-\u003caccess\u003e\n- No cleanup when needs are removed\n- No expiry on tokens (permanent until deleted)\n\n## Required behavior\n\n### Revocation (on GC with removed entries)\nWhen handler is invoked with FORT_TRIGGER=gc:\n1. Compare survivors list against token files on disk\n2. For any token files not in survivors: delete file AND revoke token in Forgejo\n3. Return only survivors in output (triggers empty-payload callback for revoked consumers)\n\n### Rotation (TTL-based reissue)\nWhen handler is invoked for rotation:\n1. Generate new token with expiry\n2. Delete old token from Forgejo\n3. Return new token (triggers callback with new credentials)\n\n## Implementation notes\n- Forgejo CLI: `forgejo admin user delete-access-token --username forge-admin --token-name \u003cname\u003e`\n- May need to issue tokens WITH expiry (Forgejo supports this) so TTL is meaningful\n- Handler needs to track token names to enable deletion\n\nDepends on: fort-rfv (sendCallback), fort-le9 (periodic invocation)\nPart of fort-c8y epic.","status":"closed","priority":3,"issue_type":"task","created_at":"2026-01-13T16:10:21.588121275Z","updated_at":"2026-01-15T02:26:55.195916834Z","closed_at":"2026-01-15T02:26:55.195916834Z","close_reason":"Closed","dependencies":[{"issue_id":"fort-vqv","depends_on_id":"fort-rfv","type":"blocks","created_at":"2026-01-13T16:10:25.496930108Z","created_by":"daemon"},{"issue_id":"fort-vqv","depends_on_id":"fort-le9","type":"blocks","created_at":"2026-01-13T16:10:25.546159148Z","created_by":"daemon"},{"issue_id":"fort-vqv","depends_on_id":"fort-c8y","type":"blocks","created_at":"2026-01-13T16:10:25.599881708Z","created_by":"daemon"},{"issue_id":"fort-vqv","depends_on_id":"fort-9gl","type":"parent-child","created_at":"2026-01-14T17:20:01.087130505Z","created_by":"daemon"}]}
{"id":"fort-w38","title":"Track and alert on stale container image versions","description":"Container apps (homepage, sillytavern, super-productivity) are now pinned to explicit versions, but there's no mechanism to detect when newer versions are available.\n\nIdeas:\n- Script that compares pinned versions to latest releases (similar to fort-dfk for pkgs/)\n- Could integrate with observability stack for alerts\n- Or periodic report during deploys\n- Some apps self-nag but not all\n\nRelated: fort-dfk covers pkgs/ derivations, this covers OCI containers.\n\nCurrent pinned versions (as of 2026-01-09):\n- homepage: v1.8.0\n- sillytavern: 1.15.0  \n- super-productivity: v16.8.1","status":"open","priority":3,"issue_type":"feature","created_at":"2026-01-09T15:04:20.833547733Z","updated_at":"2026-01-09T15:04:20.833547733Z","dependencies":[{"issue_id":"fort-w38","depends_on_id":"fort-uqd","type":"blocks","created_at":"2026-01-14T17:15:08.065252669Z","created_by":"daemon"}]}
{"id":"fort-wwh","title":"Egress alert: skip notification to dismisser","description":"When someone dismisses the egress door alert, the 'Door alert cleared' notification currently goes to all recipients. It would be better to skip sending to whoever actually pressed the dismiss button.\n\nRequires tracking who dismissed (possibly via separate input_button per adult, or via HA user context if available).","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-02T06:11:10.233395583Z","updated_at":"2026-01-14T16:30:33.415957478Z","closed_at":"2026-01-14T16:30:33.415957478Z","close_reason":"Superseded by broader HA armed/disarmed UX ticket"}
{"id":"fort-xeb","title":"Document service registry OIDC provisioning flow","description":"The dummy-creds-then-real-creds flow via service-registry is non-intuitive. Add docs to AGENTS.md or README explaining: 1) oauth2-proxy starts with placeholder creds, 2) service-registry provisions real OIDC clients, 3) proxy gets restarted with real creds","status":"closed","priority":3,"issue_type":"task","created_at":"2025-12-21T22:38:47.981881-06:00","updated_at":"2025-12-30T07:15:14.534303779Z","closed_at":"2025-12-30T07:15:14.534303779Z","close_reason":"Folding into fort-0pb - the SSO skill will cover the provisioning flow as part of OIDC mode guidance."}
{"id":"fort-xzk","title":"Create pkgs/claude-code derivation for @anthropic-ai/claude-code","description":"Replace runtime npm install with proper Nix derivation using buildNpmPackage or similar","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-21T21:23:27.212414-06:00","updated_at":"2025-12-21T21:34:24.368933-06:00","closed_at":"2025-12-21T21:34:24.368933-06:00","close_reason":"Closed"}
{"id":"fort-yan","title":"oidc-register handler creates duplicate clients instead of updating","description":"## Context\n\nDuring fort-0rj testing, pocket-id ended up with duplicate OIDC client records for the same service (e.g., two \"exocortex.gisi.network\" clients - one with kids group, one unrestricted).\n\n## Expected Behavior\n\nHandler should:\n1. Find existing client by name\n2. Update its allowed groups\n3. Return existing credentials (or regenerate secret if needed)\n\n## Observed Behavior\n\nHandler created a NEW client instead of updating the existing one, resulting in:\n- Duplicate clients in pocket-id\n- \"null\" client_id in delivered credentials (suggesting response parsing failed)\n\n## Possible Causes\n\n1. Client name lookup failed (case sensitivity? encoding?)\n2. set_allowed_groups API call failed, causing subsequent logic to diverge\n3. Cached response got corrupted, handler fell through to create-new-client path\n4. Race condition between multiple handler invocations\n\n## Investigation Steps\n\n- Add error handling/logging to set_allowed_groups calls\n- Verify client name matching logic (exact string comparison)\n- Check if GC is incorrectly removing clients\n- Review aggregate handler concurrency behavior\n\n## Current State\n\nDuplicate clients still exist in pocket-id (not cleaned up to preserve evidence).\n\n## Related\n\n- fort-0rj: Original implementation\n- fort-ajd: Handler testing strategy","status":"open","priority":3,"issue_type":"bug","created_at":"2026-01-15T14:31:41.126255924Z","updated_at":"2026-01-15T14:31:41.126255924Z","labels":["control-plane","pocket-id"]}
{"id":"fort-ydn","title":"Generalize restart capability to systemd capability","description":"Replace the single-purpose 'restart' capability with a general 'systemd' capability that supports multiple actions:\n\n```json\n{\"action\": \"restart\", \"unit\": \"nginx\", \"delay\": 2}\n{\"action\": \"failed\"}  // list failed units  \n{\"action\": \"status\", \"unit\": \"nginx\"}  // specific unit status\n{\"action\": \"list\", \"pattern\": \"fort-*\"}  // list matching units\n```\n\nThis avoids capability sprawl and provides a single interface for systemd operations.\n\nContext: During 2026-01-01 after-action, drhorrible showed 1 failed unit but we couldn't identify it - only the count was available via status endpoint.\n\nNote: drhorrible's failed unit appears to have recovered on its own.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-01T17:53:56.456498928Z","updated_at":"2026-01-13T16:40:08.885504483Z","closed_at":"2026-01-13T16:40:08.885504483Z","close_reason":"Closed"}
{"id":"fort-z01","title":"Make SilverBullet public with gatekeeper SSO","description":"Open SilverBullet to public access behind OIDC gatekeeper mode. SilverBullet doesn't have user space concept, so gatekeeper (login required, no identity passed to backend) is appropriate.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-02T15:25:55.939043926Z","updated_at":"2026-01-02T16:55:56.894280157Z","closed_at":"2026-01-02T16:55:56.894280157Z","close_reason":"Closed"}
{"id":"fort-z0b","title":"Investigate lldap-bootstrap flakiness","description":"lldap-bootstrap.service occasionally fails with 'jq: parse error: Invalid numeric literal at line 1, column 5'. Observed during deploy to drhorrible. May be a race condition or malformed secret file.","status":"closed","priority":4,"issue_type":"bug","created_at":"2025-12-21T12:46:06.509684-06:00","updated_at":"2026-01-09T12:46:11.680458988Z","closed_at":"2026-01-09T12:46:11.680458988Z","close_reason":"Closed"}
{"id":"fort-zdi","title":"Migrate container apps off :latest tags","description":"Audit and pin explicit versions for container apps currently using :latest:\n\n- apps/homepage (ghcr.io/gethomepage/homepage:latest)\n- apps/sillytavern (ghcr.io/sillytavern/sillytavern:latest)\n\nPinning versions prevents surprise breakage when cache invalidates and ensures reproducible deploys.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-02T17:17:16.366811019Z","updated_at":"2026-01-09T14:56:55.964331646Z","closed_at":"2026-01-09T14:56:55.964331646Z","close_reason":"Closed"}
{"id":"fort-zk9","title":"Create apps/claude-code-ui app module","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-21T21:14:09.693182-06:00","updated_at":"2025-12-21T22:17:42.711976-06:00","closed_at":"2025-12-21T22:17:42.711976-06:00","close_reason":"App module written with OAuth support","dependencies":[{"issue_id":"fort-zk9","depends_on_id":"fort-244","type":"blocks","created_at":"2025-12-21T21:14:15.207987-06:00","created_by":"daemon"},{"issue_id":"fort-zk9","depends_on_id":"fort-xzk","type":"blocks","created_at":"2025-12-21T21:23:44.154292-06:00","created_by":"daemon"},{"issue_id":"fort-zk9","depends_on_id":"fort-69s","type":"blocks","created_at":"2025-12-21T21:23:44.392207-06:00","created_by":"daemon"}]}
{"id":"fort-zye","title":"Radicale: self-hosted calendar for Exocortex","description":"**Desired state:** A self-hosted CalDAV server (Radicale) that Exo can write to, separate from Google Calendar.\n\n**Why:**\n- Personal/home calendar separate from work\n- Independence from Google\n- A calendar that's purely Exo-controlled and doesn't touch work systems\n\n**Stack:** Radicale (CalDAV server) + vdirsyncer + khal\n\n**Requirements:**\n- Radicale deployed somewhere in the cluster\n- Synced to khal on ratched alongside Google Calendar\n- Exo can create events that don't pollute work calendar\n- Should integrate cleanly with the work calendar sync (fort-a7j) - merged view in khal\n\n**Context:** This is the second phase of Exocortex calendar integration. Work calendar sync (fort-a7j) should be done first. Related: exocortex-5o6.","status":"closed","priority":3,"issue_type":"epic","created_at":"2026-01-09T15:02:40.971843229Z","updated_at":"2026-01-15T15:56:19.356964511Z","closed_at":"2026-01-15T15:56:19.356964511Z","close_reason":"Closed"}
{"id":"fort-zye.1","title":"Break down Radicale epic into tickets","description":"**Instructions for local agent:**\n\n1. Read the parent epic (fort-zye) for desired state\n2. Explore the fort-nix codebase to understand:\n   - Where Radicale would fit (which host, which cluster)\n   - How to expose it (internal only? via Tailscale?)\n   - Storage/backup considerations\n   - How it interacts with the work calendar sync (fort-a7j)\n3. Clarify any ambiguities with Kevin\n4. Create tickets under this epic for the actual implementation work\n\n**Dependency:** fort-a7j (work calendar sync) should be done first - this epic builds on that foundation.\n\nThis ticket was created by Exo (exocortex) as a cross-repo handoff.","status":"closed","priority":3,"issue_type":"task","created_at":"2026-01-09T15:03:15.971899007Z","updated_at":"2026-01-15T14:53:54.188161053Z","closed_at":"2026-01-15T14:53:54.188161053Z","close_reason":"Closed","dependencies":[{"issue_id":"fort-zye.1","depends_on_id":"fort-zye","type":"parent-child","created_at":"2026-01-09T15:03:15.974213552Z","created_by":"dev"}]}
